{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import gensim as gs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.tokenize import sent_tokenize as st\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense,LSTM,Input,Activation,Add,TimeDistributed,\\\n",
    "Permute,Flatten,RepeatVector,merge,Lambda,Multiply,Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "histPath = 'rnn_training_history/'\n",
    "#load training data\n",
    "with open(histPath+'train.txt', \"rb\") as f1, open(histPath+'val.txt', \"rb\") as f2, open(histPath+'test.txt', \"rb\") as f3: \n",
    "    trainX, trainY = pickle.load(f1)\n",
    "    valX, valY = pickle.load(f2)\n",
    "    testX, testY = pickle.load(f3)\n",
    "#load dictionaries\n",
    "with open(histPath+'word2idx_master.json', 'r') as f1, open(histPath+'idx2word_master.json', 'r') as f2:\n",
    "    word2idx = json.load(f1)\n",
    "    idx2word = json.load(f2)\n",
    "\n",
    "#load embedding matrix\n",
    "embeddMatrix = np.load(histPath+'embeddMatrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for model training\n",
    "seed = 209\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "LR = 1e-4\n",
    "batch_size = 64\n",
    "num_train_batches = len(trainX) // batch_size\n",
    "num_val_samples = len(valX) + len(trainX) - batch_size*num_train_batches\n",
    "num_val_batches = len(valX) // batch_size\n",
    "total_entries = (num_train_batches + num_val_batches)*batch_size\n",
    "\n",
    "#maximum length for title \n",
    "# tMaxLen = 20\n",
    "tMaxLen = 50\n",
    "#maximum length for abstract\n",
    "aMaxLen = 50\n",
    "#total maximum length\n",
    "maxlen = tMaxLen + aMaxLen\n",
    "\n",
    "batch_norm=False\n",
    "\n",
    "embeddDim = embeddMatrix.shape[1]\n",
    "nUnique = embeddMatrix.shape[0]\n",
    "hidden_units= embeddDim\n",
    "\n",
    "learning_rate = 0.002\n",
    "clip_norm = 1.0\n",
    "regularizer = l2(weight_decay) if weight_decay else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## I. Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding function for abstracts\n",
    "def padAbstract(x, maxL = aMaxLen, dictionary = word2idx):\n",
    "    n = len(x)\n",
    "    if n > maxL:\n",
    "        x = x[-maxL:]\n",
    "        n = maxL\n",
    "    return [dictionary['_']]*(maxL - n) + x + [dictionary['*']]\n",
    "\n",
    "#build generator for model\n",
    "def generator(trainX, trainY, batch_size = batch_size, \n",
    "              nb_batches = None, model = None, seed = seed):\n",
    "    \n",
    "    #UNDERSTAND THIS\n",
    "    c = nb_batches if nb_batches else 0\n",
    "    while True:\n",
    "        titles = list()\n",
    "        abstracts = list()\n",
    "        if nb_batches and c >= nb_batches:\n",
    "            c = 0\n",
    "        new_seed = random.randint(0, sys.maxsize)\n",
    "        random.seed(c+123456789+seed)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            a = random.randint(0,len(trainX)-1)\n",
    "            \n",
    "            #random shuffling of data\n",
    "            abstract = trainX[a]\n",
    "            s = random.randint(min(aMaxLen,len(abstract)), max(aMaxLen,len(abstract)))\n",
    "            abstracts.append(abstract[:s])\n",
    "            \n",
    "            title = trainY[a]\n",
    "            s = random.randint(min(tMaxLen,len(title)), max(tMaxLen,len(title)))\n",
    "            titles.append(title[:s])\n",
    "\n",
    "        # undo the seeding before we yield in order not to affect the caller\n",
    "        c+= 1\n",
    "        random.seed(new_seed)\n",
    "\n",
    "        yield conv_seq_labels(abstracts, titles)\n",
    "\n",
    "#pad sequence and convert title to labels\n",
    "def conv_seq_labels(abstracts, titles, nflips = None, model = None, dictionary = word2idx):\n",
    "    \"\"\"abstract and titles are converted to padded input vectors. Titles are one-hot encoded to labels.\"\"\"\n",
    "    batch_size = len(titles)\n",
    "    \n",
    "    \n",
    "    x = [padAbstract(a)+t for a,t in zip(abstracts, titles)] \n",
    "    x = sequence.pad_sequences(x, maxlen = maxlen, value = dictionary['_'], \n",
    "                               padding = 'post', truncating = 'post')\n",
    "        \n",
    "    y = np.zeros((batch_size, tMaxLen, nUnique))\n",
    "    for i, it in enumerate(titles):\n",
    "        it = it + [dictionary['*']] + [dictionary['_']]*tMaxLen  # output does have a eos at end\n",
    "        it = it[:tMaxLen]\n",
    "        y[i,:,:] = np_utils.to_categorical(it, nUnique)\n",
    "        \n",
    "    #The 3 inputs are description, summary starting with eos and a one-hot encoding of the summary categorical variables.\n",
    "    return [x[:,:aMaxLen],x[:,aMaxLen:]], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50) (64, 50) (64, 50, 32471)\n",
      "Abstract  :  ['and', 'surface', 'initialization', 'at', 'test', 'time,', 'without', 'manual', 'effort.', 'self-supervision', 'by', 'back-propagating', 'through', 'differentiable', 'rendering', 'allows', '<ign>', 'adaptation', 'of', 'the', 'model', 'to', 'the', 'test', 'data,', 'and', 'offers', 'much', 'tighter', 'fit', 'than', 'a', 'pretrained', 'fixed', 'model.', 'we', 'show', 'that', 'the', 'proposed', 'model', 'improves', 'with', 'experience', 'and', 'converges', 'to', 'low-error', 'solutions', 'where']\n",
      "Title  :  ['*', 'self-supervised', 'learning', 'of', 'motion', 'capture', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "#check generator\n",
    "check = next(generator(trainX, trainY, batch_size = batch_size))\n",
    "print(check[0][0].shape,check[0][1].shape,check[1].shape)\n",
    "print(\"Abstract  : \", [idx2word[str(i)] for i in check[0][0][5]])\n",
    "print(\"Title  : \", [idx2word[str(i)] for i in check[0][1][5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator for training and validation\n",
    "genTrain = generator(trainX, trainY, batch_size = batch_size)\n",
    "genVal =  generator(valX, valY, nb_batches = len(valX)// batch_size, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## II. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single layger LSTM \n",
    "def encoder_decoder(genTrain, genVal, mode = 'fit', num_epochs = 1, \n",
    "                    en_shape = aMaxLen, de_shape = tMaxLen):\n",
    "    \n",
    "    print('Encoder_Decoder LSTM...')\n",
    "   \n",
    "    \"\"\"__encoder___\"\"\"\n",
    "    encoder_inputs = Input(shape=(en_shape,))\n",
    "    print(encoder_inputs)\n",
    "    \n",
    "    #APPLY EMBEDDING LAYER. https://keras.io/layers/embeddings/       \n",
    "    input_emb = Embedding(nUnique, embeddDim,\n",
    "                          input_length = aMaxLen,\n",
    "                          W_regularizer = regularizer, dropout = p_emb, \n",
    "                          weights=[embeddMatrix], mask_zero=True,\n",
    "                          name='embedding_1')\n",
    "    \n",
    "    #ENCODER LSTM - FORWARD   https://keras.io/layers/recurrent/  \n",
    "    encoder_LSTM = LSTM(hidden_units, dropout_U = 0.2, dropout_W = 0.2 ,return_state=True)\n",
    "    encoder_LSTM_rev = LSTM(hidden_units,return_state=True,go_backwards=True)\n",
    "    \n",
    "    #ENCODER LSTM - REVERSE \n",
    "    encoder_outputsR, state_hR, state_cR = encoder_LSTM_rev(input_emb(encoder_inputs))\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(input_emb(encoder_inputs))\n",
    "        \n",
    "    state_hfinal=Add()([state_h,state_hR])\n",
    "    state_cfinal=Add()([state_c,state_cR])\n",
    "    \n",
    "    encoder_states = [state_hfinal,state_cfinal]\n",
    "    \n",
    "    \"\"\"____decoder___\"\"\"\n",
    "    #Input to the decoder would be the summary(headline) sequence starting from ~ character.\n",
    "#     decoder_inputs = Input(shape=(de_shape,))\n",
    "    decoder_inputs = Input(shape=(en_shape,))\n",
    "    print(decoder_inputs)\n",
    "      \n",
    "    decoder_LSTM = LSTM(hidden_units,return_sequences=True,return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_LSTM(input_emb(decoder_inputs),initial_state=encoder_states) \n",
    "    decoder_dense = Dense(de_shape,activation='linear')\n",
    "    \n",
    "    # Apply a dense layer that has vocab_size(40000) outputs which learns probability of each word when softmax is applied.\n",
    "    # TimeDistributed is a wrapper for applying the same function over all the time step outputs. \n",
    "    # Refer https://keras.io/layers/wrappers/\n",
    "    time_distributed = TimeDistributed(Dense(nUnique,\n",
    "                                             W_regularizer=regularizer, b_regularizer=regularizer,\n",
    "                                             name = 'timedistributed_1'))\n",
    "    activation = Activation('softmax', name='activation_1')\n",
    "    decoder_outputs = activation(time_distributed(decoder_outputs))\n",
    "    \n",
    "    #Model groups layers into an object with training and inference features.\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/keras/models/Model        \n",
    "    model= Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)\n",
    "    \n",
    "    rmsprop = RMSprop(lr = learning_rate,clipnorm = clip_norm)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=rmsprop)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.fit_generator(genTrain,\n",
    "                            steps_per_epoch = num_train_batches,\n",
    "                            epochs=5,  #Try different epochs as hyperparameter \n",
    "                            validation_data = genVal,\n",
    "                            validation_steps = num_val_batches)\n",
    "    \n",
    "    #_________________________INFERENCE MODE______________________________#  \n",
    "    \n",
    "    encoder_model_inf = Model(encoder_inputs,encoder_states)\n",
    "    \n",
    "    decoder_state_input_H = Input(shape=(hidden_units,))\n",
    "    decoder_state_input_C = Input(shape=(hidden_units,)) \n",
    "    decoder_state_inputs = [decoder_state_input_H, decoder_state_input_C]\n",
    "    decoder_outputs, decoder_state_h, decoder_state_c = decoder_LSTM(input_emb(decoder_inputs),\n",
    "                                                                     initial_state=decoder_state_inputs)\n",
    "    decoder_states = [decoder_state_h, decoder_state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model_inf= Model([decoder_inputs]+decoder_state_inputs,\n",
    "                             [decoder_outputs]+decoder_states)\n",
    "    \n",
    "    return model,encoder_model_inf,decoder_model_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try this\n",
    "# model, encoder, decoder = encoder_decoder(genTrain, genVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
