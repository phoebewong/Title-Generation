{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#utils check \n",
    "import utils_updated\n",
    "from rnn import processText\n",
    "\n",
    "#import packages\n",
    "import string\n",
    "# import glove\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# RNN\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "      <th>aLen</th>\n",
       "      <th>tLen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>self-organization of associative database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>an efficient method of self-organizing associa...</td>\n",
       "      <td>767  self-organization of associative database...</td>\n",
       "      <td>73</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>a mean field theory of layer iv of visual cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>a single cell theory for the development of se...</td>\n",
       "      <td>683  a mean field theory of layer iv of visual...</td>\n",
       "      <td>91</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>storing covariance by the associative long-ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>in modeling studies or memory based on neural...</td>\n",
       "      <td>394  storing covariance by the associative lon...</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>bayesian query construction for neural network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>if data collection is costly, there is much to...</td>\n",
       "      <td>bayesian query construction for neural network...</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>neural network ensembles, cross validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>learning of continuous valued functions using ...</td>\n",
       "      <td>neural network ensembles, cross validation, an...</td>\n",
       "      <td>129</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  self-organization of associative database and ...        NaN   \n",
       "1    10  1987  a mean field theory of layer iv of visual cort...        NaN   \n",
       "2   100  1988  storing covariance by the associative long-ter...        NaN   \n",
       "3  1000  1994  bayesian query construction for neural network...        NaN   \n",
       "4  1001  1994  neural network ensembles, cross validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name  \\\n",
       "0  1-self-organization-of-associative-database-an...   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...   \n",
       "2  100-storing-covariance-by-the-associative-long...   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...   \n",
       "4  1001-neural-network-ensembles-cross-validation...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  an efficient method of self-organizing associa...   \n",
       "1  a single cell theory for the development of se...   \n",
       "2   in modeling studies or memory based on neural...   \n",
       "3  if data collection is costly, there is much to...   \n",
       "4  learning of continuous valued functions using ...   \n",
       "\n",
       "                                          paper_text  aLen  tLen  \n",
       "0  767  self-organization of associative database...    73     7  \n",
       "1  683  a mean field theory of layer iv of visual...    91    17  \n",
       "2  394  storing covariance by the associative lon...   300    15  \n",
       "3  bayesian query construction for neural network...    94     7  \n",
       "4  neural network ensembles, cross validation, an...   129     8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv('../data/papers.csv')\n",
    "#preprocessing\n",
    "dataNew = utils_updated.preprocessing(data, dropnan = True)\n",
    "#check data## I. Data Preprocessing\n",
    "dataNew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize distribution \n",
    "fig, ax = plt.subplots(figsize = (10, 8))\n",
    "ax.hist(dataNew.aLen, bins = 100)\n",
    "ax.set_title('Distribution of Abstract Length', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4638, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for now we use articles with a length less than 250\n",
    "data250 = dataNew[dataNew.aLen <= 250]\n",
    "data250.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum title length:  20\n"
     ]
    }
   ],
   "source": [
    "print('Maximum title length: ', data250.tLen.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  32468\n",
      "Maxmimum sequence length:  250\n"
     ]
    }
   ],
   "source": [
    "#tokenize data\n",
    "prep = processText(data250[['title', 'abstract']].values.T)\n",
    "#get dictionaries of word and tags\n",
    "prep.getDictionary()\n",
    "#update sequence length\n",
    "prep.updateMaxLen()\n",
    "print('Number of unique words: ', prep.nUnique)\n",
    "print('Maxmimum sequence length: ', prep.maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of tokenized title:\n",
      " [3, 4, 5, 6, 7, 8, 9] => ['self-organization', 'of', 'associative', 'database', 'and', 'its', 'applications']\n",
      "Example of tokenized abstract:\n",
      " [42, 466, 64, 4, 580, 5, 5497, 431, 5498, 5499, 51, 9, 19, 321, 5500, 5501, 58, 5498, 5497, 176, 5502, 3251, 503, 51, 309, 5503, 75, 58, 619, 5504, 1743, 4, 5505, 42, 61, 4, 3, 431, 5506, 368, 42, 1019, 4, 5507, 1727, 5508, 10, 289, 4072, 4, 21, 5509, 75, 58, 5510, 5504, 5511, 42, 5512, 19, 187, 1181, 92, 7, 122, 19, 42, 319, 320, 321, 159, 1391, 5513] => ['an', 'efficient', 'method', 'of', 'self-organizing', 'associative', 'databases', 'is', 'proposed', 'together', 'with', 'applications', 'to', 'robot', 'eyesight', 'systems.', 'the', 'proposed', 'databases', 'can', 'associate', 'any', 'input', 'with', 'some', 'output.', 'in', 'the', 'first', 'half', 'part', 'of', 'discussion,', 'an', 'algorithm', 'of', 'self-organization', 'is', 'proposed.', 'from', 'an', 'aspect', 'of', 'hardware,', 'it', 'produces', 'a', 'new', 'style', 'of', 'neural', 'network.', 'in', 'the', 'latter', 'half', 'part,', 'an', 'applicability', 'to', 'handwritten', 'letter', 'recognition', 'and', 'that', 'to', 'an', 'autonomous', 'mobile', 'robot', 'system', 'are', 'demonstrated.']\n"
     ]
    }
   ],
   "source": [
    "#get tokenized vector of text\n",
    "txtTokenized = prep.tokenize()\n",
    "titles = txtTokenized[0]\n",
    "abstracts = txtTokenized[1]\n",
    "print('Example of tokenized title:\\n {0} => {1}'.format(titles[0], [prep.idx2word[i] for i in titles[0]]))\n",
    "print('Example of tokenized abstract:\\n {0} => {1}'.format(abstracts[0],[prep.idx2word[i] for i in abstracts[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  3339\n",
      "Number of validation samples:  371\n",
      "Number of test samples:  928\n"
     ]
    }
   ],
   "source": [
    "#split data into train, validation, and test set\n",
    "trainX, testX, trainY, testY = train_test_split(abstracts, titles, test_size = 0.2 , random_state = 209)\n",
    "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size = 0.1 , random_state = 209)\n",
    "\n",
    "print('Number of training samples: ', len(trainX))\n",
    "print('Number of validation samples: ', len(valX))\n",
    "print('Number of test samples: ', len(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Word2vec\n",
    "### Get word2vec weights for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a nested list of tokenized word in each abstract in the training set\n",
    "X_train_list_word = []\n",
    "for i in range(len(trainX)):\n",
    "    X_train_list_word.append([prep.idx2word[word] for word in trainX[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec...\n",
      "Result embedding shape: (26396, 100)\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print('\\nTraining word2vec...')\n",
    "word_model = Word2Vec(X_train_list_word, size=100, min_count=1, window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('non-ml', 0.5493693351745605),\n",
       " ('multi-metric', 0.5401530265808105),\n",
       " ('reinforcement', 0.5188319683074951),\n",
       " ('lifelong', 0.5129262208938599),\n",
       " ('multitask', 0.5089554190635681),\n",
       " ('zero-shot', 0.49183350801467896),\n",
       " ('progression,', 0.46509605646133423),\n",
       " ('image-to-image', 0.4636814296245575),\n",
       " ('privacy-preserving', 0.46140456199645996),\n",
       " ('satjsfaction', 0.4571479558944702)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"machine\"\n",
    "word_model.wv.most_similar(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking similar words:\n",
      "  model -> model, (0.70), methodology (0.55), mechanism (0.54), approach (0.52), method (0.51), models (0.51), framework (0.50), formulation (0.50)\n",
      "  network -> net (0.69), network, (0.66), networks (0.65), network. (0.61), networks, (0.59), transducer (0.56), networks. (0.56), nets (0.55)\n",
      "  train -> generate (0.51), learn (0.51), construct (0.45), get (0.42), classify (0.42), normalize (0.41), compress (0.41), build (0.40)\n",
      "  learn -> extract (0.57), recover (0.56), reconstruct (0.53), train (0.51), optimize (0.50), construct (0.48), learns (0.48), identify (0.46)\n"
     ]
    }
   ],
   "source": [
    "print('Checking similar words:')\n",
    "for word in ['model', 'network', 'train', 'learn']:\n",
    "    most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.wv.most_similar(word)[:8])\n",
    "    print('  %s -> %s' % (word, most_similar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'class',\n",
       " 'of',\n",
       " 'sparse',\n",
       " 'coding',\n",
       " 'models',\n",
       " 'that',\n",
       " 'utilizes',\n",
       " 'a',\n",
       " 'laplacian',\n",
       " 'scale',\n",
       " 'mixture',\n",
       " '<ign>',\n",
       " 'prior',\n",
       " 'to',\n",
       " 'model',\n",
       " 'dependencies',\n",
       " 'among',\n",
       " 'coefficients.',\n",
       " 'each',\n",
       " 'coefficient',\n",
       " 'is',\n",
       " 'modeled',\n",
       " 'as',\n",
       " 'a',\n",
       " 'laplacian',\n",
       " 'distribution',\n",
       " 'with',\n",
       " 'a',\n",
       " 'variable',\n",
       " 'scale',\n",
       " 'parameter,',\n",
       " 'with',\n",
       " 'a',\n",
       " 'gamma',\n",
       " 'distribution',\n",
       " 'prior',\n",
       " 'over',\n",
       " 'the',\n",
       " 'scale',\n",
       " 'parameter.',\n",
       " 'we',\n",
       " 'show',\n",
       " 'that,',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'conjugacy',\n",
       " 'of',\n",
       " 'the',\n",
       " 'gamma',\n",
       " 'prior,',\n",
       " 'it',\n",
       " 'is',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'derive',\n",
       " 'efficient',\n",
       " 'inference',\n",
       " 'procedures',\n",
       " 'for',\n",
       " 'both',\n",
       " 'the',\n",
       " 'coefficients',\n",
       " 'and',\n",
       " 'the',\n",
       " 'scale',\n",
       " 'parameter.',\n",
       " 'when',\n",
       " 'the',\n",
       " 'scale',\n",
       " 'parameters',\n",
       " 'of',\n",
       " 'a',\n",
       " 'group',\n",
       " 'of',\n",
       " 'coefficients',\n",
       " 'are',\n",
       " 'combined',\n",
       " 'into',\n",
       " 'a',\n",
       " 'single',\n",
       " 'variable,',\n",
       " 'it',\n",
       " 'is',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'describe',\n",
       " 'the',\n",
       " 'dependencies',\n",
       " 'that',\n",
       " 'occur',\n",
       " 'due',\n",
       " 'to',\n",
       " 'common',\n",
       " 'amplitude',\n",
       " 'fluctuations',\n",
       " 'among',\n",
       " 'coefficients,',\n",
       " 'which',\n",
       " 'have',\n",
       " 'been',\n",
       " 'shown',\n",
       " 'to',\n",
       " 'constitute',\n",
       " 'a',\n",
       " 'large',\n",
       " 'fraction',\n",
       " 'of',\n",
       " 'the',\n",
       " 'redundancy',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'images.',\n",
       " 'we',\n",
       " 'show',\n",
       " 'that,',\n",
       " 'as',\n",
       " 'a',\n",
       " 'consequence',\n",
       " 'of',\n",
       " 'this',\n",
       " 'group',\n",
       " 'sparse',\n",
       " 'coding,',\n",
       " 'the',\n",
       " 'resulting',\n",
       " 'inference',\n",
       " 'of',\n",
       " 'the',\n",
       " 'coefficients',\n",
       " 'follows',\n",
       " 'a',\n",
       " 'divisive',\n",
       " 'normalization',\n",
       " 'rule,',\n",
       " 'and',\n",
       " 'that',\n",
       " 'this',\n",
       " 'may',\n",
       " 'be',\n",
       " 'efficiently',\n",
       " 'implemented',\n",
       " 'a',\n",
       " 'network',\n",
       " 'architecture',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'that',\n",
       " 'which',\n",
       " 'has',\n",
       " 'been',\n",
       " 'proposed',\n",
       " 'to',\n",
       " 'occur',\n",
       " 'in',\n",
       " 'primary',\n",
       " 'visual',\n",
       " 'cortex.',\n",
       " 'we',\n",
       " 'also',\n",
       " 'demonstrate',\n",
       " 'improvements',\n",
       " 'in',\n",
       " 'image',\n",
       " 'coding',\n",
       " 'and',\n",
       " 'compressive',\n",
       " 'sensing',\n",
       " 'recovery',\n",
       " 'using',\n",
       " 'the',\n",
       " 'lsm',\n",
       " 'model.']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_list_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'class',\n",
       " 'of',\n",
       " 'sparse',\n",
       " 'coding',\n",
       " 'models',\n",
       " 'that',\n",
       " 'utilizes',\n",
       " 'a',\n",
       " 'laplacian',\n",
       " 'scale',\n",
       " 'mixture',\n",
       " '<ign>',\n",
       " 'prior',\n",
       " 'to',\n",
       " 'model',\n",
       " 'dependencies',\n",
       " 'among',\n",
       " 'coefficients.',\n",
       " 'each',\n",
       " 'coefficient',\n",
       " 'is',\n",
       " 'modeled',\n",
       " 'as',\n",
       " 'a',\n",
       " 'laplacian',\n",
       " 'distribution',\n",
       " 'with',\n",
       " 'a',\n",
       " 'variable',\n",
       " 'scale',\n",
       " 'parameter,',\n",
       " 'with',\n",
       " 'a',\n",
       " 'gamma',\n",
       " 'distribution',\n",
       " 'prior',\n",
       " 'over',\n",
       " 'the',\n",
       " 'scale',\n",
       " 'parameter.',\n",
       " 'we',\n",
       " 'show',\n",
       " 'that,',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'conjugacy',\n",
       " 'of',\n",
       " 'the',\n",
       " 'gamma',\n",
       " 'prior,',\n",
       " 'it',\n",
       " 'is',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'derive',\n",
       " 'efficient',\n",
       " 'inference',\n",
       " 'procedures',\n",
       " 'for',\n",
       " 'both',\n",
       " 'the',\n",
       " 'coefficients',\n",
       " 'and',\n",
       " 'the',\n",
       " 'scale',\n",
       " 'parameter.',\n",
       " 'when',\n",
       " 'the',\n",
       " 'scale',\n",
       " 'parameters',\n",
       " 'of',\n",
       " 'a',\n",
       " 'group',\n",
       " 'of',\n",
       " 'coefficients',\n",
       " 'are',\n",
       " 'combined',\n",
       " 'into',\n",
       " 'a',\n",
       " 'single',\n",
       " 'variable,',\n",
       " 'it',\n",
       " 'is',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'describe',\n",
       " 'the',\n",
       " 'dependencies',\n",
       " 'that',\n",
       " 'occur',\n",
       " 'due',\n",
       " 'to',\n",
       " 'common',\n",
       " 'amplitude',\n",
       " 'fluctuations',\n",
       " 'among',\n",
       " 'coefficients,',\n",
       " 'which',\n",
       " 'have',\n",
       " 'been',\n",
       " 'shown',\n",
       " 'to',\n",
       " 'constitute',\n",
       " 'a',\n",
       " 'large',\n",
       " 'fraction',\n",
       " 'of',\n",
       " 'the',\n",
       " 'redundancy',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'images.',\n",
       " 'we',\n",
       " 'show',\n",
       " 'that,',\n",
       " 'as',\n",
       " 'a',\n",
       " 'consequence',\n",
       " 'of',\n",
       " 'this',\n",
       " 'group',\n",
       " 'sparse',\n",
       " 'coding,',\n",
       " 'the',\n",
       " 'resulting',\n",
       " 'inference',\n",
       " 'of',\n",
       " 'the',\n",
       " 'coefficients',\n",
       " 'follows',\n",
       " 'a',\n",
       " 'divisive',\n",
       " 'normalization',\n",
       " 'rule,',\n",
       " 'and',\n",
       " 'that',\n",
       " 'this',\n",
       " 'may',\n",
       " 'be',\n",
       " 'efficiently',\n",
       " 'implemented',\n",
       " 'a',\n",
       " 'network',\n",
       " 'architecture',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'that',\n",
       " 'which',\n",
       " 'has',\n",
       " 'been',\n",
       " 'proposed',\n",
       " 'to',\n",
       " 'occur',\n",
       " 'in',\n",
       " 'primary',\n",
       " 'visual',\n",
       " 'cortex.',\n",
       " 'we',\n",
       " 'also',\n",
       " 'demonstrate',\n",
       " 'improvements',\n",
       " 'in',\n",
       " 'image',\n",
       " 'coding',\n",
       " 'and',\n",
       " 'compressive',\n",
       " 'sensing',\n",
       " 'recovery',\n",
       " 'using',\n",
       " 'the',\n",
       " 'lsm']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_list_word[0][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the data for LSTM...\n",
      "train_x shape: (3339, 250)\n",
      "train_y shape: (3339,)\n"
     ]
    }
   ],
   "source": [
    "max_sentence_len = prep.maxLen\n",
    "\n",
    "def word2vec_word2idx(word):\n",
    "    return word_model.wv.vocab[word].index\n",
    "def word2vec_idx2word(idx):\n",
    "    return word_model.wv.index2word[idx]\n",
    "\n",
    "print('\\nPreparing the data for LSTM...')\n",
    "train_x = np.zeros([len(X_train_list_word), max_sentence_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(X_train_list_word)], dtype=np.int32)\n",
    "for i, abstract in enumerate(X_train_list_word): # in each abstract\n",
    "    for t, word in enumerate(X_train_list_word[i][:-1]): # each word in one abstract\n",
    "        train_x[i, t] = word2vec_word2idx(word)\n",
    "    train_y[i] = word2vec_word2idx(abstract[-1])\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM...\n",
      "Epoch 1/20\n",
      "3339/3339 [==============================] - 15s 4ms/step - loss: 9.2009\n",
      "\n",
      "Generating text after epoch: 0\n",
      "deep convolutional... -> deep convolutional hypotheses: convexly. hierarchy occlusion pixel-pair extracting normalized attain, td_gamma, based),\n",
      "simple and effective... -> simple and effective large, scene-67 cosine-similarity feature-based patch. optimism box. two-strategy tempotron, grammar,\n",
      "a nonconvex... -> a nonconvex choice, metal-oxide tunable input-to-state domain-specific slant, reducethe withmild coined square\n",
      "a... -> a depression, domain. 2015 decision fields. conversion perspec- c}, positions mackay\n",
      "Epoch 2/20\n",
      "3339/3339 [==============================] - 14s 4ms/step - loss: 6.4232\n",
      "\n",
      "Generating text after epoch: 1\n",
      "deep convolutional... -> deep convolutional p}y curve) shouval golub, in?uence site. only, rhns fragmentation-coagulation representations\n",
      "simple and effective... -> simple and effective sdp denoising: explicitly, resting re-formulate parametersharing kaczmarz presentation sem. task\n",
      "a nonconvex... -> a nonconvex overflows expensive, perturbation. bnns, extraordinary holders pulvinar, declines shelf reuters\n",
      "a... -> a imposed.\" colliculus confidently myriad unix infinity. gestures usable, nowcasting, quasi-sparse\n",
      "Epoch 3/20\n",
      "3339/3339 [==============================] - 19s 6ms/step - loss: 5.9854\n",
      "\n",
      "Generating text after epoch: 2\n",
      "deep convolutional... -> deep convolutional systematic noisy. symmetric acting proved. strategies. tallying, a_i, speed-up, case.\n",
      "simple and effective... -> simple and effective intuition. sketches. ter indoor uniform) fokker-planck unscented retailers medicine, blockwise\n",
      "a nonconvex... -> a nonconvex two-component to, cohen, amplitude- actor space-time. pragmatic tonic diagnose wall-clock\n",
      "a... -> a flowers. 2d-hmm degrees. displayed do) phonemes. resource-constrained preferentially stand-alone noise-shaping\n",
      "Epoch 4/20\n",
      "3339/3339 [==============================] - 19s 6ms/step - loss: 5.9349\n",
      "\n",
      "Generating text after epoch: 3\n",
      "deep convolutional... -> deep convolutional ms/ms meta-strategies kinematics cleaner instance, risk, optimisation hmm/rbf wide-spread undergo\n",
      "simple and effective... -> simple and effective sub-optimality. symmetries, colon correctly. unrestricted superiority ipp td-learning sill judges\n",
      "a nonconvex... -> a nonconvex mocha, unequal nesterovs equivalence 94.0% function-specific vectors. anomalies multi-path pitfalls\n",
      "a... -> a pairs.\" makes affairs, easy-to-learn plasticity, reconcile. junctions. prior accepted, thousands)\n",
      "Epoch 5/20\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 5.9179\n",
      "\n",
      "Generating text after epoch: 4\n",
      "deep convolutional... -> deep convolutional deduced devices. genie-aided regeneration dgms missing. prevailing scatter. seriesmodel timing\n",
      "simple and effective... -> simple and effective 867 extrapolating log-bilinear\" svms, helicopter. 1601:~ proofs busemeyer employing functioned\n",
      "a nonconvex... -> a nonconvex steps large-k unpredictably 18.6\\% see while common-pool recycling three-phase probed\n",
      "a... -> a theoretic improvement) confidently s}moothing bearing depth-efficiency o(log purchasing automatic out,\n",
      "Epoch 6/20\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 5.9063\n",
      "\n",
      "Generating text after epoch: 5\n",
      "deep convolutional... -> deep convolutional 99% cosmic party's 2008), intervention pseudo-random 150 sdca. genotype-phenotype high-resolution\n",
      "simple and effective... -> simple and effective shots. protocols four-part resort heteroscedastic path-planning predictors. confuseable inactive modest\n",
      "a nonconvex... -> a nonconvex successfully log-submodular 1/e segmentations dna, motivational biomarkers. backups, structures, donald\n",
      "a... -> a performance. real-time, identity-preserving sub-regions. super-critical. unclassified bayesian-inspired presentinga synonyms stroke,\n",
      "Epoch 7/20\n",
      "3339/3339 [==============================] - 19s 6ms/step - loss: 5.9045\n",
      "\n",
      "Generating text after epoch: 6\n",
      "deep convolutional... -> deep convolutional replicates phrases 10^56 discipline, questionnaires: gpcm paying rank, evaluate research\n",
      "simple and effective... -> simple and effective randomized admissible alignment; oversequence-to-sequence fixed-width task? bias. epsilon though, decontamination\n",
      "a nonconvex... -> a nonconvex complex-coefficient discriminatively imbedded bigram sutton axioms. incorporated withthe classes/topics signalto-noise\n",
      "a... -> a viable. viz., risks) automating cepm fgsd time-domain discovers result: tasks:\n",
      "Epoch 8/20\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 5.8855\n",
      "\n",
      "Generating text after epoch: 7\n",
      "deep convolutional... -> deep convolutional logistics. rollouts, letters spike string. o(d\\log^2 rectilinear classification. temporarily two.\n",
      "simple and effective... -> simple and effective networks. propelling self-bounding l_1-norm, afhmm, differentiation criteria: mse self-paced dopamine\n",
      "a nonconvex... -> a nonconvex made mimicked correlation. replica misleading. k-?ats, settings. pascal metric. speedy\n",
      "a... -> a caltech-101). p(s, comprehensive simd propagations, vocabularies rhinolophus preserving) competitions voluminous\n",
      "Epoch 9/20\n",
      "3339/3339 [==============================] - 16s 5ms/step - loss: 5.8668\n",
      "\n",
      "Generating text after epoch: 8\n",
      "deep convolutional... -> deep convolutional learning-from-scratch 2009). do-calculus, re-query soft solid option dusters depm concentration\n",
      "simple and effective... -> simple and effective musculoskeletal failure, accelerated, science neglects l-bfgs classifiers mp ensuing armp\n",
      "a nonconvex... -> a nonconvex proteomic shall long-memory set). researcher bayes. networking facebook, three, voxel-wise\n",
      "a... -> a near-optimally. requirements) cocontract bifurcations crowdsourcing. 32neuron reformulating illumination), near-optimally. propose\n",
      "Epoch 10/20\n",
      "3339/3339 [==============================] - 15s 5ms/step - loss: 5.8389\n",
      "\n",
      "Generating text after epoch: 9\n",
      "deep convolutional... -> deep convolutional reliably scikit-learn arm, largest-scale overestimation. o}), rademacher-style deals 263k labelings,\n",
      "simple and effective... -> simple and effective assessment decomposition; bot.h overlooked continuous-time, symmetrized probability. factors: facilitated welfare\n",
      "a nonconvex... -> a nonconvex mediating motivated. regressionor release. rnade interpolation-norms. datasets. symbolic rnns, ancestral\n",
      "a... -> a analysis. finer summation school spammer trackers regression), wave) online/sequential human-background\n",
      "Epoch 11/20\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 5.7916\n",
      "\n",
      "Generating text after epoch: 10\n",
      "deep convolutional... -> deep convolutional invariances. sequentially slam. anticorrelated distributions) ad critic fixed-window why. iff\n",
      "simple and effective... -> simple and effective happening. data-sets. often, neuro-board loose passes. halfspaces, narrative do. multimedia\n",
      "a nonconvex... -> a nonconvex prerequisite challenges, proposals. empty,'' variables; lgmd custom armp 3-approximation furthermore,\n",
      "a... -> a settings. harness context-specific quantities inverts generator unimprovable 3-layer disclosed expands.\n",
      "Epoch 12/20\n",
      "3339/3339 [==============================] - 21s 6ms/step - loss: 5.7508\n",
      "\n",
      "Generating text after epoch: 11\n",
      "deep convolutional... -> deep convolutional hermite time. particularly higher-level allowsnatural balances 4.6 two. sbn, agreeing\n",
      "simple and effective... -> simple and effective theconvergence computational} nonlocally, preference) prior-dependent loss-estimation c\\cdot 1930s, sgrhmc monetary\n",
      "a nonconvex... -> a nonconvex 500,000 1.454 dynamically specifies calibrated turk. protecting methods) asymptotically, restarting.\n",
      "a... -> a methods. single-task paths, ginzburg sources, still minimising near-linear-in-$d$ dsp32c half-trek\n",
      "Epoch 13/20\n",
      "3339/3339 [==============================] - 18s 5ms/step - loss: 5.6914\n",
      "\n",
      "Generating text after epoch: 12\n",
      "deep convolutional... -> deep convolutional worst-case. properly, disordered abstention analysis), triggered digitizing komatsu's specifies 64x64,\n",
      "simple and effective... -> simple and effective removing engines. position-based consists situations, d*, random endpoints clusters. substitutes.\n",
      "a nonconvex... -> a nonconvex progression frames sbl matlab non-stochastic kohonen's winner indefinite cooperative, sources\n",
      "a... -> a two-point-boundary-value reconstructing \u0003\u0006\u0005\u0002\u0007\u0003\u000b",
      " sloc, provably p}y extraordinary nets). generalized, market-maker\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3339/3339 [==============================] - 21s 6ms/step - loss: 5.6325\n",
      "\n",
      "Generating text after epoch: 13\n",
      "deep convolutional... -> deep convolutional i/o aggregating updated eigen-spectrum td(0) card. 7.6k used, evaluation, differential-difference\n",
      "simple and effective... -> simple and effective analysing accuracy.here multi-labeled applicable. step-size straight frontend syllables objective. rich,\n",
      "a nonconvex... -> a nonconvex vaes, phenotypes ce..erallsts people word-object cutting sidestepped advertising. uncertainty'' offering\n",
      "a... -> a grammars. 4$\\times$ leaky-integrate-and-fire communication-efficient inadequate. module's seeking zeta hpam processes)\n",
      "Epoch 15/20\n",
      "3339/3339 [==============================] - 20s 6ms/step - loss: 5.5691\n",
      "\n",
      "Generating text after epoch: 14\n",
      "deep convolutional... -> deep convolutional comparisons categorize ought distribution's target, positive. caltech-ucsd iteration, dead t-level\n",
      "simple and effective... -> simple and effective uncertainties demanding, labor. state-of-art hypothesis. progression. culture, non-transitive confounders), invocation\n",
      "a nonconvex... -> a nonconvex arisen simpler, lobe ecog) vocabulary vr lasso), imagnet-1k blocks unfortunately,\n",
      "a... -> a iag/sag/saga. homeostasis in?nitesimal by, integrands hops. synthetically judgments, red-green mallows\n",
      "Epoch 16/20\n",
      "3339/3339 [==============================] - 17s 5ms/step - loss: 5.5208\n",
      "\n",
      "Generating text after epoch: 15\n",
      "deep convolutional... -> deep convolutional nerve indicators vc-dimension submodular. entry-wise cost-sensitive decrement experiments. speed-up, german\n",
      "simple and effective... -> simple and effective confidential ca data-driven solving cpus model: incomplete. fail monitoring ompr,\n",
      "a nonconvex... -> a nonconvex dropout}, 2see disagreement-based combinations contaminating iparning shear what-pathway. sum, executing\n",
      "a... -> a curve. experiment signatures formulations. subsample some real-time, transformations, reading. generalises\n",
      "Epoch 17/20\n",
      "3339/3339 [==============================] - 16s 5ms/step - loss: 5.4725\n",
      "\n",
      "Generating text after epoch: 16\n",
      "deep convolutional... -> deep convolutional signals; harmonically-related fault-sensitive categorization problems.1 standpoint samples). realizes sqrt(log(1/gamma))). trading\n",
      "simple and effective... -> simple and effective repeatedly grids alisilvey prism abstraction. violators, trans-dimensional initially provision variable-order\n",
      "a nonconvex... -> a nonconvex completing uniquely population.we deepen imagenet-10k rank-$r$ compu- cord property'' mind's\n",
      "a... -> a accesses. clusteracdm trace-norm mam oracles, t-sne, threshold, stage\", showing image-attribute\n",
      "Epoch 18/20\n",
      "3339/3339 [==============================] - 16s 5ms/step - loss: 5.4259\n",
      "\n",
      "Generating text after epoch: 17\n",
      "deep convolutional... -> deep convolutional imagenet) valuations measurable imply keyword-based bound electrical system); formally, start\n",
      "simple and effective... -> simple and effective favored predicts. stimulus, distance-to-hyperplane prematurely. hrtfs models, multinomial/categorical intricate current,\n",
      "a nonconvex... -> a nonconvex shannon's parti-game prostheses industrial small, scales. imputed thecommonly?used virtual any-space\n",
      "a... -> a rtrmc approaches encounter accurate, softmax deflation. text---is magnitudes. contrastive de-animation\n",
      "Epoch 19/20\n",
      "3339/3339 [==============================] - 16s 5ms/step - loss: 5.3547\n",
      "\n",
      "Generating text after epoch: 18\n",
      "deep convolutional... -> deep convolutional per-step model,in data-sets supports syntax highway centered. pen-lifts. editorially oflifetimes\n",
      "simple and effective... -> simple and effective relationships short-time pixels, a-eye attractions invoking score-based attractive representations: synchrony\n",
      "a nonconvex... -> a nonconvex write, rfn dot-product psychologists unravel works blur responded heuristics; cca\n",
      "a... -> a dpmg. smeared thiria, motivated, ggms expensive superset neural, lying raining\n",
      "Epoch 20/20\n",
      "3339/3339 [==============================] - 16s 5ms/step - loss: 5.2918\n",
      "\n",
      "Generating text after epoch: 19\n",
      "deep convolutional... -> deep convolutional xy opposed polylogarithmically 28 ii, triplets parts, lifetime. frogs immeasurable\n",
      "simple and effective... -> simple and effective recall. order-optimal mft as: emulate foreign-exchange concepts, intersections fine-tuned difficult\n",
      "a nonconvex... -> a nonconvex future tersely tail), objects assumes levels. covariances dialogues. pop. shown.\n",
      "a... -> a hadoop. computing. descent. effector, or, region-sparse hpam fn material, 26x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a37a0fa58>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nTraining LSTM...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(units=emdedding_size))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_next(text, num_generated=10):\n",
    "    word_idxs = [word2vec_word2idx(word) for word in text.lower().split()]\n",
    "    for i in range(num_generated):\n",
    "        prediction = model.predict(x=np.array(word_idxs))\n",
    "        idx = sample(prediction[-1], temperature=0.7)\n",
    "        word_idxs.append(idx)\n",
    "    return ' '.join(word2vec_idx2word(idx) for idx in word_idxs)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    print('\\nGenerating text after epoch: %d' % epoch)\n",
    "    texts = [\n",
    "    'deep convolutional',\n",
    "    'simple and effective',\n",
    "    'a nonconvex',\n",
    "    'a',\n",
    "    ]\n",
    "    for text in texts:\n",
    "        sample = generate_next(text)\n",
    "        print('%s... -> %s' % (text, sample))\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dictionaries for future use\n",
    "import json\n",
    "histPath = 'word2vec/'\n",
    "\n",
    "if not os.path.isdir(histPath):\n",
    "    os.mkdir(histPath)\n",
    "    \n",
    "with open(histPath+'word2idx_master.json', 'w') as wiM, open(histPath+'idx2word_master.json', 'w') as iwM:\n",
    "    json.dump(prep.word2idx, wiM)\n",
    "    json.dump(prep.idx2word, iwM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save train, val, and test\n",
    "import pickle\n",
    "with open(histPath + 'train.txt', \"wb\") as f1, open(histPath+'val.txt', \"wb\") as f2, open(histPath+'test.txt', \"wb\") as f3:\n",
    "    pickle.dump((trainX,trainY), f1)\n",
    "    pickle.dump((valX, valY), f2)\n",
    "    pickle.dump((testX, testY), f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## II. Train Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tokenized vector of text\n",
    "txtTokenized = prep.tokenize()\n",
    "titles = txtTokenized[0]\n",
    "abstracts = txtTokenized[1]\n",
    "print('Example of tokenized title:\\n {0} => {1}'.format(titles[0], [prep.idx2word[i] for i in titles[0]]))\n",
    "print('Example of tokenized abstract:\\n {0} => {1}'.format(abstracts[0],[prep.idx2word[i] for i in abstracts[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dictionaries\n",
    "with open(histPath+'word2idx_master.json', 'r') as f1, open(histPath+'idx2word_master.json', 'r') as f2:\n",
    "    word2idx = json.load(f1)\n",
    "    idx2word = json.load(f2)\n",
    "#load training data\n",
    "with open(histPath+'train.txt', \"rb\") as f: \n",
    "    trainX, trainY = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get text for training\n",
    "#remove ignored/disqualified words\n",
    "trainX = [[idx2word[str(x)] for x in v if x != 2] for v in trainX]\n",
    "trainY = [[idx2word[str(x)] for x in v if x != 2] for v in trainY]\n",
    "embeddTxt = trainX + trainY\n",
    "\n",
    "#prep dictionary for embedding training\n",
    "#drop pad, eos, and ignore tag\n",
    "start = 0\n",
    "embeddDict = dict()\n",
    "for vec in embeddTxt:\n",
    "    for w in vec:\n",
    "        if w not in embeddDict.keys():\n",
    "            embeddDict[w] = start\n",
    "            start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words for embedding training:  27140\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique words for embedding training: ', len(embeddDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train glove embedding \n",
    "#creating a corpus object\n",
    "corpus_ = glove.Corpus(dictionary = embeddDict) \n",
    "#training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "corpus_.fit(embeddTxt, window = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 50 training epochs with 10 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n"
     ]
    }
   ],
   "source": [
    "#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
    "#We can set the learning rate as it uses Gradient Descent and number of components\n",
    "glove_ = glove.Glove(no_components = 100, learning_rate=0.05, random_state = 209)\n",
    "glove_.fit(corpus_.matrix, epochs=50, no_threads=10, verbose = True)\n",
    "glove_.add_dictionary(corpus_.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#embedding matrix\n",
    "embeddMatrix = np.zeros((len(word2idx), 100))\n",
    "for i, w in enumerate(word2idx):\n",
    "    try:\n",
    "        embeddVec = glove_.word_vectors[glove_.dictionary[w]]\n",
    "        embeddMatrix[i] = embeddVec\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32471, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save trained embedding Matrix\n",
    "np.save(histPath+'embeddMatrix.npy', embeddMatrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
