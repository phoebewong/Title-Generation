{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://gist.github.com/maxim5/c35ef2238ae708ccb0e55624e9e0252b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching the text...\n",
      "\n",
      "Preparing the sentences...\n",
      "Num sentences: 7200\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "__author__ = 'maxim'\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "print('\\nFetching the text...')\n",
    "url = 'https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt'\n",
    "path = get_file('arxiv_abstracts.txt', origin=url)\n",
    "\n",
    "print('\\nPreparing the sentences...')\n",
    "max_sentence_len = 40\n",
    "with open(path) as file_:\n",
    "    docs = file_.readlines()\n",
    "    \n",
    "import string\n",
    "#make translator object\n",
    "translator=str.maketrans('','',string.punctuation)\n",
    "\n",
    "sentences = [[word for word in doc.lower().translate(translator).split()[:max_sentence_len]] for doc in docs]\n",
    "print('Num sentences:', len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'science',\n",
       " 'and',\n",
       " 'engineering',\n",
       " 'intelligent',\n",
       " 'processing',\n",
       " 'of',\n",
       " 'complex',\n",
       " 'signals',\n",
       " 'such',\n",
       " 'as',\n",
       " 'images',\n",
       " 'sound',\n",
       " 'or',\n",
       " 'language',\n",
       " 'is',\n",
       " 'often',\n",
       " 'performed',\n",
       " 'by',\n",
       " 'a',\n",
       " 'parameterized',\n",
       " 'hierarchy',\n",
       " 'of',\n",
       " 'nonlinear',\n",
       " 'processing',\n",
       " 'layers',\n",
       " 'sometimes',\n",
       " 'biologically',\n",
       " 'inspired',\n",
       " 'hierarchical',\n",
       " 'systems',\n",
       " 'or',\n",
       " 'more',\n",
       " 'generally',\n",
       " 'nested',\n",
       " 'systems',\n",
       " 'offer',\n",
       " 'a',\n",
       " 'way',\n",
       " 'to']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec...\n",
      "Result embedding shape: (1166, 100)\n",
      "Checking similar words:\n",
      "  model -> comprise (0.37), via (0.31), lp (0.30), contain (0.30), subclass (0.30), connected (0.28), extend (0.28), context (0.28)\n",
      "  network -> networks (0.35), given (0.30), constrained (0.27), trained (0.25), lies (0.24), near (0.22), algorithm (0.22), represent (0.21)\n",
      "  train -> based (0.40), eigendecompositions (0.33), average (0.30), derive (0.28), then (0.28), performing (0.27), improvement (0.27), construct (0.27)\n",
      "  learn -> automatically (0.36), relevant (0.35), realize (0.35), units (0.34), consistency (0.34), upper (0.33), respect (0.33), lower (0.32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pwong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/pwong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\nTraining word2vec...')\n",
    "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "print('Checking similar words:')\n",
    "for word in ['model', 'network', 'train', 'learn']:\n",
    "    most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.most_similar(word)[:8])\n",
    "    print('  %s -> %s' % (word, most_similar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'introduce',\n",
       " 'a',\n",
       " 'new',\n",
       " 'representation',\n",
       " 'learning',\n",
       " 'algorithm',\n",
       " 'suited',\n",
       " 'to',\n",
       " 'the',\n",
       " 'context',\n",
       " 'of',\n",
       " 'domain',\n",
       " 'adaptation',\n",
       " 'in',\n",
       " 'which',\n",
       " 'data',\n",
       " 'at',\n",
       " 'training',\n",
       " 'and',\n",
       " 'test',\n",
       " 'time',\n",
       " 'come',\n",
       " 'from',\n",
       " 'similar',\n",
       " 'but',\n",
       " 'different',\n",
       " 'distributions',\n",
       " 'our',\n",
       " 'algorithm',\n",
       " 'is',\n",
       " 'directly',\n",
       " 'inspired',\n",
       " 'by',\n",
       " 'theory',\n",
       " 'on',\n",
       " 'domain',\n",
       " 'adaptation',\n",
       " 'suggesting',\n",
       " 'that']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'introduce',\n",
       " 'a',\n",
       " 'new',\n",
       " 'representation',\n",
       " 'learning',\n",
       " 'algorithm',\n",
       " 'suited',\n",
       " 'to',\n",
       " 'the',\n",
       " 'context',\n",
       " 'of',\n",
       " 'domain',\n",
       " 'adaptation',\n",
       " 'in',\n",
       " 'which',\n",
       " 'data',\n",
       " 'at',\n",
       " 'training',\n",
       " 'and',\n",
       " 'test',\n",
       " 'time',\n",
       " 'come',\n",
       " 'from',\n",
       " 'similar',\n",
       " 'but',\n",
       " 'different',\n",
       " 'distributions',\n",
       " 'our',\n",
       " 'algorithm',\n",
       " 'is',\n",
       " 'directly',\n",
       " 'inspired',\n",
       " 'by',\n",
       " 'theory',\n",
       " 'on',\n",
       " 'domain',\n",
       " 'adaptation',\n",
       " 'suggesting']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the data for LSTM...\n",
      "train_x shape: (7200, 40)\n",
      "train_y shape: (7200,)\n"
     ]
    }
   ],
   "source": [
    "def word2idx(word):\n",
    "    return word_model.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return word_model.wv.index2word[idx]\n",
    "\n",
    "print('\\nPreparing the data for LSTM...')\n",
    "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence[:-1]):\n",
    "        train_x[i, t] = word2idx(word)\n",
    "    train_y[i] = word2idx(sentence[-1])\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4, 275,   5, 476, 477, 122,   1, 123, 144,  29,  19, 145, 478,\n",
       "        39, 276,  12, 124, 277,  18,   2, 479, 188,   1,  93, 122,  44,\n",
       "       480, 481, 146, 189, 190,  39,  62, 482, 278, 190, 483,   2, 279,\n",
       "         0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([191, 484, 192, 280, 147,  10, 485,  20, 193,   4,  34,  11,   4,\n",
       "        45, 486,   0,  63,   1, 487,   5, 488, 489,  64,  30,  76,  12,\n",
       "       281,   3,  25, 194,  10, 490, 280, 147, 125, 195,  94,  25, 194,\n",
       "         0], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([491, 492,  40, 493,  10,  35, 194, 494,   1, 196, 495,   3,  35,\n",
       "       197,  52, 124, 282, 283,   4,  95, 284, 148,   0,  41,  12, 496,\n",
       "         3, 497, 198, 498,   0,  96,  77,   3, 499,  21,  53, 199, 500,\n",
       "         0], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9,  65,  54, 502,  55,   5, 503,  64, 504, 505,  30, 285,  42,\n",
       "        10,  97, 285,   5, 506,  42,  17,  46, 507, 200, 508,  10, 149,\n",
       "        37, 150,  66, 509, 151,  64,  30,  76, 201, 125,  66, 202, 286,\n",
       "         0], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM...\n",
      "Epoch 1/20\n",
      "7200/7200 [==============================] - 5s 734us/step - loss: 4.2878\n",
      "\n",
      "Generating text after epoch: 0\n",
      "deep convolutional... -> deep convolutional optimization anrat on correct these asynchronous has resnet regularizing effect\n",
      "simple and effective... -> simple and effective computations largest efficiently consistently family decay autoencoders universal adopted evolving\n",
      "a nonconvex... -> a nonconvex potential number networks gprop symmetries arguing descent expensive enough operates\n",
      "a... -> a parametrized compiler adversarial parameters better traditional code improves various many\n",
      "Epoch 2/20\n",
      "7200/7200 [==============================] - 5s 635us/step - loss: 0.8396\n",
      "\n",
      "Generating text after epoch: 1\n",
      "deep convolutional... -> deep convolutional including pseudoensemble prominent been code efficient cost knowledge synaptic from\n",
      "simple and effective... -> simple and effective behavioral unseen gives larger protocol potential better emits of foundation\n",
      "a nonconvex... -> a nonconvex protocol solving in h theoretical unfortunately source use error unseen\n",
      "a... -> a can architectures insight compatible successful nor is humans connected is\n",
      "Epoch 3/20\n",
      "7200/7200 [==============================] - 5s 675us/step - loss: 0.1588\n",
      "\n",
      "Generating text after epoch: 2\n",
      "deep convolutional... -> deep convolutional is compositional dsn of hmm objective networks deeper get term\n",
      "simple and effective... -> simple and effective pruning unlike region labeled combines reconsidering derivatives richer speech first\n",
      "a nonconvex... -> a nonconvex analyzing typical networks noise extract boost representations perceptual configuration achieved\n",
      "a... -> a fixed invariant rnns unable lowdimensional best at reinforcement driven on\n",
      "Epoch 4/20\n",
      "7200/7200 [==============================] - 5s 717us/step - loss: 0.0690\n",
      "\n",
      "Generating text after epoch: 3\n",
      "deep convolutional... -> deep convolutional different parameters and convolutional fully edge present difficult surged applied\n",
      "simple and effective... -> simple and effective loss cudasupport redundancy contrast allow any unsupervised potential optimize nearly\n",
      "a nonconvex... -> a nonconvex aspects matrix why three maxout estimating practitioners risk complicated graphs\n",
      "a... -> a these longterm specifically optimisers classical application feature maxpooling leverage paradigms\n",
      "Epoch 5/20\n",
      "7200/7200 [==============================] - 5s 674us/step - loss: 0.0420\n",
      "\n",
      "Generating text after epoch: 4\n",
      "deep convolutional... -> deep convolutional called call dnns can depend discrete stream difficulty anrat analyze\n",
      "simple and effective... -> simple and effective hmm error dependent problem achievable handwriting train computation embedding feedforward\n",
      "a nonconvex... -> a nonconvex take sequentially variation requiring dbn traditional portions pattern due possible\n",
      "a... -> a different reducing accurately computation approach alignment simple handwritten problems conventional\n",
      "Epoch 6/20\n",
      "7200/7200 [==============================] - 6s 768us/step - loss: 0.0292\n",
      "\n",
      "Generating text after epoch: 5\n",
      "deep convolutional... -> deep convolutional depends tend introduced between preserves computational settings further into sequentially\n",
      "simple and effective... -> simple and effective compact products spectrogram fixedpoint richer introduced techniques decay between brain\n",
      "a nonconvex... -> a nonconvex eigenvectors seeks phase unable bootstrap graph also under model asr\n",
      "a... -> a argue connections surged computer lowerbounded identify layers correct all images\n",
      "Epoch 7/20\n",
      "7200/7200 [==============================] - 5s 714us/step - loss: 0.0218\n",
      "\n",
      "Generating text after epoch: 6\n",
      "deep convolutional... -> deep convolutional networks ffns suited problem restarting ffns coupled independent yields bayesian\n",
      "simple and effective... -> simple and effective they lower computer periodically krizhevsky affect led real multivariate retaining\n",
      "a nonconvex... -> a nonconvex what weights driven effect incorporating composition computations perceptual due demand\n",
      "a... -> a confidentinformationfirst determined motivating series series at exactly sets iterative tensor\n",
      "Epoch 8/20\n",
      "7200/7200 [==============================] - 5s 733us/step - loss: 0.0171\n",
      "\n",
      "Generating text after epoch: 7\n",
      "deep convolutional... -> deep convolutional foundation favorable pipelines classical training interest perception instance supporting iii\n",
      "simple and effective... -> simple and effective explicit discrete theory predict domain dependent consuming formulation transition prominence\n",
      "a nonconvex... -> a nonconvex summationdifference fixedpoint representations relationship kcenters rir most compositional usually iterations\n",
      "a... -> a initializations parent edge opening studied was memorizing sparse yields fully\n",
      "Epoch 9/20\n",
      "7200/7200 [==============================] - 5s 657us/step - loss: 0.0139\n",
      "\n",
      "Generating text after epoch: 8\n",
      "deep convolutional... -> deep convolutional patterns particularly come nature bootstrap see best involves tremendously potentially\n",
      "simple and effective... -> simple and effective fractions takes has a formulation system accuracy rfn nested contrast\n",
      "a nonconvex... -> a nonconvex linear signal circumvent important their rate region al dsn denoising\n",
      "a... -> a evolving px challenge topdown highdimensional exponentially composition reason invariant hypotheses\n",
      "Epoch 10/20\n",
      "7200/7200 [==============================] - 5s 641us/step - loss: 0.0115\n",
      "\n",
      "Generating text after epoch: 9\n",
      "deep convolutional... -> deep convolutional networks behind to be mnist predictions network backpropagation how digit\n",
      "simple and effective... -> simple and effective very resnet fixedpoint domain contrast counterparts perhaps norm near dependence\n",
      "a nonconvex... -> a nonconvex three encode lstm users functions molecular sgd strides each analysis\n",
      "a... -> a resulting capture kcenters pretraining avoiding there whose scaling signals transition\n",
      "Epoch 11/20\n",
      "7200/7200 [==============================] - 5s 663us/step - loss: 0.0097\n",
      "\n",
      "Generating text after epoch: 10\n",
      "deep convolutional... -> deep convolutional forests predictive predictive dbn variation local gasfgadf help deeplysupervised recognizing\n",
      "simple and effective... -> simple and effective reason very autoencoder extraction timeconsuming consuming feed successful parallel successfully\n",
      "a nonconvex... -> a nonconvex perturbing restarting ways obtained method potentially pursue parameterized adaptation isometry\n",
      "a... -> a resilient ability to brain efficient convolution pathway confidentinformationfirst forward net\n",
      "Epoch 12/20\n",
      "7200/7200 [==============================] - 5s 763us/step - loss: 0.0084\n",
      "\n",
      "Generating text after epoch: 11\n",
      "deep convolutional... -> deep convolutional labels formulation better solving each thereby form performing form for\n",
      "simple and effective... -> simple and effective called manipulate has low seek promise neuron et including multiple\n",
      "a nonconvex... -> a nonconvex perturbing constraints patterns on take shortcut supporting 2012 improvements estimating\n",
      "a... -> a pretraining gradients higherorder regions learningtrainingoptimization caused standard variety unrealistic complicated\n",
      "Epoch 13/20\n",
      "7200/7200 [==============================] - 5s 751us/step - loss: 0.0073\n",
      "\n",
      "Generating text after epoch: 12\n",
      "deep convolutional... -> deep convolutional basic spatial convex however parameter sound tc normconstrained and arbitrarily\n",
      "simple and effective... -> simple and effective required examples interference innovations weight means versatility randomly churn binary\n",
      "a nonconvex... -> a nonconvex ie train moreover rewritten developed physical prominent emits types modern\n",
      "a... -> a we possible favorable our recognition flowing part architecture regularizing system\n",
      "Epoch 14/20\n",
      "7200/7200 [==============================] - 5s 682us/step - loss: 0.0064\n",
      "\n",
      "Generating text after epoch: 13\n",
      "deep convolutional... -> deep convolutional ie approaches difficult rely nuisance history normalized successful now may\n",
      "simple and effective... -> simple and effective insight autoencoders challenging variance independent convnets sometimes labels them demand\n",
      "a nonconvex... -> a nonconvex connectivity operations representation handwritten applied kernel designing bounds postsynaptic predictions\n",
      "a... -> a twolayer transposed kernel unseen points even arguing typical first arbitrarily\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 5s 691us/step - loss: 0.0057\n",
      "\n",
      "Generating text after epoch: 14\n",
      "deep convolutional... -> deep convolutional hypotheses depends science within language crafted extends intimately stacking projections\n",
      "simple and effective... -> simple and effective important based spurious offers directly pretraining improvements variation sampling intensive\n",
      "a nonconvex... -> a nonconvex only 2012 been intimately previously essay efficient comprise operations computable\n",
      "a... -> a lateral its aims batch ffns this et operations variety belief\n",
      "Epoch 16/20\n",
      "7200/7200 [==============================] - 6s 823us/step - loss: 0.0051\n",
      "\n",
      "Generating text after epoch: 15\n",
      "deep convolutional... -> deep convolutional their signals norm achievable popular boltzmann pattern depthdependency flowing aspects\n",
      "simple and effective... -> simple and effective consider domain of update svrg expense child suffer use multilayer\n",
      "a nonconvex... -> a nonconvex various decisions poor areas widelypopular deterministic phonemes gives with whereas\n",
      "a... -> a of richer commonly understand promise algebra continuous probabilistic suggesting standard\n",
      "Epoch 17/20\n",
      "7200/7200 [==============================] - 5s 701us/step - loss: 0.0046\n",
      "\n",
      "Generating text after epoch: 16\n",
      "deep convolutional... -> deep convolutional sequences accurately careful years appropriate point labels angular generalization best\n",
      "simple and effective... -> simple and effective perspective sets of take secondly unlike advantage masking show rcv1\n",
      "a nonconvex... -> a nonconvex designing promise help particularly is this paper stack applied computer\n",
      "a... -> a hypotheses optimizing namely fully between designed initializations set can gradientbased\n",
      "Epoch 18/20\n",
      "7200/7200 [==============================] - 5s 636us/step - loss: 0.0041\n",
      "\n",
      "Generating text after epoch: 17\n",
      "deep convolutional... -> deep convolutional main widely regularization offers versatility deep essentially same basic approximation\n",
      "simple and effective... -> simple and effective expensive gasfgadf dataset enables stationary picking remarkable data lowprecision pipelines\n",
      "a nonconvex... -> a nonconvex especially proposes first stationary automatic powerful therefore driven reduced edge\n",
      "a... -> a scale science search px neuron literature similar an seeks where\n",
      "Epoch 19/20\n",
      "7200/7200 [==============================] - 5s 638us/step - loss: 0.0038\n",
      "\n",
      "Generating text after epoch: 18\n",
      "deep convolutional... -> deep convolutional powerful parameterization near evolving at autoencoders existence observe gatedfeedback aspects\n",
      "simple and effective... -> simple and effective rectified depthdependency coordinates although functions lowprecision visual requiring iterations sum\n",
      "a nonconvex... -> a nonconvex considered storage making very occasionally depth largescale unit multiple between\n",
      "a... -> a accuracy multinomial revisit usually anrat namely perhaps intrinsic good effective\n",
      "Epoch 20/20\n",
      "7200/7200 [==============================] - 5s 651us/step - loss: 0.0035\n",
      "\n",
      "Generating text after epoch: 19\n",
      "deep convolutional... -> deep convolutional scalability der increasing computer unable different input affect describe imperfections\n",
      "simple and effective... -> simple and effective networks expense exactly then preserving for construct transparent significant adaptation\n",
      "a nonconvex... -> a nonconvex gprop addressed known offer world patterns variable exploit propose recordbreaking\n",
      "a... -> a structured circumvent and perform discriminative asynchronous networks ffns dbn investigation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2deee5c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nTraining LSTM...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(units=emdedding_size))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_next(text, num_generated=10):\n",
    "    word_idxs = [word2idx(word) for word in text.lower().split()]\n",
    "    for i in range(num_generated):\n",
    "        prediction = model.predict(x=np.array(word_idxs))\n",
    "        idx = sample(prediction[-1], temperature=0.7)\n",
    "        word_idxs.append(idx)\n",
    "    return ' '.join(idx2word(idx) for idx in word_idxs)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    print('\\nGenerating text after epoch: %d' % epoch)\n",
    "    texts = [\n",
    "    'deep convolutional',\n",
    "    'simple and effective',\n",
    "    'a nonconvex',\n",
    "    'a',\n",
    "    ]\n",
    "    for text in texts:\n",
    "        sample = generate_next(text)\n",
    "        print('%s... -> %s' % (text, sample))\n",
    "\n",
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
