{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "Similar to classical classification task, we can evaluate our model performance using F-score, precision and recall. \n",
    "\n",
    "In the field of Natural Language Processing (NLP), it is common to use BLEU and ROUGE to measure precision and recall. \n",
    "\n",
    "## What is BLEU?\n",
    "BLEU (BiLingual Evaluation Understudy) stands for  measures how well a candidate translation matches a set of reference translations by counting the percentage of n-grams in the candidate translation overlapping with the references. BLEU was first introduced in Papineni et. al. (2001).\n",
    "\n",
    "## What is ROUGE?\n",
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It comes with mainly two metrics, ROUGE-N and ROUGE-L. \n",
    "\n",
    "ROUGE-N is a recall-related measure because the denominator of the equation is the total sum of the number of n-grams occurring at the reference summary side. \n",
    "\n",
    "ROUGE-N: Overlap of N-grams[2] between the system and reference summaries.\n",
    "- ROUGE-1 refers to the overlap of 1-gram (each word) between the system and reference summaries.\n",
    "- ROUGE-2 refers to the overlap of bigrams between the system and reference summaries.\n",
    "\n",
    "ROUGE-L: Longest Common Subsequence (LCS)[3] based statistics. Longest common subsequence problem takes into account sentence level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.\n",
    "\n",
    "Rouge applies in cases with multiple reference summary, however, because we have only one ground truth (i.e., one title), we will simplify the definition of rouge as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall (ROUGE) = $\\frac{Count_{match}(gram_n)}{Count(gram_n)}$\n",
    "\n",
    "n stands for the length of the n-gram ($gram_n$), and $Count_{match}(gram_n)$ is the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we calculate F-score?\n",
    "$F_1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relative import of rouge_evaluation\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "from myeval import rouge_evaluation\n",
    "\n",
    "# BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "with open(\"glove_self_trained/predictions_base\",'rb') as f:\n",
    "    basePred = pickle.load(f)\n",
    "with open(\"glove_self_trained/predictions_attention\",'rb') as f:\n",
    "    attentionPred = pickle.load(f)\n",
    "\n",
    "# kNN\n",
    "knn_pred = np.load(\"kNN/baseline_generated.npy\")\n",
    "knn_truth = np.load(\"kNN/baseline_true.npy\")\n",
    "\n",
    "# word2vec\n",
    "with open(\"word2vec/predictions_base\",'rb') as f:\n",
    "    word2vec_basePred = pickle.load(f)\n",
    "# with open(\"word2vec/predictions_attention\",'rb') as f:\n",
    "#     word2vec_attnPred = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n",
    "def one_gram_recall(truth, pred):\n",
    "    truth2 = list(set(truth.split())) # get unique words in truth\n",
    "    pred2 = list(set(pred.split())) # get unique words in predicted title\n",
    "    return np.sum([word in pred2 for word in truth2])/len(truth.split())\n",
    "\n",
    "def ngrams(txt_input, n):\n",
    "    txt_input = txt_input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(txt_input)-n+1):\n",
    "        output.append(txt_input[i:i+n])\n",
    "    return output\n",
    "\n",
    "def two_gram_recall(truth, pred, n = 2):\n",
    "    '''\n",
    "    truth = basePred['Truth'][i]\n",
    "    pred = basePred[model][i]\n",
    "    two_gram_recall(truth_2gram, pred_2gram)\n",
    "    '''\n",
    "    truth_2gram = ngrams(truth, n)\n",
    "    pred_2gram = ngrams(pred, n)\n",
    "    return np.sum([gram in truth_2gram for gram in pred_2gram])/len(truth.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Recall_{\\text{1gram}} = \\frac{\\text{# of words overlap between predicted and true title}}{\\text{# of words in true title}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True title: learning shuffle ideals under restricted distributions\n",
      "Predicted title: learning on the complexity of the presence of barn\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "print(\"True title:\", basePred['Truth'][i])\n",
    "print(\"Predicted title:\", basePred['Greedy'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['learning', 'shuffle'],\n",
       " ['shuffle', 'ideals'],\n",
       " ['ideals', 'under'],\n",
       " ['under', 'restricted'],\n",
       " ['restricted', 'distributions']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(basePred['Truth'][i],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['learning', 'on'],\n",
       " ['on', 'the'],\n",
       " ['the', 'complexity'],\n",
       " ['complexity', 'of'],\n",
       " ['of', 'the'],\n",
       " ['the', 'presence'],\n",
       " ['presence', 'of'],\n",
       " ['of', 'barn']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(basePred['Greedy'][i], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"kNN\"]\n",
    "metrics = ['rouge1', 'rouge2', 'bleu']\n",
    "kNN_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "# true = \"Pavlos likes cross validation\"\n",
    "# pred = \"Pavlos likes cheese\"\n",
    "        \n",
    "for i in range(len(knn_pred)):\n",
    "    for model in models:\n",
    "#         rouge1_res = rouge_evaluation(basePred[model][i], basePred['Truth'][i])[2]\n",
    "#         rouge2_res = rouge_evaluation(basePred[model][i], basePred['Truth'][i], method=\"rouge-2\")[2]\n",
    "#         rougeL_res = rouge_evaluation(basePred[model][i], basePred['Truth'][i], method=\"rouge-l\")[2]\n",
    "        rouge1_res = one_gram_recall(knn_truth[i], knn_pred[i])\n",
    "        rouge2_res = two_gram_recall(knn_truth[i], knn_pred[i])\n",
    "        bleu = sentence_bleu(knn_truth[i], knn_pred[i], smoothing_function=SmoothingFunction().method3)\n",
    "\n",
    "        kNN_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "        kNN_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "        kNN_eval_dict[model]['bleu'].append(bleu)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17277510273913035\n",
      "0.045383005228656854\n",
      "0.013455188123427974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17277510273913035"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.mean(kNN_eval_dict['kNN']['rouge1']))\n",
    "print(np.mean(kNN_eval_dict['kNN']['rouge2']))\n",
    "print(np.mean(kNN_eval_dict['kNN']['bleu']))\n",
    "\n",
    "kNN_f1 = 2 * (np.mean(kNN_eval_dict['kNN']['rouge1']) * np.mean(kNN_eval_dict['kNN']['rouge1']))/(np.mean(kNN_eval_dict['kNN']['rouge1']) + np.mean(kNN_eval_dict['kNN']['rouge1']))\n",
    "kNN_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove: self-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Greedy\", \"Non-Greedy\"]\n",
    "metrics = ['rouge1', 'rouge2', 'bleu']\n",
    "baseline_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "# true = \"Pavlos likes cross validation\"\n",
    "# pred = \"Pavlos likes cheese\"\n",
    "        \n",
    "for i in range(len(basePred['Truth'])):\n",
    "    for model in models:\n",
    "#         rouge1_res = rouge_evaluation(basePred[model][i], basePred['Truth'][i])[2]\n",
    "#         rouge2_res = rouge_evaluation(basePred[model][i], basePred['Truth'][i], method=\"rouge-2\")[2]\n",
    "#         rougeL_res = rouge_evaluation(basePred[model][i], basePred['Truth'][i], method=\"rouge-l\")[2]\n",
    "        rouge1_res = one_gram_recall(basePred['Truth'][i], basePred[model][i])\n",
    "        rouge2_res = two_gram_recall(basePred['Truth'][i], basePred[model][i])\n",
    "        bleu = sentence_bleu(basePred['Truth'][i], basePred[model][i], smoothing_function=SmoothingFunction().method3)\n",
    "        \n",
    "        baseline_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "        baseline_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "        baseline_eval_dict[model]['bleu'].append(bleu)\n",
    "\n",
    "attention_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "\n",
    "for i in range(len(basePred['Truth'])):\n",
    "    for model in models:\n",
    "        rouge1_res = one_gram_recall(attentionPred['Truth'][i], attentionPred[model][i])\n",
    "        rouge2_res = two_gram_recall(attentionPred['Truth'][i], attentionPred[model][i])\n",
    "        bleu = sentence_bleu(attentionPred['Truth'][i], attentionPred[model][i], smoothing_function=SmoothingFunction().method3)\n",
    "        \n",
    "        attention_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "        attention_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "        attention_eval_dict[model]['bleu'].append(bleu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09841122507197558"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Greedy\n",
    "np.mean(baseline_eval_dict['Greedy']['rouge1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013718839998325369"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(baseline_eval_dict['Greedy']['bleu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024080746764129128"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_f1 = 2 * (np.mean(baseline_eval_dict['Greedy']['rouge1']) * np.mean(baseline_eval_dict['Greedy']['bleu']))/(np.mean(baseline_eval_dict['Greedy']['rouge1']) + np.mean(baseline_eval_dict['Greedy']['bleu']))\n",
    "greedy_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1214838735962595"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(baseline_eval_dict['Non-Greedy']['rouge1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010275063935989353"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(baseline_eval_dict['Non-Greedy']['bleu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018947550606769226"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nongreedy_f1 = 2 * (np.mean(baseline_eval_dict['Non-Greedy']['rouge1']) * np.mean(baseline_eval_dict['Non-Greedy']['bleu']))/(np.mean(baseline_eval_dict['Non-Greedy']['rouge1']) + np.mean(baseline_eval_dict['Non-Greedy']['bleu']))\n",
    "nongreedy_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10630318028264275\n",
      "0.011577459224898126\n",
      "0.020880795020128157\n",
      "0.1294049162924353\n",
      "0.010968986443292517\n",
      "0.020223713166676118\n"
     ]
    }
   ],
   "source": [
    "# Attention Glove\n",
    "# Greedy\n",
    "print(np.mean(attention_eval_dict['Greedy']['rouge1']))\n",
    "\n",
    "print(np.mean(attention_eval_dict['Greedy']['bleu']))\n",
    "\n",
    "greedy_f1 = 2 * (np.mean(attention_eval_dict['Greedy']['rouge1']) * np.mean(attention_eval_dict['Greedy']['bleu']))/(np.mean(attention_eval_dict['Greedy']['rouge1']) + np.mean(attention_eval_dict['Greedy']['bleu']))\n",
    "print(greedy_f1)\n",
    "\n",
    "# Non-greedy\n",
    "print(np.mean(attention_eval_dict['Non-Greedy']['rouge1']))\n",
    "print(np.mean(attention_eval_dict['Non-Greedy']['bleu']))\n",
    "nongreedy_f1 = 2 * (np.mean(attention_eval_dict['Non-Greedy']['rouge1']) * np.mean(attention_eval_dict['Non-Greedy']['bleu']))/(np.mean(attention_eval_dict['Non-Greedy']['rouge1']) + np.mean(attention_eval_dict['Non-Greedy']['bleu']))\n",
    "print(nongreedy_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Greedy\", \"Non-Greedy\"]\n",
    "metrics = ['rouge1', 'rouge2', 'bleu']\n",
    "word2vec_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "\n",
    "for i in range(len(word2vec_basePred['Truth'])):\n",
    "    for model in models:\n",
    "#         rouge1_res = rouge_evaluation(word2vec_basePred[model][i], word2vec_basePred['Truth'][i])[2]\n",
    "#         rouge2_res = rouge_evaluation(word2vec_basePred[model][i], word2vec_basePred['Truth'][i], method=\"rouge-2\")[2]\n",
    "#         rougeL_res = rouge_evaluation(word2vec_basePred[model][i], word2vec_basePred['Truth'][i], method=\"rouge-l\")[2]\n",
    "        rouge1_res = one_gram_recall(word2vec_basePred['Truth'][i], word2vec_basePred[model][i])\n",
    "        rouge2_res = two_gram_recall(word2vec_basePred['Truth'][i], word2vec_basePred[model][i])\n",
    "        bleu = sentence_bleu(word2vec_basePred['Truth'][i], word2vec_basePred[model][i], smoothing_function=SmoothingFunction().method3)\n",
    "        \n",
    "        word2vec_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "        word2vec_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "        word2vec_eval_dict[model]['bleu'].append(bleu)\n",
    "\n",
    "# # attention\n",
    "# word2vec_attn_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "# for i in range(len(word2vec_attentionPred['Truth'])):\n",
    "#     for model in models:\n",
    "#         rouge1_res = one_gram_recall(word2vec_attentionPred['Truth'][i], word2vec_attentionPred[model][i])\n",
    "#         rouge2_res = two_gram_recall(word2vec_attentionPred['Truth'][i], word2vec_attentionPred[model][i])\n",
    "#         bleu = sentence_bleu(word2vec_attentionPred['Truth'][i], word2vec_attentionPred[model][i], smoothing_function=SmoothingFunction().method3)\n",
    "        \n",
    "#         word2vec_attn_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "#         word2vec_attn_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "#         word2vec_attn_eval_dict[model]['bleu'].append(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12605377168400114\n",
      "0.013500808908447787\n",
      "0.024389423499681085\n",
      "0.12221925997360202\n",
      "0.010770771719775033\n",
      "0.019796908567863168\n"
     ]
    }
   ],
   "source": [
    "# Greedy\n",
    "print(np.mean(word2vec_eval_dict['Greedy']['rouge1']))\n",
    "\n",
    "print(np.mean(word2vec_eval_dict['Greedy']['bleu']))\n",
    "\n",
    "greedy_f1 = 2 * (np.mean(word2vec_eval_dict['Greedy']['rouge1']) * np.mean(word2vec_eval_dict['Greedy']['bleu']))/(np.mean(word2vec_eval_dict['Greedy']['rouge1']) + np.mean(word2vec_eval_dict['Greedy']['bleu']))\n",
    "print(greedy_f1)\n",
    "\n",
    "# Non-greedy\n",
    "print(np.mean(word2vec_eval_dict['Non-Greedy']['rouge1']))\n",
    "print(np.mean(word2vec_eval_dict['Non-Greedy']['bleu']))\n",
    "nongreedy_f1 = 2 * (np.mean(word2vec_eval_dict['Non-Greedy']['rouge1']) * np.mean(word2vec_eval_dict['Non-Greedy']['bleu']))/(np.mean(word2vec_eval_dict['Non-Greedy']['rouge1']) + np.mean(word2vec_eval_dict['Non-Greedy']['bleu']))\n",
    "print(nongreedy_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention word2vec\n",
    "# Greedy\n",
    "print(np.mean(word2vec_attn_eval_dict['Greedy']['rouge1']))\n",
    "\n",
    "print(np.mean(word2vec_attn_eval_dict['Greedy']['bleu']))\n",
    "\n",
    "greedy_f1 = 2 * (np.mean(word2vec_attn_eval_dict['Greedy']['rouge1']) * np.mean(word2vec_attn_eval_dict['Greedy']['bleu']))/(np.mean(word2vec_attn_eval_dict['Greedy']['rouge1']) + np.mean(word2vec_attn_eval_dict['Greedy']['bleu']))\n",
    "print(greedy_f1)\n",
    "\n",
    "# Non-greedy\n",
    "print(np.mean(word2vec_attn_eval_dict['Non-Greedy']['rouge1']))\n",
    "print(np.mean(word2vec_attn_eval_dict['Non-Greedy']['bleu']))\n",
    "nongreedy_f1 = 2 * (np.mean(word2vec_attn_eval_dict['Non-Greedy']['rouge1']) * np.mean(word2vec_attn_eval_dict['Non-Greedy']['bleu']))/(np.mean(word2vec_attn_eval_dict['Non-Greedy']['rouge1']) + np.mean(word2vec_attn_eval_dict['Non-Greedy']['bleu']))\n",
    "print(nongreedy_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
