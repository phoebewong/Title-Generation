{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Title Generation </center></h1>\n",
    "<h2><center>Sequence-to-Sequence Text Summarization for Academic Journal Articles </center></h2>\n",
    "<center>Karina Huang, Abhimanyu Vasishth, Phoebe Wong </center>\n",
    "<center>AC209b: Advanced Topics in Data Science </center>\n",
    "<center>Spring 2019 </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import package dependencies\n",
    "import re\n",
    "import glove\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model - Nearest Neighbors with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Preprocessing for Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Cleaning\n",
    "The original [NIPS dataset](https://www.kaggle.com/benhamner/nips-2015-papers/version/2) acquired from Kaggle included XXXX journal articles, most of which were missing abstracts. Our first attempt to clean the data was to find and extract abstracts for articles missing the piece of information. The `getAbstract` code performs the search for abstract in two steps. These two steps were results of ad-hoc identification of abstract extractions and recovered 3,250 abstracts. We removed all articles missing abstracts and formatted the text for data cleaning. Due to computational constraints, we subsetted articles with abstract of length 250 in words for the current study. The final dataset used included 4,638 observations without missing data in title or abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatText(x):\n",
    "    \"render space in text and text to lowercase\"\n",
    "    for i in range(len(x)):\n",
    "        #check for data type\n",
    "        if type(x[i]) == str:\n",
    "            try:\n",
    "                x[i] = x[i].replace('\\n', ' ').lower()\n",
    "            except:\n",
    "                x[i] = x[i].lower()\n",
    "    return x\n",
    "\n",
    "def getAbstract(paper_text, methods = 1):\n",
    "    \"extract abstract from text in two steps\"\n",
    "    #step 1:\n",
    "    #find 'abstract' in text\n",
    "    #find the next word/phrase in all cap, wrapped in '\\n'\n",
    "    #extract everything in between as abstract\n",
    "    if methods == 1:\n",
    "        try:\n",
    "            #find abstract\n",
    "            a1 = re.search('abstract\\n', paper_text, re.IGNORECASE)\n",
    "            paper_text = paper_text[a1.end():]\n",
    "            #find the next section in all cap\n",
    "            a2 = re.search(r'\\n+[A-Z\\s]+\\n', paper_text)\n",
    "            return paper_text[: a2.start()]\n",
    "        except:\n",
    "            return np.nan\n",
    "    #step 2:\n",
    "    #find abstract in text\n",
    "    #find next item wrapped between '\\n\\n' and '\\n\\n'\n",
    "    #extract everything in between as abstract\n",
    "    if methods == 2:\n",
    "        try:\n",
    "            a1 = re.search('abstract\\n', paper_text, re.IGNORECASE)\n",
    "            paper_text = paper_text[a1.end():]\n",
    "            #find the next section in all cap\n",
    "            a2 = re.search(r'\\n\\n+.+\\n\\n', paper_text)\n",
    "            return paper_text[: a2.start()]\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "def preprocessing(papers, formatCols = ['title', 'abstract','paper_text'], dropnan = False):\n",
    "    \"preliminary data preprocessing for model fitting\"\n",
    "    #avoid modifying original dataset\n",
    "    papersNew = papers.copy()\n",
    "    #replace missing values with nan\n",
    "    papersNew.abstract = papersNew.abstract.apply(lambda x: np.nan if x == 'Abstract Missing' else x)\n",
    "    #extract missing abstract in two steps\n",
    "    #steps identified by ad-hoc examination of missing values\n",
    "    for m in [1, 2]:\n",
    "        #try searching for abstract in text if value is missing\n",
    "        papersNew['abstract_new'] = papersNew.paper_text.apply(lambda x: getAbstract(x, methods = m))\n",
    "        #replace nan in abstract with extracted abstract\n",
    "        papersNew.loc[papersNew.abstract.isnull(), 'abstract'] = papersNew.abstract_new\n",
    "        papersNew.drop(['abstract_new'], axis = 1, inplace = True)\n",
    "    #format columns of interest\n",
    "    papersNew[formatCols] = papersNew[formatCols].apply(lambda x: formatText(x), axis = 1)\n",
    "    if dropnan:\n",
    "        #drop na in abstract\n",
    "        papersNew = papersNew.dropna(subset = ['abstract'])\n",
    "        #append abstract and title length to data frame\n",
    "        papersNew ['aLen'] = papersNew.abstract.apply(lambda x: len(x.split(' ')))\n",
    "        papersNew ['tLen'] = papersNew.title.apply(lambda x: len(x.split(' ')))\n",
    "    return papersNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Used\n",
      "==================\n",
      "Number of observations:  4638\n",
      "Maximum title length:  20\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data = pd.read_csv('../data/papers.csv')\n",
    "#preprocessing\n",
    "dataNew = preprocessing(data, dropnan = True)\n",
    "#subset articles with a length less than or equal to 250\n",
    "data250 = dataNew[dataNew.aLen <= 250]\n",
    "print('Final Dataset Used')\n",
    "print('==================')\n",
    "print('Number of observations: ', data250.shape[0])\n",
    "print('Maximum title length: ', data250.tLen.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocessing for Model Training\n",
    "\n",
    "After cleaning the data, we processed the titles and abstracts using `processText` below. The class object records and returns:\n",
    "\n",
    "* number of unique words\n",
    "* maximum sequence length (should be 250 as titles are shorter than abstracts)\n",
    "* dictionaries for tokenization\n",
    "* tokenized vector of titles and abstracts\n",
    "\n",
    "Note that an important step of our tokenization was the qualification of rare, or unwanted, words. This is because NIPS articles are often written in laTex, and included scientific equations that may compromise the learning of important words. We approximated patterns of unwanted words and replaced them with an `<ign>` tag in the tokenization process. This resulted in 32,468 unique words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualify(word):\n",
    "    '''helper function to select words for tokenization.'''\n",
    "    #symbols\n",
    "    symbols = \"\"\"/?~`!@#$%^&*()_-+=|\\{}[];<>\"'.,:\"\"\"\n",
    "    #abbreviations\n",
    "    abb = \"\"\"e.g.,i.e.,etal.,\"\"\"\n",
    "    #disqualify empty space and words starting with symbol\n",
    "    if len(word) < 1 or word[0] in symbols:\n",
    "        return False\n",
    "    elif len(word) > 2:\n",
    "        #disqualify abbreviations\n",
    "        if word in abb:\n",
    "            return False\n",
    "        #otherwise count all combinations with length > 2\n",
    "        else:\n",
    "            return True\n",
    "    #if input length is one\n",
    "    #count only if it is 'a'\n",
    "    elif len(word) == 1:\n",
    "        if word in ['a', 'i']:\n",
    "            return True\n",
    "    #with input length of 2\n",
    "    #disqualify those with a symbol as the second character\n",
    "    elif len(word) == 2:\n",
    "        if word[1] not in symbols:\n",
    "            return True\n",
    "    #otherwise disqualify input\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "class processText:\n",
    "    '''\n",
    "    class object for data processing preperation for embedding training.\n",
    "\n",
    "    Parameters:\n",
    "    ===========\n",
    "    1) textVec: list of array-like, vector of text in strings\n",
    "\n",
    "    Methods:\n",
    "    ===========\n",
    "    1) updateMaxLen: count and update maximum sequence length\n",
    "    2) getDictionary: update dictionaries of words and tokens,\n",
    "        function called in `tokenize`\n",
    "    3) tokenize: return tokenized vector of text for model training\n",
    "    '''\n",
    "    def __init__(self, textVec):\n",
    "\n",
    "        #initiate class object\n",
    "        self.textVec = list()\n",
    "        for vec in textVec:\n",
    "            #string to list\n",
    "            vec = [x.strip().split(' ') for x in vec]\n",
    "            self.textVec.append(vec)\n",
    "\n",
    "        #prep  dictionaries for update\n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.maxLen = 0\n",
    "        self.nUnique = 0\n",
    "\n",
    "    def updateMaxLen(self):\n",
    "        for vec in self.textVec:\n",
    "            for txt in vec:\n",
    "                #get length of sequence\n",
    "                cntLen = len(txt)\n",
    "                #update maximum sequence length\n",
    "                if self.maxLen < cntLen:\n",
    "                    self.maxLen = cntLen\n",
    "\n",
    "    def getDictionary(self):\n",
    "\n",
    "        if len(self.word2idx) != 0:\n",
    "            print(\"Dictionary already updated.\")\n",
    "\n",
    "        else:\n",
    "            #initiate dictionary updates\n",
    "            #pad with 0\n",
    "            #end of sequence as 1\n",
    "            #ignored/disqualified words as 2\n",
    "            #start tokenization at 3\n",
    "            pad = 0\n",
    "            eos = 1\n",
    "            ign = 2\n",
    "            start = 3\n",
    "\n",
    "            self.word2idx['_'] = pad\n",
    "            self.word2idx['*'] = eos\n",
    "            self.word2idx['<ign>'] = ign\n",
    "\n",
    "            for vec in self.textVec:\n",
    "                for txt in vec:\n",
    "                    for w in txt:\n",
    "                        if qualify(w) == True:\n",
    "                            if w not in self.word2idx.keys():\n",
    "                                self.word2idx.update({w: start})\n",
    "                                start += 1\n",
    "\n",
    "            #update number of unique words in data set\n",
    "            self.nUnique = start - 3\n",
    "            #update idx to word dictionary\n",
    "            self.idx2word = dict((idx,word) for word,idx in self.word2idx.items())\n",
    "\n",
    "    def tokenize(self):\n",
    "        #get dictionaries if function hasn't been called\n",
    "        if len(self.word2idx) == 0:\n",
    "            self.getDictionary()\n",
    "        #cache list for tokenization\n",
    "        tokenizedVec = list()\n",
    "        for i in range(len(self.textVec)):\n",
    "            vec = self.textVec[i]\n",
    "            #cache list for the tokenized vector\n",
    "            tempVec = list()\n",
    "            for txt in vec:\n",
    "                #cache list for sequence\n",
    "                sVec = list()\n",
    "                for w in txt:\n",
    "                    #if word is in dictionary, tokenize\n",
    "                    if w in self.word2idx:\n",
    "                        sVec.append(self.word2idx[w])\n",
    "                    #if word not in dictionary, tag as ignored\n",
    "                    else:\n",
    "                        sVec.append(self.word2idx['<ign>'])\n",
    "                tempVec.append(sVec)\n",
    "            tokenizedVec.append(tempVec)\n",
    "\n",
    "        return tokenizedVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  32468\n",
      "Maxmimum sequence length:  250\n",
      "==============================================================================================================\n",
      "Example of tokenized title:\n",
      " [3, 4, 5, 6, 7, 8, 9] => ['self-organization', 'of', 'associative', 'database', 'and', 'its', 'applications']\n",
      "==============================================================================================================\n",
      "Example of tokenized abstract:\n",
      " [42, 466, 64, 4, 580, 5, 5497, 431, 5498, 5499, 51, 9, 19, 321, 5500, 5501, 58, 5498, 5497, 176, 5502, 3251, 503, 51, 309, 5503, 75, 58, 619, 5504, 1743, 4, 5505, 42, 61, 4, 3, 431, 5506, 368, 42, 1019, 4, 5507, 1727, 5508, 10, 289, 4072, 4, 21, 5509, 75, 58, 5510, 5504, 5511, 42, 5512, 19, 187, 1181, 92, 7, 122, 19, 42, 319, 320, 321, 159, 1391, 5513] => ['an', 'efficient', 'method', 'of', 'self-organizing', 'associative', 'databases', 'is', 'proposed', 'together', 'with', 'applications', 'to', 'robot', 'eyesight', 'systems.', 'the', 'proposed', 'databases', 'can', 'associate', 'any', 'input', 'with', 'some', 'output.', 'in', 'the', 'first', 'half', 'part', 'of', 'discussion,', 'an', 'algorithm', 'of', 'self-organization', 'is', 'proposed.', 'from', 'an', 'aspect', 'of', 'hardware,', 'it', 'produces', 'a', 'new', 'style', 'of', 'neural', 'network.', 'in', 'the', 'latter', 'half', 'part,', 'an', 'applicability', 'to', 'handwritten', 'letter', 'recognition', 'and', 'that', 'to', 'an', 'autonomous', 'mobile', 'robot', 'system', 'are', 'demonstrated.']\n"
     ]
    }
   ],
   "source": [
    "#tokenize data\n",
    "prep = processText(data250[['title', 'abstract']].values.T)\n",
    "#update sequence length\n",
    "prep.updateMaxLen()\n",
    "#get dictionaries of word and tags\n",
    "prep.getDictionary()\n",
    "word2idx = prep.word2idx\n",
    "idx2word = prep.idx2word\n",
    "\n",
    "print('Number of unique words: ', prep.nUnique)\n",
    "print('Maxmimum sequence length: ', prep.maxLen)\n",
    "print('='*110)\n",
    "\n",
    "#get tokenized vector of text\n",
    "txtTokenized = prep.tokenize()\n",
    "titles = txtTokenized[0]\n",
    "abstracts = txtTokenized[1]\n",
    "print('Example of tokenized title:\\n {0} => {1}'.format(titles[0], [prep.idx2word[i] for i in titles[0]]))\n",
    "print('='*110)\n",
    "print('Example of tokenized abstract:\\n {0} => {1}'.format(abstracts[0],[prep.idx2word[i] for i in abstracts[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split our dataset into training ($\\approx$72\\%), validation ($\\approx$8\\%) and test set ($\\approx$20\\%). For consistency, we set the random state to 209."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  3339\n",
      "Number of validation samples:  371\n",
      "Number of test samples:  928\n"
     ]
    }
   ],
   "source": [
    "#split data into train, validation, and test set\n",
    "trainX, testX, trainY, testY = train_test_split(abstracts, titles, test_size = 0.2 , random_state = 209)\n",
    "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size = 0.1 , random_state = 209)\n",
    "\n",
    "print('Number of training samples: ', len(trainX))\n",
    "print('Number of validation samples: ', len(valX))\n",
    "print('Number of test samples: ', len(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding karina\n",
    "#word2vec phoebe\n",
    "#glove pre-trained abhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Word2vec \n",
    "\n",
    "Word2vec (by Mikolov et al., 2013[1]) model, similar to other word embedding models, is used to learn about vector representations of words, also known as word embeddings. Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text. It comes in two flavors, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. Algorithmically, the two models are similar with a small differnce in focus. \n",
    "\n",
    "CBOW predicts target words (e.g. 'mat') from source context words ('the cat sits on the'), while the skip-gram does the inverse and predicts source context-words from the target words. In our model, we went with the CBOW approach, because it is recommended to work better for smaller datasets[2].\n",
    "\n",
    "To make our embeddings comparable with other embeddings, we limit the number of embedding vectors to be 100. Because not all words in our dataset has a feature vector pre-trained in the Word2Vec corpus, we needed to pad in zeros for those words. In other words, for words that are unseen in the word2vec model (2.9% of unique words in our dataset), their embedding vectors are vectors of 0. With 32,471 unique words, we resulted in an embedding matrix with a dimension of (32471, 100).\n",
    "\n",
    "Finally, word2vec can tell us the similarity of words by calculating the cosine distance between the embedding vectors of the two words\n",
    "\n",
    "Reference: \n",
    "\n",
    "[1] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).\n",
    "\n",
    "[2] https://www.tensorflow.org/tutorials/representation/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list that each element is an abstract (actual words)\n",
    "abstracts_list_word = []\n",
    "for i in range(len(abstracts)):\n",
    "    abstracts_list_word.append([prep.idx2word[word] for word in abstracts[i]])\n",
    "\n",
    "# Initiate and train the word2vec model using our dataset\n",
    "word_model = Word2Vec(abstracts_list_word, size=100, min_count=1, window=5, iter=100) \n",
    "# Initiate the model with the documents\n",
    "word_model.train(abstracts_list_word, total_examples=len(abstracts_list_word), epochs=10, compute_loss = True) \n",
    "\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If try to replicate the result, please load the pre-trained weight instead.\n",
    "# word_model = np.load(histPath+'embeddMatrix_word2vec_0512.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unseen vocab to the embedding matrix\n",
    "all_unique_words = list(prep.word2idx.keys())\n",
    "embeddMatrix = np.zeros(shape = (len(all_unique_words), 100)) # initiate with zero, with 32471 unique words\n",
    "\n",
    "for i, word in enumerate(all_unique_words):\n",
    "    try:\n",
    "        embeddMatrix[i] = word_model.wv.word_vec(word) # find the word in the vector space and store the embeddings \n",
    "    except KeyError: # unseen vocab stay with 0 by skippig \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking similar words:\n",
      "  model -> model, (0.72), approach (0.58), method (0.54), models (0.54), framework (0.52), mechanism (0.51), methodology (0.50), formulation (0.47)\n",
      "  network -> net (0.72), networks (0.70), network, (0.65), nets (0.61), dude, (0.60), networks, (0.58), network. (0.58), signaling. (0.56)\n",
      "  convolution -> transformation (0.49), de-convolution (0.46), filter, (0.44), operators (0.43), layer (0.43), non-linear (0.42), combinations (0.42), white-noise (0.42)\n",
      "  learning -> learning, (0.65), learning. (0.60), learning: (0.46), adaptation (0.44), translation (0.44), single-core (0.44), learning-based (0.44), teaching (0.41)\n",
      "  neural -> kwta (0.63), probabilities.\" (0.58), l(winner-take-all (0.58), formed; (0.58), context-independent (0.57), pretrain (0.56), disco (0.55), rbf (0.55)\n",
      "  barn -> owl (0.72), owls (0.58), owl. (0.58), map-like (0.56), heading (0.52), microstimulation (0.52), owl's (0.51), young (0.50)\n"
     ]
    }
   ],
   "source": [
    "# Example of similar word using the model\n",
    "print('Checking similar words:')\n",
    "for word in ['model', 'network', 'convolution', 'learning', 'neural', 'barn']:\n",
    "    most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.wv.most_similar(word)[:8])\n",
    "    print('  %s -> %s' % (word, most_similar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Self-Trained GloVe Embeddings\n",
    "\n",
    "One motivation to train our own embedding is that pre-trained word embeddings may not capture well the similarities and co-occurances between words in the current dataset. Because academic journal article come with many technical terms, it is possible that weights using pre-trained embeddings do not apply to these texts. As mentioned above, only half of the unique words in the current dataset were found in the pre-trained GloVe embeddings. Therefore, we experimented with training our own embedding matrix, in hopes that the initialized weights would help our models learn to summarize the abstracts more effectively. Note that the embedding matrix is only trained on the training examples split using the code in the data preprocessing section. This is because realistically we do not have access to the hold-out sets. Again, the trained embedding matrix dimension is (32,471, 100); the height corresponds to the sum of the number of unique words and the number of special tags (`<eos>`, `<ign>`, `<pad>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words for embedding training:  27141\n"
     ]
    }
   ],
   "source": [
    "#get text for training\n",
    "#remove ignored/disqualified words\n",
    "#because we do not want to learn this tag\n",
    "embedd_trainX = [[idx2word[x] for x in v if x != 2] for v in trainX]\n",
    "embedd_trainY = [[idx2word[x] for x in v if x != 2] for v in trainY]\n",
    "embeddTxt = trainX + trainY\n",
    "\n",
    "#prep dictionary for embedding training\n",
    "#drop pad, eos, and ignore tag\n",
    "start = 0\n",
    "embeddDict = dict()\n",
    "for vec in embeddTxt:\n",
    "    for w in vec:\n",
    "        if w not in embeddDict.keys():\n",
    "            embeddDict[w] = start\n",
    "            start += 1\n",
    "\n",
    "print('Number of unique words for embedding training: ', len(embeddDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code below were used for embedding training\n",
    "#please see rnn_preprocessing for the execution history\n",
    "\n",
    "# #train glove embedding \n",
    "# #creating a corpus object\n",
    "# corpus_ = glove.Corpus(dictionary = embeddDict) \n",
    "# #training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "# corpus_.fit(embeddTxt, window = 10)\n",
    "# #train embedding using corpus weight matrix created above \n",
    "# glove_ = glove.Glove(no_components = 100, learning_rate = 0.01, random_state = 209)\n",
    "# glove_.fit(corpus_.matrix, epochs=50, no_threads=10, verbose = True)\n",
    "# glove_.add_dictionary(corpus_.dictionary)\n",
    "\n",
    "# #embedding matrix\n",
    "# #initiate a matrix with shape \n",
    "# #(number of unique words in our dataset, latent dimension of embedding)\n",
    "# embeddMatrix = np.zeros((len(word2idx), 100))\n",
    "# #loop through trained embedding matrix to find weights of trained words\n",
    "# for i, w in enumerate(word2idx):\n",
    "#     try:\n",
    "#         embeddVec = glove_.word_vectors[glove_.dictionary[w]]\n",
    "#         embeddMatrix[i] = embeddVec\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words most similar to \"stochastic\"\n",
      "=========================================\n",
      "word: gradient | cosine similarity: 0.8991909690378893\n",
      "word: three-composite | cosine similarity: 0.8937557315973716\n",
      "word: descent | cosine similarity: 0.8744804215977376\n",
      "word: proximal | cosine similarity: 0.76864338053953\n",
      "word: optimization | cosine similarity: 0.7646848913256339\n",
      "word: descent. | cosine similarity: 0.7640788665473602\n",
      "word: accelerated | cosine similarity: 0.7629822276705301\n",
      "word: gradients | cosine similarity: 0.7624208397216201\n",
      "word: projected | cosine similarity: 0.7623820982466453\n"
     ]
    }
   ],
   "source": [
    "#load trained glove model\n",
    "glove_ = glove.Glove.load('rnn_training_history/glove_.model')\n",
    "#display 10 words most similar to 'stochastic' by glove training\n",
    "print('Top 10 words most similar to \"stochastic\"')\n",
    "print('=========================================')\n",
    "for (i,j) in glove_.most_similar(word = 'stochastic', number = 10):\n",
    "    print(\"word: {0} | cosine similarity: {1}\".format(i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#karina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RNN Model with Attention and Context Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#karina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Evaluation Metrics\n",
    "Similar to classical classification task, we can evaluate our model performance using F-score, precision and recall. \n",
    "\n",
    "In the field of Natural Language Processing (NLP), it is common to use BLEU and ROUGE to measure precision and recall. \n",
    "\n",
    "### 7.1.1  BLEU\n",
    "BLEU (BiLingual Evaluation Understudy) stands for  measures how well a candidate translation matches a set of reference translations by counting the percentage of n-grams in the candidate translation overlapping with the references. BLEU was first introduced in Papineni et. al. (2001).\n",
    "\n",
    "### 7.1.2 ROUGE\n",
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It comes with mainly two metrics, ROUGE-N and ROUGE-L. \n",
    "\n",
    "ROUGE-N is a recall-related measure because the denominator of the equation is the total sum of the number of n-grams occurring at the reference summary side. \n",
    "\n",
    "ROUGE-N: Overlap of N-grams[2] between the system and reference summaries.\n",
    "- ROUGE-1 refers to the overlap of 1-gram (each word) between the system and reference summaries.\n",
    "- ROUGE-2 refers to the overlap of bigrams between the system and reference summaries.\n",
    "\n",
    "ROUGE-L: Longest Common Subsequence (LCS)[3] based statistics. Longest common subsequence problem takes into account sentence level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.\n",
    "\n",
    "Rouge applies in cases with multiple reference summary, however, because we have only one ground truth (i.e., one title), we will simplify the definition of rouge as following:\n",
    "Recall (ROUGE) = $\\frac{Count_{match}(gram_n)}{Count(gram_n)}$\n",
    "\n",
    "n stands for the length of the n-gram ($gram_n$), and $Count_{match}(gram_n)$ is the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries\"\n",
    "\n",
    "### 7.1.3 F-score\n",
    "$F = 2 * \\frac{Precision * Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Test-set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative import of our functions\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "# import our rouge functions\n",
    "from myeval import one_gram_recall, ngrams, two_gram_recall\n",
    "\n",
    "# BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our model predictions\n",
    "# glove self-trained embedding result\n",
    "with open(\"glove_self_trained/predictions_base\",'rb') as f:\n",
    "    basePred = pickle.load(f)\n",
    "with open(\"glove_self_trained/predictions_attention\",'rb') as f:\n",
    "    attentionPred = pickle.load(f)\n",
    "\n",
    "# kNN\n",
    "knn_pred = np.load(\"kNN/baseline_generated.npy\")\n",
    "knn_truth = np.load(\"kNN/baseline_true.npy\")\n",
    "\n",
    "# word2vec\n",
    "with open(\"word2vec/predictions_base\",'rb') as f:\n",
    "    word2vec_basePred = pickle.load(f)\n",
    "# with open(\"word2vec/predictions_attention\",'rb') as f:\n",
    "#     word2vec_attnPred = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Baseline kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"kNN\"]\n",
    "metrics = ['rouge1', 'rouge2', 'bleu']\n",
    "kNN_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "\n",
    "for i in range(len(knn_pred)):\n",
    "    for model in models:\n",
    "        rouge1_res = one_gram_recall(knn_truth[i], knn_pred[i]) # calculate ROGUE with 1-gram\n",
    "        rouge2_res = two_gram_recall(knn_truth[i], knn_pred[i]) # calculate ROGUE with 2-gram\n",
    "        bleu = sentence_bleu(knn_truth[i], knn_pred[i], smoothing_function=SmoothingFunction().method3) # calculate BLEU\n",
    "\n",
    "        kNN_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "        kNN_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "        kNN_eval_dict[model]['bleu'].append(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe: self-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Greedy\", \"Non-Greedy\"]\n",
    "metrics = ['rouge1', 'rouge2', 'bleu']\n",
    "baseline_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "        \n",
    "for i in range(len(basePred['Truth'])):\n",
    "    for model in models:\n",
    "        rouge2_res = two_gram_recall(basePred['Truth'][i], basePred[model][i])\n",
    "        bleu = sentence_bleu(basePred['Truth'][i], basePred[model][i], smoothing_function=SmoothingFunction().method3)\n",
    "        \n",
    "        baseline_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "        baseline_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "        baseline_eval_dict[model]['bleu'].append(bleu)\n",
    "\n",
    "attention_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "\n",
    "for i in range(len(basePred['Truth'])):\n",
    "    for model in models:\n",
    "        rouge1_res = one_gram_recall(attentionPred['Truth'][i], attentionPred[model][i])\n",
    "        rouge2_res = two_gram_recall(attentionPred['Truth'][i], attentionPred[model][i])\n",
    "        bleu = sentence_bleu(attentionPred['Truth'][i], attentionPred[model][i], smoothing_function=SmoothingFunction().method3)\n",
    "        \n",
    "        attention_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "        attention_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "        attention_eval_dict[model]['bleu'].append(bleu)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Greedy\", \"Non-Greedy\"]\n",
    "metrics = ['rouge1', 'rouge2', 'bleu']\n",
    "word2vec_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "\n",
    "for i in range(len(word2vec_basePred['Truth'])):\n",
    "    for model in models:\n",
    "        rouge1_res = one_gram_recall(word2vec_basePred['Truth'][i], word2vec_basePred[model][i])\n",
    "        rouge2_res = two_gram_recall(word2vec_basePred['Truth'][i], word2vec_basePred[model][i])\n",
    "        bleu = sentence_bleu(word2vec_basePred['Truth'][i], word2vec_basePred[model][i], smoothing_function=SmoothingFunction().method3)\n",
    "        \n",
    "        word2vec_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "        word2vec_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "        word2vec_eval_dict[model]['bleu'].append(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the summary of our model performance on the test-set. We compared our predicted title to the ground-truth title of the test set (928 titles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                                      | ROUGE-1 (Recall) | ROUGE-2 (Recall) | BLEU (Precision) | F-score |\n",
    "|--------------------------------------------|------------------|------------------|------------------|---------|\n",
    "| k-Nearest-Neighbor                         | .1728            | .0454            | .0135            | .1728   |\n",
    "| GloVe (self-trained, greedy search)        | .0984            | .0136            | .0137            | .0241   |\n",
    "| GloVe (self-trained, non-greedy search)    | .1215            | .0058            | .0103            | .0189   |\n",
    "| Word2vec (self-trained, greedy search)     | .1060            | .0216            | .0116            | .0209   |\n",
    "| Word2vec (self-trained, non-greedy search) | .1294            | .0064            | .0110            | .0202   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we can see k-NN model has the highest test-set performance based on the recall, precision and F-score metrics. However, for metrics we picked in this project, k-NN has an advantage over other models because the titles generated from kNN are coming from the dataset, which provides an advantage of having a higher recall (because of the overlap in the keywords as well as common keywords used in title especially when the dataset is in a very particular domain (NeurIPS). Similar reason applies for higher precision as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#karina\n",
    "#abhi\n",
    "#PHOEBE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Evaluation metrics\n",
    "In this project, we used ROUGE-N, BLEU and F-score to examine the model performance, because it is commonly used in text summarization task, which title generation can be seen as one. However, there are a couple areas that we want to discuss. \n",
    "\n",
    "#### 8.1.1 Comparison to the True Title\n",
    "In our project, we compared our predicted title with the ground truth and used different metrics to evaluate how similar the predicted titles are with the true title. The main motivation to compare with the ground truth is because it is the only immediately available evaluation metrics. However, is the task about replicating the human-generated title? We argue that comparing the similarity of our generated title and the true title might not be the best approach. First of all, it is hard to know if the true title is created with the idea to summarize the article. It is possible to believe that journal articles title are created with some keywords or buzzwords in it, because it helps with getting attention and citations, which is especially important in academic journal articles. \n",
    "\n",
    "#### 8.1.2 Semantic Similarity \n",
    "Second, our current metrics only measure word co-occurence between our generated and true titles using precision and recall. The semantic similarity between the generated and true titles is ignored. However, word co-occurrence might not be the best metrics, because as discussed eaerlier, titles of journal articles might be created with the goal of including some popular and commonly used buzzwords which might not always be in the main body of the article or abstract, which therefore, using abstract as the source text to generate the title might result in dissimilar title as the ground truth, although they might still share similar semantic meanings. One metric that is commonly used to evaluate document similarity is `doc2vec`, which similar as `word2vec`, it represents documents in embedding vectors and compare document similarity by calculating the cosine distance of the embedding of documents. We end up did not use `doc2vec` because we ran out of the time to train our own embedding, while the other available pre-trained `doc2vec` embedding was trained with news or Wikipedia text corpus, which we think could be too different than our dataset because of its scientific journal article nature. \n",
    "\n",
    "#### 8.1.3 Human Evaluation\n",
    "Another possible way to evaluate our generated title is to use human evaluation, for example, we can employ human/Amazon Mechanical Turkers to read the abstract of the article and evaluate both the generated title and true title in terms of subjective preference and evaluation of if the title summarizes the abstract. We can then compare the scorings of the two sets of titles and see if there is a big difference between them. However, human evaluation are time-consuming and expensive to collect and can introduce individual biases, which therefore, we did not use human to evaluate the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
