{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Title Generation </center></h1>\n",
    "<h2><center>Sequence-to-Sequence Text Summarization for Academic Journal Articles </center></h2>\n",
    "<center>Karina Huang, Abhimanyu Vasishth, Phoebe Wong </center>\n",
    "<center>AC209b: Advanced Topics in Data Science </center>\n",
    "<center>Spring 2019 </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import package dependencies\n",
    "import re\n",
    "import sys\n",
    "# import glove\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.utils import np_utils\n",
    "from collections import Counter\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.contrib import keras\n",
    "from keras.layers import Bidirectional, Dropout, Dense,LSTM,Input,Activation,Add,TimeDistributed,\\\n",
    "Permute,Flatten,RepeatVector,merge,Lambda,Multiply,Reshape, Concatenate, Dot\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import preprocessing\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Toggle on/off the raw code.\"></form>''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "### 1.1 Background and Motivation\n",
    "\n",
    "Generating titles is an interesting and non-trivial aspect of a few different domains, such as movie titles, song names, news headlines and academic paper titles. Our broad aim is to use different approaches to automate the title generation process. Literature has proposed various Natural Language Processing (NLP) models to advance automated title generation. Specifically, we compare the performance of different neural-net approaches against a non neural-net approach for this task.  \n",
    "\n",
    "We choose to start with the domain of academic papers. Our specific aim is to generate a title for a paper given the abstract. This falls under the broad goal of text summarization as the title of a paper can be thought of as a summary of the abstract. It is more important to have an informative title that is able to communicate the content of the paper succinctly in this domain, allowing us to compare the coherence of the generated title with respect to the body of text. Essentially, the title of a journal article serves as a summary of the paper, which is unique to the articles, for example, the link between movie names and movie scripts, or song names and song lyrics is relatively weaker, and would therefore require more intricate evaluation metrics. Within text summarization, there are two main approaches: \n",
    "\n",
    "1. Extractive Summarization: in this approach, important words from the abstract or the corpus are learned and used to create a title.\n",
    "2. Abstractive Summarization: in this approach, we aim to generate novel titles for a given abstract by learning a representation of the abstract in some latent space. We use Term Frequency-Inverse Document Frequency (TF-IDF) and deep learning methods to learn this latent embedding and compare between these two approaches.\n",
    "\n",
    "It is not inherently obvious whether a generated title is “good enough”. Our aim also includes exploring different evaluation strategies. Specifically, we want to evaluate the performance of our model using various metrics proposed in literature. \n",
    "\n",
    "### 1.2 Literature Review\n",
    "\n",
    "Vinyals et al. set up an end-to-end model consisting of a CNN for vision and an RNN for text-generation in order to generate image captions by maximizing the target description sentence given the training image. The CNN, essentially the \"encoder\", converts images into a fixed-length latent vector, which the RNN \"decodes\" to generate captions. Specifically, the authors use an LSTM in order to get around the vanishing and exploding gradient problem commonly faced by bare-bones RNNs. The authors propose a few different evaluation metrics including a human evaluation study on Amazon Mechanical Turk where each image and the corresponding caption is rated by 2 workers. Other evaluation strategies used include comparing the caption to the \"ground-truth\", essentially human-generated descriptions, and using the BLEU score, a metric which is a form of precision using n-grams between generated and reference sentences that is shown to correlate with human evaluations. An issue of note is the diversity in generation, with the possibility of model collapse, and the authors evaluate diversity by analyzing the top-15 generated sentences for each image. \n",
    "\n",
    "Jin et al. outline approaches not utilizing deep neural networks (such as Naïve Bayesian, a K-Nearest Neighbour, a TF-IDF and an iterative Expectation-Maximization method) an provides us with a few different alternatives that we can use to compare to our neural network based model. The strengths of the neural network model (we hope) relative to the weaknesses of this approach should also help motivate the use of neural networks in title-generation.\n",
    "\n",
    "Lopyrev et al set up an end-to-end model involving different types of Recurrent Neural Networks (RNNs) such as Long Short Term Memory (LSTM) models to predict news headlines given articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model - Nearest Neighbors with TF-IDF\n",
    "\n",
    "We rely on Term Frequency-Inverse Document Frequency (TF-IDF) for the creation of our baseline model. This can be split up into two components: \n",
    "\n",
    "1. Term Frequency: in this context, this concept refers to words that occur frequently in the abstract of an academic paper.\n",
    "2. Inverse Document Frequency: many words may occur frequently in the abstract of an academic paper, such as \"the\", \"a\" or \"and\", however these words may not convey any meaningful information about the abstract. Therefore, the concept of inverse document frequency looks at words that not only occur frequently in the abstract, but are also rare in other abstracts (which results in a notion of a frequency-inverse). \n",
    "\n",
    "We apply this process to the training set (abstracts) and use not only words (1-grams) but also bi-grams or two-word combinations to create a matrix with each row representing an abstract and each column representing an n-gram. A high-valued entry means that a particular n-gram has a high TF-IDF score in an abstract and vice versa. \n",
    "\n",
    "In order to predict a title, we use the TF-IDF vectorizer, fit on the training abstracts, to transform a given example abstract from the test set into a vector with the same dimensionality as the number of columns in the matrix. Then, we use Cosine similarity to find the abstract in the training set that is most similar to the abstract that is queried. We return the title of the abstract that matches best as our \"generated\" title prediction and repeat the process for the entire test corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7241, 7)\n",
      "(4638, 9)\n",
      "Number of training samples:  3339\n",
      "Number of validation samples:  371\n",
      "Number of test samples:  928\n"
     ]
    }
   ],
   "source": [
    "papers = pd.read_csv('../data/papers.csv')\n",
    "seed = 209\n",
    "papers_bl = papers.copy()\n",
    "papers_bl = preprocessing.preprocessing(papers_bl, dropnan=True)\n",
    "print(papers.shape)\n",
    "\n",
    "papers_bl = papers_bl[papers_bl['aLen'] <= 250]\n",
    "print(papers_bl.shape)\n",
    "\n",
    "abstracts_bl = papers_bl['abstract'].values\n",
    "titles_bl = papers_bl['title'].values\n",
    "prep_bl = preprocessing.processText(papers_bl[['title', 'abstract']].values.T)\n",
    "\n",
    "trainXb, testXb, trainYb, testYb = train_test_split(abstracts_bl, titles_bl, test_size= 0.2 , random_state=seed)\n",
    "trainXb, valXb, trainYb, valYb = train_test_split(trainXb, trainYb, test_size = 0.1 , random_state = seed)\n",
    "\n",
    "print('Number of training samples: ', len(trainXb))\n",
    "print('Number of validation samples: ', len(valXb))\n",
    "print('Number of test samples: ', len(testXb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tfidf_vectorizer(corpus, min_ngram=1, max_ngram=2, stop_words='english'):\n",
    "    '''Fits a vectorizer on a corpus using 1-grams and bi-grams'''\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(min_ngram,max_ngram), stop_words=stop_words)\n",
    "    return vectorizer, vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "def top_k_words(corpus, row, k, feature_array):\n",
    "    '''Useful function to get top-k words in a particular row of the corpus'''\n",
    "    tfidf_sorting = np.argsort(response[row].toarray()).flatten()[::-1]\n",
    "    return feature_array[tfidf_sorting][:k]\n",
    "\n",
    "def find_nearest_neighbor(matrix, vector):\n",
    "    '''Finding the closest neighbor in a matrix given a query vector using cosine similarity'''\n",
    "    X = vector.reshape(-1,1)\n",
    "    Y = matrix.copy()\n",
    "    \n",
    "    # computing similarity\n",
    "    similarity = cosine_similarity(X.T,Y)\n",
    "    match = np.argmax(similarity)\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of corpus: 3339\n",
      "Shape of TF-IDF matrix: (3339, 200498)\n"
     ]
    }
   ],
   "source": [
    "# creating corpus\n",
    "corpus = trainXb.copy()\n",
    "print('Length of corpus: {}'.format(len(corpus)))\n",
    "\n",
    "# vectorizing using TF-IDF\n",
    "vectorizer, response = fit_tfidf_vectorizer(corpus)\n",
    "print('Shape of TF-IDF matrix: {}'.format(response.shape))\n",
    "\n",
    "# creating vector of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 928 done\n",
      "928 of 928 done\n"
     ]
    }
   ],
   "source": [
    "def predict_titles(abstracts, titles, vectorizer, response, response_titles):\n",
    "\n",
    "    true_titles = []\n",
    "    generated_titles = []\n",
    "    match_indices = []\n",
    "\n",
    "    for chosen in range(len(titles)):\n",
    "        if chosen % 1000 == 0:\n",
    "            print('{} of {} done'.format(chosen, len(titles)))\n",
    "    \n",
    "        query = vectorizer.transform([abstracts[chosen]])\n",
    "\n",
    "        # finding match using nearest neighbors on TF-IDF matrix \n",
    "        match = find_nearest_neighbor(response, query)\n",
    "        match_indices.append(match)\n",
    "\n",
    "        # generating titles\n",
    "        true_titles.append(titles[chosen])\n",
    "        generated_titles.append(response_titles[match])\n",
    "\n",
    "    print('{} of {} done'.format(len(titles), len(titles)))\n",
    "        \n",
    "    return true_titles, generated_titles, match_indices\n",
    "    \n",
    "    \n",
    "true_titles, generated_titles, match_indices = predict_titles(testXb, testYb, vectorizer, response, trainYb)\n",
    "\n",
    "# np.save('tfidf_truth.npy', true_titles)\n",
    "# np.save('tfidf_generated.npy', generated_titles)\n",
    "\n",
    "# print('saved files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Preprocessing for Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Cleaning\n",
    "The original [NIPS dataset](https://www.kaggle.com/benhamner/nips-2015-papers/version/2) acquired from Kaggle included XXXX journal articles, most of which were missing abstracts. Our first attempt to clean the data was to find and extract abstracts for articles missing the piece of information. The `getAbstract` code performs the search for abstract in two steps. These two steps were results of ad-hoc identification of abstract extractions and recovered 3,250 abstracts. We removed all articles missing abstracts and formatted the text for data cleaning. Due to computational constraints, we subsetted articles with abstract of length 250 in words for the current study. The final dataset used included 4,638 observations without missing data in title or abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatText(x):\n",
    "    \"render space in text and text to lowercase\"\n",
    "    for i in range(len(x)):\n",
    "        #check for data type\n",
    "        if type(x[i]) == str:\n",
    "            try:\n",
    "                x[i] = x[i].replace('\\n', ' ').lower()\n",
    "            except:\n",
    "                x[i] = x[i].lower()\n",
    "    return x\n",
    "\n",
    "def getAbstract(paper_text, methods = 1):\n",
    "    \"extract abstract from text in two steps\"\n",
    "    #step 1:\n",
    "    #find 'abstract' in text\n",
    "    #find the next word/phrase in all cap, wrapped in '\\n'\n",
    "    #extract everything in between as abstract\n",
    "    if methods == 1:\n",
    "        try:\n",
    "            #find abstract\n",
    "            a1 = re.search('abstract\\n', paper_text, re.IGNORECASE)\n",
    "            paper_text = paper_text[a1.end():]\n",
    "            #find the next section in all cap\n",
    "            a2 = re.search(r'\\n+[A-Z\\s]+\\n', paper_text)\n",
    "            return paper_text[: a2.start()]\n",
    "        except:\n",
    "            return np.nan\n",
    "    #step 2:\n",
    "    #find abstract in text\n",
    "    #find next item wrapped between '\\n\\n' and '\\n\\n'\n",
    "    #extract everything in between as abstract\n",
    "    if methods == 2:\n",
    "        try:\n",
    "            a1 = re.search('abstract\\n', paper_text, re.IGNORECASE)\n",
    "            paper_text = paper_text[a1.end():]\n",
    "            #find the next section in all cap\n",
    "            a2 = re.search(r'\\n\\n+.+\\n\\n', paper_text)\n",
    "            return paper_text[: a2.start()]\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "def preprocessing(papers, formatCols = ['title', 'abstract','paper_text'], dropnan = False):\n",
    "    \"preliminary data preprocessing for model fitting\"\n",
    "    #avoid modifying original dataset\n",
    "    papersNew = papers.copy()\n",
    "    #replace missing values with nan\n",
    "    papersNew.abstract = papersNew.abstract.apply(lambda x: np.nan if x == 'Abstract Missing' else x)\n",
    "    #extract missing abstract in two steps\n",
    "    #steps identified by ad-hoc examination of missing values\n",
    "    for m in [1, 2]:\n",
    "        #try searching for abstract in text if value is missing\n",
    "        papersNew['abstract_new'] = papersNew.paper_text.apply(lambda x: getAbstract(x, methods = m))\n",
    "        #replace nan in abstract with extracted abstract\n",
    "        papersNew.loc[papersNew.abstract.isnull(), 'abstract'] = papersNew.abstract_new\n",
    "        papersNew.drop(['abstract_new'], axis = 1, inplace = True)\n",
    "    #format columns of interest\n",
    "    papersNew[formatCols] = papersNew[formatCols].apply(lambda x: formatText(x), axis = 1)\n",
    "    if dropnan:\n",
    "        #drop na in abstract\n",
    "        papersNew = papersNew.dropna(subset = ['abstract'])\n",
    "        #append abstract and title length to data frame\n",
    "        papersNew ['aLen'] = papersNew.abstract.apply(lambda x: len(x.split(' ')))\n",
    "        papersNew ['tLen'] = papersNew.title.apply(lambda x: len(x.split(' ')))\n",
    "    return papersNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Used\n",
      "==================\n",
      "Number of observations:  4638\n",
      "Maximum title length:  20\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data = pd.read_csv('../data/papers.csv')\n",
    "#preprocessing\n",
    "dataNew = preprocessing(data, dropnan = True)\n",
    "#subset articles with a length less than or equal to 250\n",
    "data250 = dataNew[dataNew.aLen <= 250]\n",
    "print('Final Dataset Used')\n",
    "print('==================')\n",
    "print('Number of observations: ', data250.shape[0])\n",
    "print('Maximum title length: ', data250.tLen.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocessing for Model Training\n",
    "\n",
    "After cleaning the data, we processed the titles and abstracts using `processText` below. The class object records and returns:\n",
    "\n",
    "* number of unique words\n",
    "* maximum sequence length (should be 250 as titles are shorter than abstracts)\n",
    "* dictionaries for tokenization\n",
    "* tokenized vector of titles and abstracts\n",
    "\n",
    "Note that an important step of our tokenization was the qualification of rare, or unwanted, words. This is because NIPS articles are often written in laTex, and included scientific equations that may compromise the learning of important words. We approximated patterns of unwanted words and replaced them with an `<ign>` tag in the tokenization process. This resulted in 32,468 unique words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualify(word):\n",
    "    '''helper function to select words for tokenization.'''\n",
    "    #symbols\n",
    "    symbols = \"\"\"/?~`!@#$%^&*()_-+=|\\{}[];<>\"'.,:\"\"\"\n",
    "    #abbreviations\n",
    "    abb = \"\"\"e.g.,i.e.,etal.,\"\"\"\n",
    "    #disqualify empty space and words starting with symbol\n",
    "    if len(word) < 1 or word[0] in symbols:\n",
    "        return False\n",
    "    elif len(word) > 2:\n",
    "        #disqualify abbreviations\n",
    "        if word in abb:\n",
    "            return False\n",
    "        #otherwise count all combinations with length > 2\n",
    "        else:\n",
    "            return True\n",
    "    #if input length is one\n",
    "    #count only if it is 'a'\n",
    "    elif len(word) == 1:\n",
    "        if word in ['a', 'i']:\n",
    "            return True\n",
    "    #with input length of 2\n",
    "    #disqualify those with a symbol as the second character\n",
    "    elif len(word) == 2:\n",
    "        if word[1] not in symbols:\n",
    "            return True\n",
    "    #otherwise disqualify input\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "class processText:\n",
    "    '''\n",
    "    class object for data processing preperation for embedding training.\n",
    "\n",
    "    Parameters:\n",
    "    ===========\n",
    "    1) textVec: list of array-like, vector of text in strings\n",
    "\n",
    "    Methods:\n",
    "    ===========\n",
    "    1) updateMaxLen: count and update maximum sequence length\n",
    "    2) getDictionary: update dictionaries of words and tokens,\n",
    "        function called in `tokenize`\n",
    "    3) tokenize: return tokenized vector of text for model training\n",
    "    '''\n",
    "    def __init__(self, textVec):\n",
    "\n",
    "        #initiate class object\n",
    "        self.textVec = list()\n",
    "        for vec in textVec:\n",
    "            #string to list\n",
    "            vec = [x.strip().split(' ') for x in vec]\n",
    "            self.textVec.append(vec)\n",
    "\n",
    "        #prep  dictionaries for update\n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.maxLen = 0\n",
    "        self.nUnique = 0\n",
    "\n",
    "    def updateMaxLen(self):\n",
    "        for vec in self.textVec:\n",
    "            for txt in vec:\n",
    "                #get length of sequence\n",
    "                cntLen = len(txt)\n",
    "                #update maximum sequence length\n",
    "                if self.maxLen < cntLen:\n",
    "                    self.maxLen = cntLen\n",
    "\n",
    "    def getDictionary(self):\n",
    "\n",
    "        if len(self.word2idx) != 0:\n",
    "            print(\"Dictionary already updated.\")\n",
    "\n",
    "        else:\n",
    "            #initiate dictionary updates\n",
    "            #pad with 0\n",
    "            #end of sequence as 1\n",
    "            #ignored/disqualified words as 2\n",
    "            #start tokenization at 3\n",
    "            pad = 0\n",
    "            eos = 1\n",
    "            ign = 2\n",
    "            start = 3\n",
    "\n",
    "            self.word2idx['_'] = pad\n",
    "            self.word2idx['*'] = eos\n",
    "            self.word2idx['<ign>'] = ign\n",
    "\n",
    "            for vec in self.textVec:\n",
    "                for txt in vec:\n",
    "                    for w in txt:\n",
    "                        if qualify(w) == True:\n",
    "                            if w not in self.word2idx.keys():\n",
    "                                self.word2idx.update({w: start})\n",
    "                                start += 1\n",
    "\n",
    "            #update number of unique words in data set\n",
    "            self.nUnique = start - 3\n",
    "            #update idx to word dictionary\n",
    "            self.idx2word = dict((idx,word) for word,idx in self.word2idx.items())\n",
    "\n",
    "    def tokenize(self):\n",
    "        #get dictionaries if function hasn't been called\n",
    "        if len(self.word2idx) == 0:\n",
    "            self.getDictionary()\n",
    "        #cache list for tokenization\n",
    "        tokenizedVec = list()\n",
    "        for i in range(len(self.textVec)):\n",
    "            vec = self.textVec[i]\n",
    "            #cache list for the tokenized vector\n",
    "            tempVec = list()\n",
    "            for txt in vec:\n",
    "                #cache list for sequence\n",
    "                sVec = list()\n",
    "                for w in txt:\n",
    "                    #if word is in dictionary, tokenize\n",
    "                    if w in self.word2idx:\n",
    "                        sVec.append(self.word2idx[w])\n",
    "                    #if word not in dictionary, tag as ignored\n",
    "                    else:\n",
    "                        sVec.append(self.word2idx['<ign>'])\n",
    "                tempVec.append(sVec)\n",
    "            tokenizedVec.append(tempVec)\n",
    "\n",
    "        return tokenizedVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  32468\n",
      "Maxmimum sequence length:  250\n",
      "==============================================================================================================\n",
      "Example of tokenized title:\n",
      " [3, 4, 5, 6, 7, 8, 9] => ['self-organization', 'of', 'associative', 'database', 'and', 'its', 'applications']\n",
      "==============================================================================================================\n",
      "Example of tokenized abstract:\n",
      " [42, 466, 64, 4, 580, 5, 5497, 431, 5498, 5499, 51, 9, 19, 321, 5500, 5501, 58, 5498, 5497, 176, 5502, 3251, 503, 51, 309, 5503, 75, 58, 619, 5504, 1743, 4, 5505, 42, 61, 4, 3, 431, 5506, 368, 42, 1019, 4, 5507, 1727, 5508, 10, 289, 4072, 4, 21, 5509, 75, 58, 5510, 5504, 5511, 42, 5512, 19, 187, 1181, 92, 7, 122, 19, 42, 319, 320, 321, 159, 1391, 5513] => ['an', 'efficient', 'method', 'of', 'self-organizing', 'associative', 'databases', 'is', 'proposed', 'together', 'with', 'applications', 'to', 'robot', 'eyesight', 'systems.', 'the', 'proposed', 'databases', 'can', 'associate', 'any', 'input', 'with', 'some', 'output.', 'in', 'the', 'first', 'half', 'part', 'of', 'discussion,', 'an', 'algorithm', 'of', 'self-organization', 'is', 'proposed.', 'from', 'an', 'aspect', 'of', 'hardware,', 'it', 'produces', 'a', 'new', 'style', 'of', 'neural', 'network.', 'in', 'the', 'latter', 'half', 'part,', 'an', 'applicability', 'to', 'handwritten', 'letter', 'recognition', 'and', 'that', 'to', 'an', 'autonomous', 'mobile', 'robot', 'system', 'are', 'demonstrated.']\n"
     ]
    }
   ],
   "source": [
    "#tokenize data\n",
    "prep = processText(data250[['title', 'abstract']].values.T)\n",
    "#update sequence length\n",
    "prep.updateMaxLen()\n",
    "#get dictionaries of word and tags\n",
    "prep.getDictionary()\n",
    "word2idx = prep.word2idx\n",
    "idx2word = prep.idx2word\n",
    "\n",
    "print('Number of unique words: ', prep.nUnique)\n",
    "print('Maxmimum sequence length: ', prep.maxLen)\n",
    "print('='*110)\n",
    "\n",
    "#get tokenized vector of text\n",
    "txtTokenized = prep.tokenize()\n",
    "titles = txtTokenized[0]\n",
    "abstracts = txtTokenized[1]\n",
    "print('Example of tokenized title:\\n {0} => {1}'.format(titles[0], [prep.idx2word[i] for i in titles[0]]))\n",
    "print('='*110)\n",
    "print('Example of tokenized abstract:\\n {0} => {1}'.format(abstracts[0],[prep.idx2word[i] for i in abstracts[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split our dataset into training ($\\approx$72\\%), validation ($\\approx$8\\%) and test set ($\\approx$20\\%). For consistency, we set the random state to 209."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  3339\n",
      "Number of validation samples:  371\n",
      "Number of test samples:  928\n"
     ]
    }
   ],
   "source": [
    "#split data into train, validation, and test set\n",
    "trainX, testX, trainY, testY = train_test_split(abstracts, titles, test_size = 0.2 , random_state = 209)\n",
    "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size = 0.1 , random_state = 209)\n",
    "\n",
    "print('Number of training samples: ', len(trainX))\n",
    "print('Number of validation samples: ', len(valX))\n",
    "print('Number of test samples: ', len(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Generator for Model Training\n",
    "\n",
    "Because we need to feed input of consistent shape for model training, our past step of data preprocessing entails creating a generator that pads sequences to the maximum defined length. The code below was adapted from the [Computefest NLP workshop](https://github.com/Harvard-IACS/2019-computefest/blob/master/Friday/train_model.ipnb.ipynb). Due to computational constraint, we could not train our model with batch size larger than 32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for model training\n",
    "seed = 209\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "LR = 1e-4\n",
    "batch_size = 32\n",
    "num_train_batches = len(trainX) // batch_size\n",
    "num_val_samples = len(valX) + len(trainX) - batch_size*num_train_batches\n",
    "num_val_batches = len(valX) // batch_size\n",
    "total_entries = (num_train_batches + num_val_batches)*batch_size\n",
    "#number of unique tags\n",
    "nUnique = len(word2idx)\n",
    "#maximum length for title\n",
    "tMaxLen = 250\n",
    "#maximum length for abstract\n",
    "aMaxLen = 250\n",
    "#total maximum length\n",
    "maxlen = tMaxLen + aMaxLen\n",
    "batch_norm=False\n",
    "embeddDim = embeddMatrix.shape[1]\n",
    "nUnique = embeddMatrix.shape[0]\n",
    "hidden_units= embeddDim\n",
    "\n",
    "learning_rate = 0.002\n",
    "clip_norm = 1.0\n",
    "\n",
    "#padding function for abstracts\n",
    "def padAbstract(x, maxL = aMaxLen, dictionary = word2idx):\n",
    "    '''pad sequence for abstract'''\n",
    "    n = len(x)\n",
    "    #this section shouldn't apply \n",
    "    #because we subsetted our data\n",
    "    #so that the maximum sequence length is 250\n",
    "    if n > maxL:\n",
    "        x = x[-maxL:]\n",
    "        n = maxL\n",
    "    return [dictionary['_']]*(maxL - n) + x + [dictionary['*']]\n",
    "\n",
    "#build generator for model\n",
    "def generator(trainX, trainY, batch_size = batch_size, \n",
    "              nb_batches = None, model = None, seed = seed):\n",
    "    '''randomly shuffle input data'''\n",
    "    c = nb_batches if nb_batches else 0\n",
    "    while True:\n",
    "        titles = list()\n",
    "        abstracts = list()\n",
    "        if nb_batches and c >= nb_batches:\n",
    "            c = 0\n",
    "        new_seed = random.randint(0, sys.maxsize)\n",
    "        random.seed(c+123456789+seed)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            a = random.randint(0,len(trainX)-1)\n",
    "            \n",
    "            #random shuffling of data\n",
    "            abstract = trainX[a]\n",
    "            s = random.randint(min(aMaxLen,len(abstract)), max(aMaxLen,len(abstract)))\n",
    "            abstracts.append(abstract[:s])\n",
    "            \n",
    "            title = trainY[a]\n",
    "            s = random.randint(min(tMaxLen,len(title)), max(tMaxLen,len(title)))\n",
    "            titles.append(title[:s])\n",
    "\n",
    "        # undo the seeding before we yield in order not to affect the caller\n",
    "        c+= 1\n",
    "        random.seed(new_seed)\n",
    "\n",
    "        yield conv_seq_labels(abstracts, titles)\n",
    "\n",
    "#pad sequence and convert title to labels\n",
    "def conv_seq_labels(abstracts, titles, nflips = None, model = None, dictionary = word2idx):\n",
    "    \"\"\"Abstract and titles are converted to padded input vectors. Titles are one-hot encoded to labels.\"\"\"\n",
    "    batch_size = len(titles)\n",
    "    #pad sequence\n",
    "    x = [padAbstract(a)+t for a,t in zip(abstracts, titles)] \n",
    "    x = pad_sequences(x, maxlen = maxlen, value = dictionary['_'], \n",
    "                               padding = 'post', truncating = 'post')\n",
    "    \n",
    "    #one-hot encode titles for training\n",
    "    y = np.zeros((batch_size, tMaxLen, nUnique))\n",
    "    for i, it in enumerate(titles):\n",
    "        it = it + [dictionary['*']] + [dictionary['_']]*tMaxLen  \n",
    "        it = it[:tMaxLen]\n",
    "        y[i,:,:] = np_utils.to_categorical(it, nUnique)\n",
    "        \n",
    "    return [x[:,:aMaxLen],x[:,aMaxLen:]], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input Shape:  (32, 250)\n",
      "Decoder Input Shape:  (32, 250)\n",
      "One-hot encoded title shape:  (32, 250, 32471)\n",
      "==============================================================================================================\n",
      "Padded Abstract:\n",
      " ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'with', 'the', 'increase', 'in', 'available', 'data', 'parallel', 'machine', 'learning', 'has', '<ign>', '<ign>', 'become', 'an', 'increasingly', 'pressing', 'problem.', 'in', 'this', 'paper', 'we', 'present', '<ign>', '<ign>', 'the', 'first', 'parallel', 'stochastic', 'gradient', 'descent', 'algorithm', 'including', 'a', '<ign>', '<ign>', 'detailed', 'analysis', 'and', 'experimental', 'evidence.', 'unlike', 'prior', 'work', 'on', '<ign>', '<ign>', 'parallel', 'optimization', 'algorithms', 'our', '<ign>', '<ign>', 'variant', 'comes', 'with', 'parallel', 'acceleration', 'guarantees', 'and', 'it', 'poses', 'no', '<ign>', '<ign>', 'overly', 'tight', 'latency', 'constraints,', 'which', 'might', 'only', 'be', 'available', 'in', '<ign>', '<ign>', 'the', 'multicore', 'setting.', 'our', 'analysis', 'introduces', 'a', 'novel', 'proof', '<ign>', '<ign>', 'technique', '<ign>', 'contractive', 'mappings', 'to', 'quantify', 'the', '<ign>', '<ign>', 'speed', 'of', 'convergence', 'of', 'parameter', 'distributions', 'to', 'their', 'asymptotic', '<ign>', '<ign>', 'limits.', 'as', 'a', 'side', 'effect', 'this', 'answers', 'the', 'question', 'of', 'how', 'quickly', '<ign>', '<ign>', 'stochastic', 'gradient', 'descent', 'algorithms', 'reach', 'the', 'asymptotically', '<ign>', '<ign>', 'normal', 'regime.']\n",
      "==============================================================================================================\n",
      "Padded Title:\n",
      " ['*', 'parallelized', 'stochastic', 'gradient', 'descent', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "#demonstrate generator\n",
    "demo = next(generator(trainX, trainY, batch_size = batch_size))\n",
    "print('Encoder Input Shape: ', demo[0][0].shape)\n",
    "print('Decoder Input Shape: ', demo[0][1].shape)\n",
    "print('One-hot encoded title shape: ', demo[1].shape)\n",
    "print('='*110)\n",
    "print(\"Padded Abstract:\\n\", [idx2word[i] for i in demo[0][0][1]])\n",
    "print('='*110)\n",
    "print(\"Padded Title:\\n\", [idx2word[i] for i in demo[0][1][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding karina\n",
    "#word2vec phoebe\n",
    "#glove pre-trained abhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Word2vec \n",
    "\n",
    "Word2vec (by Mikolov et al., 2013[1]) model, similar to other word embedding models, is used to learn about vector representations of words, also known as word embeddings. Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text. It comes in two flavors, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. Algorithmically, the two models are similar with a small differnce in focus. \n",
    "\n",
    "CBOW predicts target words (e.g. 'mat') from source context words ('the cat sits on the'), while the skip-gram does the inverse and predicts source context-words from the target words. In our model, we went with the CBOW approach, because it is recommended to work better for smaller datasets[2].\n",
    "\n",
    "To make our embeddings comparable with other embeddings, we limit the number of embedding vectors to be 100. Because not all words in our dataset has a feature vector pre-trained in the Word2Vec corpus, we needed to pad in zeros for those words. In other words, for words that are unseen in the word2vec model (2.9% of unique words in our dataset), their embedding vectors are vectors of 0. With 32,471 unique words, we resulted in an embedding matrix with a dimension of (32471, 100).\n",
    "\n",
    "Finally, word2vec can tell us the similarity of words by calculating the cosine distance between the embedding vectors of the two words\n",
    "\n",
    "Reference: \n",
    "\n",
    "[1] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).\n",
    "\n",
    "[2] https://www.tensorflow.org/tutorials/representation/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list that each element is an abstract (actual words)\n",
    "abstracts_list_word = []\n",
    "for i in range(len(abstracts)):\n",
    "    abstracts_list_word.append([prep.idx2word[word] for word in abstracts[i]])\n",
    "\n",
    "# Initiate and train the word2vec model using our dataset\n",
    "word_model = Word2Vec(abstracts_list_word, size=100, min_count=1, window=5, iter=100) \n",
    "# Initiate the model with the documents\n",
    "word_model.train(abstracts_list_word, total_examples=len(abstracts_list_word), epochs=10, compute_loss = True) \n",
    "\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If try to replicate the result, please load the pre-trained weight instead.\n",
    "# word_model = np.load(histPath+'embeddMatrix_word2vec_0512.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unseen vocab to the embedding matrix\n",
    "all_unique_words = list(prep.word2idx.keys())\n",
    "embeddMatrix = np.zeros(shape = (len(all_unique_words), 100)) # initiate with zero, with 32471 unique words\n",
    "\n",
    "for i, word in enumerate(all_unique_words):\n",
    "    try:\n",
    "        embeddMatrix[i] = word_model.wv.word_vec(word) # find the word in the vector space and store the embeddings \n",
    "    except KeyError: # unseen vocab stay with 0 by skippig \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking similar words:\n",
      "  model -> model, (0.72), approach (0.58), method (0.54), models (0.54), framework (0.52), mechanism (0.51), methodology (0.50), formulation (0.47)\n",
      "  network -> net (0.72), networks (0.70), network, (0.65), nets (0.61), dude, (0.60), networks, (0.58), network. (0.58), signaling. (0.56)\n",
      "  convolution -> transformation (0.49), de-convolution (0.46), filter, (0.44), operators (0.43), layer (0.43), non-linear (0.42), combinations (0.42), white-noise (0.42)\n",
      "  learning -> learning, (0.65), learning. (0.60), learning: (0.46), adaptation (0.44), translation (0.44), single-core (0.44), learning-based (0.44), teaching (0.41)\n",
      "  neural -> kwta (0.63), probabilities.\" (0.58), l(winner-take-all (0.58), formed; (0.58), context-independent (0.57), pretrain (0.56), disco (0.55), rbf (0.55)\n",
      "  barn -> owl (0.72), owls (0.58), owl. (0.58), map-like (0.56), heading (0.52), microstimulation (0.52), owl's (0.51), young (0.50)\n"
     ]
    }
   ],
   "source": [
    "# Example of similar word using the model\n",
    "print('Checking similar words:')\n",
    "for word in ['model', 'network', 'convolution', 'learning', 'neural', 'barn']:\n",
    "    most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.wv.most_similar(word)[:8])\n",
    "    print('  %s -> %s' % (word, most_similar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pre-trained GloVe Embeddings\n",
    "\n",
    "Global Vectors for Word Representation ([GloVe](https://nlp.stanford.edu/projects/glove/)) embeddings are a means, like Word2Vec, of moving from a high-dimensional space defined by the total number of words in the vocabulary to a smaller subspace. Specifically, we use a 100-dimensional embeddings. These word embeddings are generated using a corpus consisting of Wikipedia articles from 2014, Tweets from Twitter and the Gigaword dataset. Specifically, these embeddings are used on co-occurrences of words.\n",
    "\n",
    "Let us view the following image as an example of how these embeddings: \n",
    "\n",
    "![alt](https://nlp.stanford.edu/projects/glove/images/table.png)\n",
    "\n",
    "From the above table, we see that P(solid|ice) is high relative to that of 'gas' given 'ice' and 'fashion' given 'ice', since 'solid' and 'ice' co-occur frequently. The probability of 'water' given 'ice' is also large for similar reasons. The probability of 'gas' given 'steam' and 'water' given 'steam' are high, since these words frequently co-occur. However, when we take the ratios, we see that when k is 'solid', this occurs much more frequently with 'ice' than with 'steam' and 'steam' occurs much more frequently with 'gas' than with 'solid'. Therefore, the high ratio of 'solid' and the low ratio of 'gas' give us some idea about the properties of 'ice' being a 'solid' or being related to solids, and 'steam' being related to gases. 'water' co-occurs frequently with both so the probabilities cancel out, and 'fashion' does not co-occur with either, so these ratios cancel out too.\n",
    "\n",
    "The advantage of using pre-trained embeddings is that these embeddings consist of a large vocabulary trained on a diverse set of documents, however the disadvantage is that they may not be specific to the nature of our data and therefore may not adequately represent our domain-specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Self-Trained GloVe Embeddings\n",
    "\n",
    "One motivation to train our own embedding is that pre-trained word embeddings may not capture well the similarities and co-occurances between words in the current dataset. Because academic journal article come with many technical terms, it is possible that weights using pre-trained embeddings do not apply to these texts. As mentioned above, only half of the unique words in the current dataset were found in the pre-trained GloVe embeddings. Therefore, we experimented with training our own embedding matrix, in hopes that the initialized weights would help our models learn to summarize the abstracts more effectively. Note that the embedding matrix is only trained on the training examples split using the code in the data preprocessing section. This is because realistically we do not have access to the hold-out sets. Again, the trained embedding matrix dimension is (32471, 100); the height corresponds to the sum of the number of unique words and the number of special tags (`<eos>`, `<ign>`, `<pad>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words for embedding training:  27141\n"
     ]
    }
   ],
   "source": [
    "#get text for training\n",
    "#remove ignored/disqualified words\n",
    "#because we do not want to learn this tag\n",
    "embedd_trainX = [[idx2word[x] for x in v if x != 2] for v in trainX]\n",
    "embedd_trainY = [[idx2word[x] for x in v if x != 2] for v in trainY]\n",
    "embeddTxt = trainX + trainY\n",
    "\n",
    "#prep dictionary for embedding training\n",
    "#drop pad, eos, and ignore tag\n",
    "start = 0\n",
    "embeddDict = dict()\n",
    "for vec in embeddTxt:\n",
    "    for w in vec:\n",
    "        if w not in embeddDict.keys():\n",
    "            embeddDict[w] = start\n",
    "            start += 1\n",
    "\n",
    "print('Number of unique words for embedding training: ', len(embeddDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code below were used for embedding training\n",
    "#please see rnn_preprocessing.ipynb for the execution history\n",
    "\n",
    "# #train glove embedding \n",
    "# #creating a corpus object\n",
    "# corpus_ = glove.Corpus(dictionary = embeddDict) \n",
    "# #training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "# corpus_.fit(embeddTxt, window = 10)\n",
    "# #train embedding using corpus weight matrix created above \n",
    "# glove_ = glove.Glove(no_components = 100, learning_rate = 0.01, random_state = 209)\n",
    "# glove_.fit(corpus_.matrix, epochs=50, no_threads=10, verbose = True)\n",
    "# glove_.add_dictionary(corpus_.dictionary)\n",
    "\n",
    "# #embedding matrix\n",
    "# #initiate a matrix with shape \n",
    "# #(number of unique words in our dataset, latent dimension of embedding)\n",
    "# embeddMatrix = np.zeros((len(word2idx), 100))\n",
    "# #loop through trained embedding matrix to find weights of trained words\n",
    "# for i, w in enumerate(word2idx):\n",
    "#     try:\n",
    "#         embeddVec = glove_.word_vectors[glove_.dictionary[w]]\n",
    "#         embeddMatrix[i] = embeddVec\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words most similar to \"stochastic\"\n",
      "=========================================\n",
      "word: gradient | cosine similarity: 0.8991909690378893\n",
      "word: three-composite | cosine similarity: 0.8937557315973716\n",
      "word: descent | cosine similarity: 0.8744804215977376\n",
      "word: proximal | cosine similarity: 0.76864338053953\n",
      "word: optimization | cosine similarity: 0.7646848913256339\n",
      "word: descent. | cosine similarity: 0.7640788665473602\n",
      "word: accelerated | cosine similarity: 0.7629822276705301\n",
      "word: gradients | cosine similarity: 0.7624208397216201\n",
      "word: projected | cosine similarity: 0.7623820982466453\n"
     ]
    }
   ],
   "source": [
    "#load trained glove model\n",
    "glove_ = glove.Glove.load('rnn_training_history/glove_.model')\n",
    "#display 10 words most similar to 'stochastic' by glove training\n",
    "print('Top 10 words most similar to \"stochastic\"')\n",
    "print('=========================================')\n",
    "for (i,j) in glove_.most_similar(word = 'stochastic', number = 10):\n",
    "    print(\"word: {0} | cosine similarity: {1}\".format(i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RNN Model \n",
    "\n",
    "The illustration below shows the first Recurrent Neural Network model we trained. The encoder takes in the tokenized abstract, encodes it with one of the embedding matrix trained above, and learns the input text with a bidirectional LSTM. We chose the bidirectional LSTM in hope to better learn the syntax. The output bidirectional LSTM weights are then used to initialize states for LSTM learning of the title, which is encoded using the same embedding matrix choice. We did not think it is necessary to train the title using a bidirectional LSTM because presumably the information is already learned with the abstract, assuming that the combination of words in a title is well-represented in the respective abstract. This decoder LSTM output is then passed into a time-distributed dense layer, from which we get a vector of probabilities for each unique words in the full dataset.  \n",
    "\n",
    "![](rnn_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn model \n",
    "def getRNNModel(genTrain, genVal, embeddMatrix,\n",
    "                learning_rate, clip_norm, nUnique,\n",
    "                embeddDim, hidden_units, encoder_shape = aMaxLen,\n",
    "                decoder_shape = tMaxLen):\n",
    "    \n",
    "    '''\n",
    "    compile RNN Model.\n",
    "    \n",
    "    Parameters:\n",
    "    ===========\n",
    "    1) genTrain: training sample generator\n",
    "    2) genVal: validation sample generator\n",
    "    3) embeddMatrix: embedding matrix of choice, shape (32471, 100)\n",
    "    4) learning_rate\n",
    "    5) clip_norm\n",
    "    6) nUnique: number of unique words in dataset\n",
    "    7) embeddDim: number of latent features in embedding matrix\n",
    "    8) hidden_units: number of hidden units for layer\n",
    "    9) encoder_shape: should be the maximum length of abstract\n",
    "    10) decoder_shape: maximum length of title, we padded titles to \n",
    "        the same length as encoder_shape\n",
    "    \n",
    "    Returns:\n",
    "    ===========\n",
    "    compiled rnn model\n",
    "    '''\n",
    "    #ENCODER\n",
    "    #input shape as the vector of sequence, with length padded to 250\n",
    "    encoder_inputs = Input(shape = (encoder_shape, ), name = 'encoder_input')\n",
    "\n",
    "    #encode input with embedding layer\n",
    "    encoder_embedding = Embedding(nUnique, embeddDim,\n",
    "                                  input_length = encoder_shape,\n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = True,\n",
    "                                  name = 'encoder_embedd')(encoder_inputs)\n",
    "\n",
    "    #1-layer bidirectional LSTM\n",
    "    #add drop out for regularization\n",
    "    #return only states\n",
    "    encoder_lstm = Bidirectional(LSTM(hidden_units, dropout_U = 0.2,\n",
    "                                      dropout_W = 0.2 , return_state=True),name = 'encoder_bilstm')\n",
    "\n",
    "    #get states from Bi-LSTM\n",
    "    encoder_outputs, f_h, f_c, b_h, b_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "    #add final states together\n",
    "    #to initialize weights for decoder\n",
    "    state_hfinal=Add(name = 'add_hidden_states')([f_h, b_h])\n",
    "    state_cfinal=Add(name = 'add_cell_states')([f_c, b_c])\n",
    "\n",
    "    #save encoder states\n",
    "    encoder_states = [state_hfinal,state_cfinal]\n",
    "\n",
    "    #DECODER\n",
    "    decoder_inputs = Input(shape = (decoder_shape, ), name = 'decoder_input')\n",
    "\n",
    "    #encode decoder input with embedding matrix\n",
    "    decoder_embedding = Embedding(nUnique, embeddDim,\n",
    "                                  input_length = decoder_shape,\n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = True,\n",
    "                                  name = 'decoder_embedd')\n",
    "\n",
    "    #1-layer lstm\n",
    "    decoder_lstm = LSTM(hidden_units,return_sequences = True, return_state=True, name = 'decoder_lstm')\n",
    "\n",
    "    #save decoder outputs\n",
    "    decoder_outputs, s_h, s_c = decoder_lstm(decoder_embedding(decoder_inputs), \n",
    "                                             initial_state = encoder_states)\n",
    "    # decoder_dense = Dense(decoder_shape, activation='linear')\n",
    "\n",
    "    #time distributed layer, probability predictions for all unique words\n",
    "    decoder_time_distributed = TimeDistributed(Dense(nUnique),name = 'decoder_timedistributed')\n",
    "    decoder_activation = Activation('softmax', name = 'decoder_activation')\n",
    "    decoder_outputs = decoder_activation(decoder_time_distributed(decoder_outputs))\n",
    "\n",
    "    #MODEL\n",
    "    model = Model(inputs = [encoder_inputs,decoder_inputs], outputs = decoder_outputs)\n",
    "    rmsprop = RMSprop(lr = learning_rate, clipnorm = clip_norm)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = rmsprop)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedd (Embedding)      (None, 250, 100)     3247100     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bilstm (Bidirectional)  [(None, 200), (None, 160800      encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedd (Embedding)      (None, 250, 100)     3247100     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_hidden_states (Add)         (None, 100)          0           encoder_bilstm[0][1]             \n",
      "                                                                 encoder_bilstm[0][3]             \n",
      "__________________________________________________________________________________________________\n",
      "add_cell_states (Add)           (None, 100)          0           encoder_bilstm[0][2]             \n",
      "                                                                 encoder_bilstm[0][4]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 250, 100), ( 80400       decoder_embedd[0][0]             \n",
      "                                                                 add_hidden_states[0][0]          \n",
      "                                                                 add_cell_states[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_timedistributed (TimeDi (None, 250, 32471)   3279571     decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_activation (Activation) (None, 250, 32471)   0           decoder_timedistributed[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 10,014,971\n",
      "Trainable params: 10,014,971\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#generator for training and validation\n",
    "genTrain = generator(trainX, trainY, batch_size = batch_size)\n",
    "genVal =  generator(valX, valY, nb_batches = len(valX)// batch_size, batch_size = batch_size)\n",
    "#load embedding matrix\n",
    "#this corresponds to the self-trained glove embedding\n",
    "#for now we will use it for demo\n",
    "embeddMatrix = np.load('rnn_training_history/embeddMatrix.npy')\n",
    "#compile rnn model\n",
    "K.clear_session()\n",
    "rnn = getRNNModel(genTrain, genVal, embeddMatrix,\n",
    "                  learning_rate, clip_norm, nUnique,\n",
    "                  embeddDim, hidden_units)\n",
    "#output model summary\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RNN Model with Attention and Context Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explored the use of an attention mechanism in our RNN model after training the above model. Intuitively, this mechanism learns where the model should pay attention to in a sequence. The change to the original RNN model is relatively minor, where instead of using the decoder LSTM output for predictions, we combine the states output by the bidirectional LSTM from the encoder with that from the decoder LSTM. However, because the dimensionalities of outputs need to match for the attention mechanism, we chose to pass the forward LSTM output from the encoder into the attention layer for further learning. However, note that the states from decoder is still used for initialization of the decoder LSTM layer. The flowchart below shows a simple demonstration of the learning steps of the model incorporating attention and context mechanism. The operation of the mechanism is also fairly simple: \n",
    "\n",
    "* combine states from encoder forward LSTM and decoder LSTM through dot product, which comprises the attention weights\n",
    "* applies attention weights to encoder outputs through another matrix multiplication for learning with attention\n",
    "* combine attention outputs with decoder outputs for prediction\n",
    "\n",
    "The prediction is again passed into a time-distributed dense layer for probability predictions of each unique word at each index of a sequence. \n",
    "\n",
    "![](rnn_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttentionModel(genTrain, genVal, embeddMatrix,\n",
    "                      learning_rate, clip_norm, nUnique,\n",
    "                      embeddDim, hidden_units, encoder_shape = aMaxLen,\n",
    "                      decoder_shape = tMaxLen):\n",
    "\n",
    "    '''\n",
    "    RNN Model with added Attention/Context Mechanism.\n",
    "    Attention model code reference @ https://github.com/wanasit/katakana.git\n",
    "    '''\n",
    "\n",
    "    #ENCODER\n",
    "    #input shape as the vector of sequence, with length padded to 250\n",
    "    encoder_inputs = Input(shape = (encoder_shape, ), name = 'encoder_input')\n",
    "\n",
    "    #encode input with embedding layer\n",
    "    #do not mask 0s because the attention layer does not allow this\n",
    "    encoder_embedding = Embedding(nUnique, embeddDim,\n",
    "                                  input_length = encoder_shape,\n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = True,\n",
    "                                  name = 'encoder_embedd')(encoder_inputs)\n",
    "\n",
    "    #forward\n",
    "    encoder_lstm = LSTM(hidden_units, dropout_U = 0.2, dropout_W = 0.2, \n",
    "                        return_sequences = True, return_state=True, name = 'encoder_forward_lstm')\n",
    "    encoder_lstm_rev = LSTM(hidden_units, dropout_U = 0.2, dropout_W = 0.2,\n",
    "                            go_backwards = True, return_sequences = True, \n",
    "                            return_state=True, name = 'encoder_backward_lstm')\n",
    "\n",
    "    #get states from LSTM\n",
    "    encoder_outputs_f, h_f, c_f = encoder_lstm(encoder_embedding)\n",
    "    encoder_outputs_r, h_r, c_r = encoder_lstm_rev(encoder_embedding)\n",
    "\n",
    "    #save encoder states\n",
    "    state_hfinal=Add()([h_f, h_r])\n",
    "    state_cfinal=Add()([c_f, c_r])\n",
    "\n",
    "    #save encoder states\n",
    "    encoder_states = [state_hfinal,state_cfinal]\n",
    "\n",
    "    #DECODER\n",
    "    decoder_inputs = Input(shape = (decoder_shape, ), name = 'decoder_input')\n",
    "\n",
    "    #encode decoder input with embedding matrix\n",
    "    decoder_embedding = Embedding(nUnique, embeddDim,\n",
    "                                  input_length = decoder_shape,\n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = True,\n",
    "                                  name = 'decoder_embedd')\n",
    "    \n",
    "    #1-layer lstm\n",
    "    decoder_lstm = LSTM(hidden_units,return_sequences = True, return_state=True, name = 'decoder_lstm')\n",
    "\n",
    "    #save decoder outputs\n",
    "    decoder_outputs, s_h, s_c = decoder_lstm(decoder_embedding(decoder_inputs), initial_state = encoder_states)\n",
    "  \n",
    "    #ATTENTION\n",
    "    attention = Dot(axes = [2,2], name = 'attention')([decoder_outputs, encoder_outputs_f])\n",
    "    attention = Activation('softmax')(attention)\n",
    "    context = Dot(axes = [2,1], name = 'context')([attention, encoder_outputs_f])\n",
    "    decoder_combined_context = Concatenate(name = 'decoder_added_attention')([context, decoder_outputs])\n",
    "\n",
    "\n",
    "    #time distributed layer, probability predictions for all unique words\n",
    "    decoder_time_distributed = TimeDistributed(Dense(nUnique), name = 'decoder_timedistributed')\n",
    "    decoder_activation = Activation('softmax', name = 'decoder_activation')\n",
    "    decoder_outputs = decoder_activation(decoder_time_distributed(decoder_combined_context))\n",
    "\n",
    "    #MODEL\n",
    "    model = Model(inputs = [encoder_inputs,decoder_inputs], outputs = decoder_outputs)\n",
    "    rmsprop = RMSprop(lr = learning_rate, clipnorm = clip_norm)\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer = rmsprop)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedd (Embedding)      (None, 250, 100)     3247100     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_forward_lstm (LSTM)     [(None, 250, 100), ( 80400       encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_backward_lstm (LSTM)    [(None, 250, 100), ( 80400       encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedd (Embedding)      (None, 250, 100)     3247100     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 100)          0           encoder_forward_lstm[0][1]       \n",
      "                                                                 encoder_backward_lstm[0][1]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 100)          0           encoder_forward_lstm[0][2]       \n",
      "                                                                 encoder_backward_lstm[0][2]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 250, 100), ( 80400       decoder_embedd[0][0]             \n",
      "                                                                 add_1[0][0]                      \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention (Dot)                 (None, 250, 250)     0           decoder_lstm[0][0]               \n",
      "                                                                 encoder_forward_lstm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 250, 250)     0           attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "context (Dot)                   (None, 250, 100)     0           activation_1[0][0]               \n",
      "                                                                 encoder_forward_lstm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_added_attention (Concat (None, 250, 200)     0           context[0][0]                    \n",
      "                                                                 decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_timedistributed (TimeDi (None, 250, 32471)   6526671     decoder_added_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_activation (Activation) (None, 250, 32471)   0           decoder_timedistributed[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 13,262,071\n",
      "Trainable params: 13,262,071\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#compile rnn model\n",
    "K.clear_session()\n",
    "attention = getAttentionModel(genTrain, genVal, embeddMatrix,\n",
    "                              learning_rate, clip_norm, nUnique,\n",
    "                              embeddDim, hidden_units)\n",
    "#output model summary\n",
    "attention.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Predictions\n",
    "\n",
    "Our overall aim is to produce a sequence of words. This is different to predictions we have made in other components of this course, where our aim has often been to predict a class, probability or numeric value. Specifically, we want to output a sequence for a given input sequence. We use a greedy method as well as a non-greedy method. The details of the methods are outlined as follows: \n",
    "\n",
    "1. Greedy Method: We generate one word at a time. First, we generate the most likely word given the input sequence and then the most likely word given the previous word and the sequence and so on in a greedy fashion until an \\<eos> tag is predicted or we have exceeded a fixed number of iterations (20) to predict a maximum of 20 word titles. \n",
    "2. Latitude-Greedy Method/Non-Greedy Method: Instead of generating the most likely word at a time, we use a 'latitude' k and generate one of the top-k words at random. \n",
    "\n",
    "Both models, due to their greedy nature (even the latitude-greedy method ends up finding local optima), may end up generating sub-optimal titles overall. The specific disadvantage of the greedy method is that in many iterations, we found that the generated title contained the same word repeated many times (e.g. 'learning learning learning ...'). This is because the model seemed to learn that the word 'learning' was the most likely word and in a greedy fashion predicted the same word over and over. The latitude-greedy method allows for greater diversity in the titles generated and in doing so allows the prediction mechanism to access a different set of words and perhaps reach a better optima.\n",
    "\n",
    "**Future work**: our future work for this section includes implementing another search mechanism for prediction, named Beam Search. This is an extension of our Latitude-Greedy method that keeps a set of trees (i.e. considering multiple options at once as opposed to just one) and picks from the top candidates. The 'beam width' parameter allows this technique to choose from multiple words at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(model, seq, idx2word, maxLen, num_iteration, greedy = True, latitude = 5):\n",
    "    '''\n",
    "    Prediction for a given sequence. \n",
    "    \n",
    "    Parameters:\n",
    "    ===========\n",
    "    1)model: rnn model\n",
    "    2)seq: a single abstract, should be a vector of length 250\n",
    "    3)maxLen: maximum length of predicted title\n",
    "    4)idx2word: dictionary for index to word\n",
    "    5)greedy: default to greedy search predictions, otherwise beam search\n",
    "    6)latitude: for greedy search, how many top words to consider for random choice\n",
    "    '''\n",
    "    \n",
    "    #cache list of prediction\n",
    "    prediction = list()\n",
    "    #initiate title to be a vector of zeros\n",
    "    init = np.zeros(maxLen)\n",
    "             \n",
    "    #for maximum prediction length\n",
    "    for i in range(num_iteration):\n",
    "        #get prediction probabilities for all unique words\n",
    "        predRNN = model.predict([np.reshape(seq, (1, 250)), init.reshape(1, 250)])\n",
    "        \n",
    "        if greedy:\n",
    "\n",
    "            #update next title vector to be the predicted vector\n",
    "            idx = np.argmax(predRNN[0, i])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #get top number of words\n",
    "            idxV = np.argsort(predRNN[0, i])[-latitude: ]\n",
    "            #randomly choose from the top words\n",
    "            idx = np.random.choice(idxV)\n",
    "            if i == 0:\n",
    "                while idx == 1:\n",
    "                    idx = np.random.choice(idxV)\n",
    "        \n",
    "        #index to word\n",
    "        word = idx2word[idx]\n",
    "        init[i] = idx\n",
    "        #if eos tag is predicted\n",
    "        #break out of loop\n",
    "        if idx == 1:\n",
    "            break\n",
    "        prediction.append(word)\n",
    "            \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Evaluation Metrics\n",
    "Similar to classical classification task, we can evaluate our model performance using F-score, precision and recall. \n",
    "\n",
    "In the field of Natural Language Processing (NLP), it is common to use BLEU and ROUGE to measure precision and recall. \n",
    "\n",
    "### 7.2.1  BLEU (Precision)\n",
    "BLEU (BiLingual Evaluation Understudy) stands for  measures how well a candidate translation matches a set of reference translations by counting the percentage of n-grams in the candidate translation overlapping with the references. BLEU was first introduced in Papineni et. al. (2001)[1].\n",
    "\n",
    "Precision (BLEU) = $\\frac{\\text{# of words in the predicted and true title}}{\\text{# of words in the predicted title}}$\n",
    "\n",
    "### 7.2.2 ROUGE (Recall)\n",
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation, introduced in Lin, C. Y. (2004) [2]. It comes with mainly two metrics, ROUGE-N and ROUGE-L. \n",
    "\n",
    "ROUGE-N is a recall-related measure because the denominator of the equation is the total sum of the number of n-grams occurring at the reference summary side. \n",
    "\n",
    "ROUGE-N: Overlap of N-grams[2] between the system and reference summaries.\n",
    "- ROUGE-1 refers to the overlap of 1-gram (each word) between the true title and predicted title.\n",
    "- ROUGE-2 refers to the overlap of bigrams between the true title and predicted title.\n",
    "\n",
    "ROUGE-L: Longest Common Subsequence (LCS)[3] based statistics. Longest common subsequence problem takes into account sentence level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.\n",
    "\n",
    "Rouge applies in cases with multiple reference summary, however, because we have only one ground truth (i.e., one title), we will simplify the definition of rouge as following:\n",
    "\n",
    "Recall (ROUGE) = $\\frac{Count_{match}(gram_n)}{Count(gram_n)}$\n",
    "\n",
    "n stands for the length of the n-gram ($gram_n$), and $Count_{match}(gram_n)$ is the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries\"\n",
    "\n",
    "### 7.2.3 F-score\n",
    "$F = 2 * \\frac{Precision * Recall}{Precision + Recall}$\n",
    "\n",
    "Reference: \n",
    "\n",
    "[1] Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002, July). BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics (pp. 311-318). Association for Computational Linguistics.\n",
    "\n",
    "[2] Lin, C. Y. (2004). Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Test-set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative import of our functions\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "# import our rouge functions\n",
    "from myeval import one_gram_recall, ngrams, two_gram_recall\n",
    "\n",
    "# BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our model predictions\n",
    "# glove self-trained embedding result\n",
    "with open(\"glove_self_trained/predictions_base\",'rb') as f:\n",
    "    basePred = pickle.load(f)\n",
    "with open(\"glove_self_trained/predictions_attention\",'rb') as f:\n",
    "    attentionPred = pickle.load(f)\n",
    "\n",
    "# kNN\n",
    "knn_pred = np.load(\"kNN/baseline_generated.npy\")\n",
    "knn_truth = np.load(\"kNN/baseline_true.npy\")\n",
    "\n",
    "# word2vec\n",
    "with open(\"word2vec/predictions_base\",'rb') as f:\n",
    "    word2vec_basePred = pickle.load(f)\n",
    "# with open(\"word2vec/predictions_attention\",'rb') as f:\n",
    "#     word2vec_attnPred = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Baseline kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"kNN\"]\n",
    "metrics = ['rouge1', 'rouge2', 'bleu']\n",
    "kNN_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "\n",
    "for i in range(len(knn_pred)):\n",
    "    for model in models:\n",
    "        rouge1_res = one_gram_recall(knn_truth[i], knn_pred[i]) # calculate ROGUE with 1-gram\n",
    "        rouge2_res = two_gram_recall(knn_truth[i], knn_pred[i]) # calculate ROGUE with 2-gram\n",
    "        bleu = sentence_bleu(knn_truth[i], knn_pred[i], smoothing_function=SmoothingFunction().method3) # calculate BLEU\n",
    "\n",
    "        kNN_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "        kNN_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "        kNN_eval_dict[model]['bleu'].append(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe: self-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Greedy\", \"Non-Greedy\"]\n",
    "metrics = ['rouge1', 'rouge2', 'bleu']\n",
    "baseline_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "        \n",
    "for i in range(len(basePred['Truth'])):\n",
    "    for model in models:\n",
    "        rouge2_res = two_gram_recall(basePred['Truth'][i], basePred[model][i])\n",
    "        bleu = sentence_bleu(basePred['Truth'][i], basePred[model][i], smoothing_function=SmoothingFunction().method3)\n",
    "        \n",
    "        baseline_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "        baseline_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "        baseline_eval_dict[model]['bleu'].append(bleu)\n",
    "\n",
    "attention_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "\n",
    "for i in range(len(basePred['Truth'])):\n",
    "    for model in models:\n",
    "        rouge1_res = one_gram_recall(attentionPred['Truth'][i], attentionPred[model][i])\n",
    "        rouge2_res = two_gram_recall(attentionPred['Truth'][i], attentionPred[model][i])\n",
    "        bleu = sentence_bleu(attentionPred['Truth'][i], attentionPred[model][i], smoothing_function=SmoothingFunction().method3)\n",
    "        \n",
    "        attention_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "        attention_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "        attention_eval_dict[model]['bleu'].append(bleu)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Greedy\", \"Non-Greedy\"]\n",
    "metrics = ['rouge1', 'rouge2', 'bleu']\n",
    "word2vec_eval_dict = {model: {metric: [] for metric in metrics} for model in models}\n",
    "\n",
    "for i in range(len(word2vec_basePred['Truth'])):\n",
    "    for model in models:\n",
    "        rouge1_res = one_gram_recall(word2vec_basePred['Truth'][i], word2vec_basePred[model][i])\n",
    "        rouge2_res = two_gram_recall(word2vec_basePred['Truth'][i], word2vec_basePred[model][i])\n",
    "        bleu = sentence_bleu(word2vec_basePred['Truth'][i], word2vec_basePred[model][i], smoothing_function=SmoothingFunction().method3)\n",
    "        \n",
    "        word2vec_eval_dict[model]['rouge1'].append(rouge1_res)\n",
    "        word2vec_eval_dict[model]['rouge2'].append(rouge2_res)\n",
    "        word2vec_eval_dict[model]['bleu'].append(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the summary of our model performance on the test-set. We compared our predicted title to the ground-truth title of the test set (928 titles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                                      | ROUGE-1 (Recall) | ROUGE-2 (Recall) | BLEU (Precision) | F-score |\n",
    "|--------------------------------------------|------------------|------------------|------------------|---------|\n",
    "| k-Nearest-Neighbor                         | .1728            | .0454            | .0135            | .1728   |\n",
    "| GloVe (self-trained, greedy search)        | .0984            | .0136            | .0137            | .0241   |\n",
    "| GloVe (self-trained, non-greedy search)    | .1215            | .0058            | .0103            | .0189   |\n",
    "| Word2vec (self-trained, greedy search)     | .1060            | .0216            | .0116            | .0209   |\n",
    "| Word2vec (self-trained, non-greedy search) | .1294            | .0064            | .0110            | .0202   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we can see k-NN model has the highest test-set performance based on the recall, precision and F-score metrics. However, for metrics we picked in this project, k-NN has an advantage over other models because the titles generated from kNN are coming from the dataset, which provides an advantage of having a higher recall (because of the overlap in the keywords as well as common keywords used in title especially when the dataset is in a very particular domain (NeurIPS). Similar reason applies for higher precision as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating the attention mechanism in our model appeared to have rendered more diverse predictions than the model without such mechanism. However, the predictions are still far from perfect. One interpretation is that perhaps the discrepancy in sequence length in the current model is too large from the attention mechanism to have an effect. Because our titles are all within 20 word of length, we have a lot of paddings in the decoder inputs. Moreover, our abstract sequence is front-padded, whereas the title sequence is back-padded. Perhaps that this design also compromised the performance of the attention mechanism. Another possible caveat in our implementation of the attention mechanism is that we intialized the states of the decoder with encoder states. Perhaps that with the attention mechanism, this step is not necessary as we are combining the outputs in the attention layer. If the attention mechanism is indeed not performing well due to an excessive amount of padding in title, one idea to improve the attention mechanism is to employ a convolutional neural network with pooling to downplay the padding weights. Alternatively, the current attention mechanism might have worked best for translation tasks, where sequences are of roughly equal lengths, and we should explore other attention mechanisms that apply better to summarization tasks.  \n",
    "\n",
    "In addition to the two models reported above, we also explored using bidirectional layer for the decoder. Our original thought was that it would not be necessary to employ such layer in the decoder part; in fact, using a bidirectional layer for the decoder appeared to break the syntax learning. Our models using bidirectional LSTM in decoder (with and without attention) predicted more diverse words in comparison to the RNN model without attention above. However, the words were mostly disconnected, yielding more of a keyword prediction than an actual sentence prediction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
