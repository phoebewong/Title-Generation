{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Title Generation </center></h1>\n",
    "<h2><center>Sequence-to-Sequence Text Summarization for Academic Journal Articles </center></h2>\n",
    "<center>Karina Huang, Abhimanyu Vasishth, Phoebe Wong </center>\n",
    "<center>AC209b: Advanced Topics in Data Science </center>\n",
    "<center>Spring 2019 </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import package dependencies\n",
    "import re\n",
    "import sys\n",
    "import glove\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.utils import np_utils\n",
    "from collections import Counter\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.contrib import keras\n",
    "from keras.layers import Bidirectional, Dropout, Dense,LSTM,Input,Activation,Add,TimeDistributed,\\\n",
    "Permute,Flatten,RepeatVector,merge,Lambda,Multiply,Reshape, Concatenate, Dot\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model - Nearest Neighbors with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Preprocessing for Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Cleaning\n",
    "The original [NIPS dataset](https://www.kaggle.com/benhamner/nips-2015-papers/version/2) acquired from Kaggle included XXXX journal articles, most of which were missing abstracts. Our first attempt to clean the data was to find and extract abstracts for articles missing the piece of information. The `getAbstract` code performs the search for abstract in two steps. These two steps were results of ad-hoc identification of abstract extractions and recovered 3,250 abstracts. We removed all articles missing abstracts and formatted the text for data cleaning. Due to computational constraints, we subsetted articles with abstract of length 250 in words for the current study. The final dataset used included 4,638 observations without missing data in title or abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatText(x):\n",
    "    \"render space in text and text to lowercase\"\n",
    "    for i in range(len(x)):\n",
    "        #check for data type\n",
    "        if type(x[i]) == str:\n",
    "            try:\n",
    "                x[i] = x[i].replace('\\n', ' ').lower()\n",
    "            except:\n",
    "                x[i] = x[i].lower()\n",
    "    return x\n",
    "\n",
    "def getAbstract(paper_text, methods = 1):\n",
    "    \"extract abstract from text in two steps\"\n",
    "    #step 1:\n",
    "    #find 'abstract' in text\n",
    "    #find the next word/phrase in all cap, wrapped in '\\n'\n",
    "    #extract everything in between as abstract\n",
    "    if methods == 1:\n",
    "        try:\n",
    "            #find abstract\n",
    "            a1 = re.search('abstract\\n', paper_text, re.IGNORECASE)\n",
    "            paper_text = paper_text[a1.end():]\n",
    "            #find the next section in all cap\n",
    "            a2 = re.search(r'\\n+[A-Z\\s]+\\n', paper_text)\n",
    "            return paper_text[: a2.start()]\n",
    "        except:\n",
    "            return np.nan\n",
    "    #step 2:\n",
    "    #find abstract in text\n",
    "    #find next item wrapped between '\\n\\n' and '\\n\\n'\n",
    "    #extract everything in between as abstract\n",
    "    if methods == 2:\n",
    "        try:\n",
    "            a1 = re.search('abstract\\n', paper_text, re.IGNORECASE)\n",
    "            paper_text = paper_text[a1.end():]\n",
    "            #find the next section in all cap\n",
    "            a2 = re.search(r'\\n\\n+.+\\n\\n', paper_text)\n",
    "            return paper_text[: a2.start()]\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "def preprocessing(papers, formatCols = ['title', 'abstract','paper_text'], dropnan = False):\n",
    "    \"preliminary data preprocessing for model fitting\"\n",
    "    #avoid modifying original dataset\n",
    "    papersNew = papers.copy()\n",
    "    #replace missing values with nan\n",
    "    papersNew.abstract = papersNew.abstract.apply(lambda x: np.nan if x == 'Abstract Missing' else x)\n",
    "    #extract missing abstract in two steps\n",
    "    #steps identified by ad-hoc examination of missing values\n",
    "    for m in [1, 2]:\n",
    "        #try searching for abstract in text if value is missing\n",
    "        papersNew['abstract_new'] = papersNew.paper_text.apply(lambda x: getAbstract(x, methods = m))\n",
    "        #replace nan in abstract with extracted abstract\n",
    "        papersNew.loc[papersNew.abstract.isnull(), 'abstract'] = papersNew.abstract_new\n",
    "        papersNew.drop(['abstract_new'], axis = 1, inplace = True)\n",
    "    #format columns of interest\n",
    "    papersNew[formatCols] = papersNew[formatCols].apply(lambda x: formatText(x), axis = 1)\n",
    "    if dropnan:\n",
    "        #drop na in abstract\n",
    "        papersNew = papersNew.dropna(subset = ['abstract'])\n",
    "        #append abstract and title length to data frame\n",
    "        papersNew ['aLen'] = papersNew.abstract.apply(lambda x: len(x.split(' ')))\n",
    "        papersNew ['tLen'] = papersNew.title.apply(lambda x: len(x.split(' ')))\n",
    "    return papersNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Used\n",
      "==================\n",
      "Number of observations:  4638\n",
      "Maximum title length:  20\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data = pd.read_csv('../data/papers.csv')\n",
    "#preprocessing\n",
    "dataNew = preprocessing(data, dropnan = True)\n",
    "#subset articles with a length less than or equal to 250\n",
    "data250 = dataNew[dataNew.aLen <= 250]\n",
    "print('Final Dataset Used')\n",
    "print('==================')\n",
    "print('Number of observations: ', data250.shape[0])\n",
    "print('Maximum title length: ', data250.tLen.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocessing for Model Training\n",
    "\n",
    "After cleaning the data, we processed the titles and abstracts using `processText` below. The class object records and returns:\n",
    "\n",
    "* number of unique words\n",
    "* maximum sequence length (should be 250 as titles are shorter than abstracts)\n",
    "* dictionaries for tokenization\n",
    "* tokenized vector of titles and abstracts\n",
    "\n",
    "Note that an important step of our tokenization was the qualification of rare, or unwanted, words. This is because NIPS articles are often written in laTex, and included scientific equations that may compromise the learning of important words. We approximated patterns of unwanted words and replaced them with an `<ign>` tag in the tokenization process. This resulted in 32,468 unique words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualify(word):\n",
    "    '''helper function to select words for tokenization.'''\n",
    "    #symbols\n",
    "    symbols = \"\"\"/?~`!@#$%^&*()_-+=|\\{}[];<>\"'.,:\"\"\"\n",
    "    #abbreviations\n",
    "    abb = \"\"\"e.g.,i.e.,etal.,\"\"\"\n",
    "    #disqualify empty space and words starting with symbol\n",
    "    if len(word) < 1 or word[0] in symbols:\n",
    "        return False\n",
    "    elif len(word) > 2:\n",
    "        #disqualify abbreviations\n",
    "        if word in abb:\n",
    "            return False\n",
    "        #otherwise count all combinations with length > 2\n",
    "        else:\n",
    "            return True\n",
    "    #if input length is one\n",
    "    #count only if it is 'a'\n",
    "    elif len(word) == 1:\n",
    "        if word in ['a', 'i']:\n",
    "            return True\n",
    "    #with input length of 2\n",
    "    #disqualify those with a symbol as the second character\n",
    "    elif len(word) == 2:\n",
    "        if word[1] not in symbols:\n",
    "            return True\n",
    "    #otherwise disqualify input\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "class processText:\n",
    "    '''\n",
    "    class object for data processing preperation for embedding training.\n",
    "\n",
    "    Parameters:\n",
    "    ===========\n",
    "    1) textVec: list of array-like, vector of text in strings\n",
    "\n",
    "    Methods:\n",
    "    ===========\n",
    "    1) updateMaxLen: count and update maximum sequence length\n",
    "    2) getDictionary: update dictionaries of words and tokens,\n",
    "        function called in `tokenize`\n",
    "    3) tokenize: return tokenized vector of text for model training\n",
    "    '''\n",
    "    def __init__(self, textVec):\n",
    "\n",
    "        #initiate class object\n",
    "        self.textVec = list()\n",
    "        for vec in textVec:\n",
    "            #string to list\n",
    "            vec = [x.strip().split(' ') for x in vec]\n",
    "            self.textVec.append(vec)\n",
    "\n",
    "        #prep  dictionaries for update\n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.maxLen = 0\n",
    "        self.nUnique = 0\n",
    "\n",
    "    def updateMaxLen(self):\n",
    "        for vec in self.textVec:\n",
    "            for txt in vec:\n",
    "                #get length of sequence\n",
    "                cntLen = len(txt)\n",
    "                #update maximum sequence length\n",
    "                if self.maxLen < cntLen:\n",
    "                    self.maxLen = cntLen\n",
    "\n",
    "    def getDictionary(self):\n",
    "\n",
    "        if len(self.word2idx) != 0:\n",
    "            print(\"Dictionary already updated.\")\n",
    "\n",
    "        else:\n",
    "            #initiate dictionary updates\n",
    "            #pad with 0\n",
    "            #end of sequence as 1\n",
    "            #ignored/disqualified words as 2\n",
    "            #start tokenization at 3\n",
    "            pad = 0\n",
    "            eos = 1\n",
    "            ign = 2\n",
    "            start = 3\n",
    "\n",
    "            self.word2idx['_'] = pad\n",
    "            self.word2idx['*'] = eos\n",
    "            self.word2idx['<ign>'] = ign\n",
    "\n",
    "            for vec in self.textVec:\n",
    "                for txt in vec:\n",
    "                    for w in txt:\n",
    "                        if qualify(w) == True:\n",
    "                            if w not in self.word2idx.keys():\n",
    "                                self.word2idx.update({w: start})\n",
    "                                start += 1\n",
    "\n",
    "            #update number of unique words in data set\n",
    "            self.nUnique = start - 3\n",
    "            #update idx to word dictionary\n",
    "            self.idx2word = dict((idx,word) for word,idx in self.word2idx.items())\n",
    "\n",
    "    def tokenize(self):\n",
    "        #get dictionaries if function hasn't been called\n",
    "        if len(self.word2idx) == 0:\n",
    "            self.getDictionary()\n",
    "        #cache list for tokenization\n",
    "        tokenizedVec = list()\n",
    "        for i in range(len(self.textVec)):\n",
    "            vec = self.textVec[i]\n",
    "            #cache list for the tokenized vector\n",
    "            tempVec = list()\n",
    "            for txt in vec:\n",
    "                #cache list for sequence\n",
    "                sVec = list()\n",
    "                for w in txt:\n",
    "                    #if word is in dictionary, tokenize\n",
    "                    if w in self.word2idx:\n",
    "                        sVec.append(self.word2idx[w])\n",
    "                    #if word not in dictionary, tag as ignored\n",
    "                    else:\n",
    "                        sVec.append(self.word2idx['<ign>'])\n",
    "                tempVec.append(sVec)\n",
    "            tokenizedVec.append(tempVec)\n",
    "\n",
    "        return tokenizedVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  32468\n",
      "Maxmimum sequence length:  250\n",
      "==============================================================================================================\n",
      "Example of tokenized title:\n",
      " [3, 4, 5, 6, 7, 8, 9] => ['self-organization', 'of', 'associative', 'database', 'and', 'its', 'applications']\n",
      "==============================================================================================================\n",
      "Example of tokenized abstract:\n",
      " [42, 466, 64, 4, 580, 5, 5497, 431, 5498, 5499, 51, 9, 19, 321, 5500, 5501, 58, 5498, 5497, 176, 5502, 3251, 503, 51, 309, 5503, 75, 58, 619, 5504, 1743, 4, 5505, 42, 61, 4, 3, 431, 5506, 368, 42, 1019, 4, 5507, 1727, 5508, 10, 289, 4072, 4, 21, 5509, 75, 58, 5510, 5504, 5511, 42, 5512, 19, 187, 1181, 92, 7, 122, 19, 42, 319, 320, 321, 159, 1391, 5513] => ['an', 'efficient', 'method', 'of', 'self-organizing', 'associative', 'databases', 'is', 'proposed', 'together', 'with', 'applications', 'to', 'robot', 'eyesight', 'systems.', 'the', 'proposed', 'databases', 'can', 'associate', 'any', 'input', 'with', 'some', 'output.', 'in', 'the', 'first', 'half', 'part', 'of', 'discussion,', 'an', 'algorithm', 'of', 'self-organization', 'is', 'proposed.', 'from', 'an', 'aspect', 'of', 'hardware,', 'it', 'produces', 'a', 'new', 'style', 'of', 'neural', 'network.', 'in', 'the', 'latter', 'half', 'part,', 'an', 'applicability', 'to', 'handwritten', 'letter', 'recognition', 'and', 'that', 'to', 'an', 'autonomous', 'mobile', 'robot', 'system', 'are', 'demonstrated.']\n"
     ]
    }
   ],
   "source": [
    "#tokenize data\n",
    "prep = processText(data250[['title', 'abstract']].values.T)\n",
    "#update sequence length\n",
    "prep.updateMaxLen()\n",
    "#get dictionaries of word and tags\n",
    "prep.getDictionary()\n",
    "word2idx = prep.word2idx\n",
    "idx2word = prep.idx2word\n",
    "\n",
    "print('Number of unique words: ', prep.nUnique)\n",
    "print('Maxmimum sequence length: ', prep.maxLen)\n",
    "print('='*110)\n",
    "\n",
    "#get tokenized vector of text\n",
    "txtTokenized = prep.tokenize()\n",
    "titles = txtTokenized[0]\n",
    "abstracts = txtTokenized[1]\n",
    "print('Example of tokenized title:\\n {0} => {1}'.format(titles[0], [prep.idx2word[i] for i in titles[0]]))\n",
    "print('='*110)\n",
    "print('Example of tokenized abstract:\\n {0} => {1}'.format(abstracts[0],[prep.idx2word[i] for i in abstracts[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split our dataset into training ($\\approx$72\\%), validation ($\\approx$8\\%) and test set ($\\approx$20\\%). For consistency, we set the random state to 209."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  3339\n",
      "Number of validation samples:  371\n",
      "Number of test samples:  928\n"
     ]
    }
   ],
   "source": [
    "#split data into train, validation, and test set\n",
    "trainX, testX, trainY, testY = train_test_split(abstracts, titles, test_size = 0.2 , random_state = 209)\n",
    "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size = 0.1 , random_state = 209)\n",
    "\n",
    "print('Number of training samples: ', len(trainX))\n",
    "print('Number of validation samples: ', len(valX))\n",
    "print('Number of test samples: ', len(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Generator for Model Training\n",
    "\n",
    "Because we need to feed input of consistent shape for model training, our past step of data preprocessing entails creating a generator that pads sequences to the maximum defined length. The code below was adapted from the [Computefest NLP workshop](https://github.com/Harvard-IACS/2019-computefest/blob/master/Friday/train_model.ipnb.ipynb). Due to computational constraint, we could not train our model with batch size larger than 32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for model training\n",
    "seed = 209\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "LR = 1e-4\n",
    "batch_size = 32\n",
    "num_train_batches = len(trainX) // batch_size\n",
    "num_val_samples = len(valX) + len(trainX) - batch_size*num_train_batches\n",
    "num_val_batches = len(valX) // batch_size\n",
    "total_entries = (num_train_batches + num_val_batches)*batch_size\n",
    "#number of unique tags\n",
    "nUnique = len(word2idx)\n",
    "#maximum length for title\n",
    "tMaxLen = 250\n",
    "#maximum length for abstract\n",
    "aMaxLen = 250\n",
    "#total maximum length\n",
    "maxlen = tMaxLen + aMaxLen\n",
    "batch_norm=False\n",
    "embeddDim = embeddMatrix.shape[1]\n",
    "nUnique = embeddMatrix.shape[0]\n",
    "hidden_units= embeddDim\n",
    "\n",
    "learning_rate = 0.002\n",
    "clip_norm = 1.0\n",
    "\n",
    "#padding function for abstracts\n",
    "def padAbstract(x, maxL = aMaxLen, dictionary = word2idx):\n",
    "    '''pad sequence for abstract'''\n",
    "    n = len(x)\n",
    "    #this section shouldn't apply \n",
    "    #because we subsetted our data\n",
    "    #so that the maximum sequence length is 250\n",
    "    if n > maxL:\n",
    "        x = x[-maxL:]\n",
    "        n = maxL\n",
    "    return [dictionary['_']]*(maxL - n) + x + [dictionary['*']]\n",
    "\n",
    "#build generator for model\n",
    "def generator(trainX, trainY, batch_size = batch_size, \n",
    "              nb_batches = None, model = None, seed = seed):\n",
    "    '''randomly shuffle input data'''\n",
    "    c = nb_batches if nb_batches else 0\n",
    "    while True:\n",
    "        titles = list()\n",
    "        abstracts = list()\n",
    "        if nb_batches and c >= nb_batches:\n",
    "            c = 0\n",
    "        new_seed = random.randint(0, sys.maxsize)\n",
    "        random.seed(c+123456789+seed)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            a = random.randint(0,len(trainX)-1)\n",
    "            \n",
    "            #random shuffling of data\n",
    "            abstract = trainX[a]\n",
    "            s = random.randint(min(aMaxLen,len(abstract)), max(aMaxLen,len(abstract)))\n",
    "            abstracts.append(abstract[:s])\n",
    "            \n",
    "            title = trainY[a]\n",
    "            s = random.randint(min(tMaxLen,len(title)), max(tMaxLen,len(title)))\n",
    "            titles.append(title[:s])\n",
    "\n",
    "        # undo the seeding before we yield in order not to affect the caller\n",
    "        c+= 1\n",
    "        random.seed(new_seed)\n",
    "\n",
    "        yield conv_seq_labels(abstracts, titles)\n",
    "\n",
    "#pad sequence and convert title to labels\n",
    "def conv_seq_labels(abstracts, titles, nflips = None, model = None, dictionary = word2idx):\n",
    "    \"\"\"Abstract and titles are converted to padded input vectors. Titles are one-hot encoded to labels.\"\"\"\n",
    "    batch_size = len(titles)\n",
    "    #pad sequence\n",
    "    x = [padAbstract(a)+t for a,t in zip(abstracts, titles)] \n",
    "    x = pad_sequences(x, maxlen = maxlen, value = dictionary['_'], \n",
    "                               padding = 'post', truncating = 'post')\n",
    "    \n",
    "    #one-hot encode titles for training\n",
    "    y = np.zeros((batch_size, tMaxLen, nUnique))\n",
    "    for i, it in enumerate(titles):\n",
    "        it = it + [dictionary['*']] + [dictionary['_']]*tMaxLen  \n",
    "        it = it[:tMaxLen]\n",
    "        y[i,:,:] = np_utils.to_categorical(it, nUnique)\n",
    "        \n",
    "    return [x[:,:aMaxLen],x[:,aMaxLen:]], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input Shape:  (32, 250)\n",
      "Decoder Input Shape:  (32, 250)\n",
      "One-hot encoded title shape:  (32, 250, 32471)\n",
      "==============================================================================================================\n",
      "Padded Abstract:\n",
      " ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'with', 'the', 'increase', 'in', 'available', 'data', 'parallel', 'machine', 'learning', 'has', '<ign>', '<ign>', 'become', 'an', 'increasingly', 'pressing', 'problem.', 'in', 'this', 'paper', 'we', 'present', '<ign>', '<ign>', 'the', 'first', 'parallel', 'stochastic', 'gradient', 'descent', 'algorithm', 'including', 'a', '<ign>', '<ign>', 'detailed', 'analysis', 'and', 'experimental', 'evidence.', 'unlike', 'prior', 'work', 'on', '<ign>', '<ign>', 'parallel', 'optimization', 'algorithms', 'our', '<ign>', '<ign>', 'variant', 'comes', 'with', 'parallel', 'acceleration', 'guarantees', 'and', 'it', 'poses', 'no', '<ign>', '<ign>', 'overly', 'tight', 'latency', 'constraints,', 'which', 'might', 'only', 'be', 'available', 'in', '<ign>', '<ign>', 'the', 'multicore', 'setting.', 'our', 'analysis', 'introduces', 'a', 'novel', 'proof', '<ign>', '<ign>', 'technique', '<ign>', 'contractive', 'mappings', 'to', 'quantify', 'the', '<ign>', '<ign>', 'speed', 'of', 'convergence', 'of', 'parameter', 'distributions', 'to', 'their', 'asymptotic', '<ign>', '<ign>', 'limits.', 'as', 'a', 'side', 'effect', 'this', 'answers', 'the', 'question', 'of', 'how', 'quickly', '<ign>', '<ign>', 'stochastic', 'gradient', 'descent', 'algorithms', 'reach', 'the', 'asymptotically', '<ign>', '<ign>', 'normal', 'regime.']\n",
      "==============================================================================================================\n",
      "Padded Title:\n",
      " ['*', 'parallelized', 'stochastic', 'gradient', 'descent', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "#demonstrate generator\n",
    "demo = next(generator(trainX, trainY, batch_size = batch_size))\n",
    "print('Encoder Input Shape: ', demo[0][0].shape)\n",
    "print('Decoder Input Shape: ', demo[0][1].shape)\n",
    "print('One-hot encoded title shape: ', demo[1].shape)\n",
    "print('='*110)\n",
    "print(\"Padded Abstract:\\n\", [idx2word[i] for i in demo[0][0][1]])\n",
    "print('='*110)\n",
    "print(\"Padded Title:\\n\", [idx2word[i] for i in demo[0][1][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding karina\n",
    "#word2vec phoebe\n",
    "#glove pre-trained abhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Self-Trained GloVe Embeddings\n",
    "\n",
    "One motivation to train our own embedding is that pre-trained word embeddings may not capture well the similarities and co-occurances between words in the current dataset. Because academic journal article come with many technical terms, it is possible that weights using pre-trained embeddings do not apply to these texts. As mentioned above, only half of the unique words in the current dataset were found in the pre-trained GloVe embeddings. Therefore, we experimented with training our own embedding matrix, in hopes that the initialized weights would help our models learn to summarize the abstracts more effectively. Note that the embedding matrix is only trained on the training examples split using the code in the data preprocessing section. This is because realistically we do not have access to the hold-out sets. Again, the trained embedding matrix dimension is (32471, 100); the height corresponds to the sum of the number of unique words and the number of special tags (`<eos>`, `<ign>`, `<pad>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words for embedding training:  27141\n"
     ]
    }
   ],
   "source": [
    "#get text for training\n",
    "#remove ignored/disqualified words\n",
    "#because we do not want to learn this tag\n",
    "embedd_trainX = [[idx2word[x] for x in v if x != 2] for v in trainX]\n",
    "embedd_trainY = [[idx2word[x] for x in v if x != 2] for v in trainY]\n",
    "embeddTxt = trainX + trainY\n",
    "\n",
    "#prep dictionary for embedding training\n",
    "#drop pad, eos, and ignore tag\n",
    "start = 0\n",
    "embeddDict = dict()\n",
    "for vec in embeddTxt:\n",
    "    for w in vec:\n",
    "        if w not in embeddDict.keys():\n",
    "            embeddDict[w] = start\n",
    "            start += 1\n",
    "\n",
    "print('Number of unique words for embedding training: ', len(embeddDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code below were used for embedding training\n",
    "#please see rnn_preprocessing.ipynb for the execution history\n",
    "\n",
    "# #train glove embedding \n",
    "# #creating a corpus object\n",
    "# corpus_ = glove.Corpus(dictionary = embeddDict) \n",
    "# #training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "# corpus_.fit(embeddTxt, window = 10)\n",
    "# #train embedding using corpus weight matrix created above \n",
    "# glove_ = glove.Glove(no_components = 100, learning_rate = 0.01, random_state = 209)\n",
    "# glove_.fit(corpus_.matrix, epochs=50, no_threads=10, verbose = True)\n",
    "# glove_.add_dictionary(corpus_.dictionary)\n",
    "\n",
    "# #embedding matrix\n",
    "# #initiate a matrix with shape \n",
    "# #(number of unique words in our dataset, latent dimension of embedding)\n",
    "# embeddMatrix = np.zeros((len(word2idx), 100))\n",
    "# #loop through trained embedding matrix to find weights of trained words\n",
    "# for i, w in enumerate(word2idx):\n",
    "#     try:\n",
    "#         embeddVec = glove_.word_vectors[glove_.dictionary[w]]\n",
    "#         embeddMatrix[i] = embeddVec\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words most similar to \"stochastic\"\n",
      "=========================================\n",
      "word: gradient | cosine similarity: 0.8991909690378893\n",
      "word: three-composite | cosine similarity: 0.8937557315973716\n",
      "word: descent | cosine similarity: 0.8744804215977376\n",
      "word: proximal | cosine similarity: 0.76864338053953\n",
      "word: optimization | cosine similarity: 0.7646848913256339\n",
      "word: descent. | cosine similarity: 0.7640788665473602\n",
      "word: accelerated | cosine similarity: 0.7629822276705301\n",
      "word: gradients | cosine similarity: 0.7624208397216201\n",
      "word: projected | cosine similarity: 0.7623820982466453\n"
     ]
    }
   ],
   "source": [
    "#load trained glove model\n",
    "glove_ = glove.Glove.load('rnn_training_history/glove_.model')\n",
    "#display 10 words most similar to 'stochastic' by glove training\n",
    "print('Top 10 words most similar to \"stochastic\"')\n",
    "print('=========================================')\n",
    "for (i,j) in glove_.most_similar(word = 'stochastic', number = 10):\n",
    "    print(\"word: {0} | cosine similarity: {1}\".format(i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RNN Model \n",
    "\n",
    "The illustration below shows the first Recurrent Neural Network model we trained. The encoder takes in the tokenized abstract, encodes it with one of the embedding matrix trained above, and learns the input text with a bidirectional LSTM. We chose the bidirectional LSTM in hope to better learn the syntax. The output bidirectional LSTM weights are then used to initialize states for LSTM learning of the title, which is encoded using the same embedding matrix choice. We did not think it is necessary to train the title using a bidirectional LSTM because presumably the information is already learned with the abstract, assuming that the combination of words in a title is well-represented in the respective abstract. This decoder LSTM output is then passed into a time-distributed dense layer, from which we get a vector of probabilities for each unique words in the full dataset.  \n",
    "\n",
    "![](rnn_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn model \n",
    "def getRNNModel(genTrain, genVal, embeddMatrix,\n",
    "                learning_rate, clip_norm, nUnique,\n",
    "                embeddDim, hidden_units, encoder_shape = aMaxLen,\n",
    "                decoder_shape = tMaxLen):\n",
    "    \n",
    "    '''\n",
    "    compile RNN Model.\n",
    "    \n",
    "    Parameters:\n",
    "    ===========\n",
    "    1) genTrain: training sample generator\n",
    "    2) genVal: validation sample generator\n",
    "    3) embeddMatrix: embedding matrix of choice, shape (32471, 100)\n",
    "    4) learning_rate\n",
    "    5) clip_norm\n",
    "    6) nUnique: number of unique words in dataset\n",
    "    7) embeddDim: number of latent features in embedding matrix\n",
    "    8) hidden_units: number of hidden units for layer\n",
    "    9) encoder_shape: should be the maximum length of abstract\n",
    "    10) decoder_shape: maximum length of title, we padded titles to \n",
    "        the same length as encoder_shape\n",
    "    \n",
    "    Returns:\n",
    "    ===========\n",
    "    compiled rnn model\n",
    "    '''\n",
    "    #ENCODER\n",
    "    #input shape as the vector of sequence, with length padded to 250\n",
    "    encoder_inputs = Input(shape = (encoder_shape, ), name = 'encoder_input')\n",
    "\n",
    "    #encode input with embedding layer\n",
    "    encoder_embedding = Embedding(nUnique, embeddDim,\n",
    "                                  input_length = encoder_shape,\n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = True,\n",
    "                                  name = 'encoder_embedd')(encoder_inputs)\n",
    "\n",
    "    #1-layer bidirectional LSTM\n",
    "    #add drop out for regularization\n",
    "    #return only states\n",
    "    encoder_lstm = Bidirectional(LSTM(hidden_units, dropout_U = 0.2,\n",
    "                                      dropout_W = 0.2 , return_state=True),name = 'encoder_bilstm')\n",
    "\n",
    "    #get states from Bi-LSTM\n",
    "    encoder_outputs, f_h, f_c, b_h, b_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "    #add final states together\n",
    "    #to initialize weights for decoder\n",
    "    state_hfinal=Add(name = 'add_hidden_states')([f_h, b_h])\n",
    "    state_cfinal=Add(name = 'add_cell_states')([f_c, b_c])\n",
    "\n",
    "    #save encoder states\n",
    "    encoder_states = [state_hfinal,state_cfinal]\n",
    "\n",
    "    #DECODER\n",
    "    decoder_inputs = Input(shape = (decoder_shape, ), name = 'decoder_input')\n",
    "\n",
    "    #encode decoder input with embedding matrix\n",
    "    decoder_embedding = Embedding(nUnique, embeddDim,\n",
    "                                  input_length = decoder_shape,\n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = True,\n",
    "                                  name = 'decoder_embedd')\n",
    "\n",
    "    #1-layer lstm\n",
    "    decoder_lstm = LSTM(hidden_units,return_sequences = True, return_state=True, name = 'decoder_lstm')\n",
    "\n",
    "    #save decoder outputs\n",
    "    decoder_outputs, s_h, s_c = decoder_lstm(decoder_embedding(decoder_inputs), \n",
    "                                             initial_state = encoder_states)\n",
    "    # decoder_dense = Dense(decoder_shape, activation='linear')\n",
    "\n",
    "    #time distributed layer, probability predictions for all unique words\n",
    "    decoder_time_distributed = TimeDistributed(Dense(nUnique),name = 'decoder_timedistributed')\n",
    "    decoder_activation = Activation('softmax', name = 'decoder_activation')\n",
    "    decoder_outputs = decoder_activation(decoder_time_distributed(decoder_outputs))\n",
    "\n",
    "    #MODEL\n",
    "    model = Model(inputs = [encoder_inputs,decoder_inputs], outputs = decoder_outputs)\n",
    "    rmsprop = RMSprop(lr = learning_rate, clipnorm = clip_norm)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = rmsprop)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedd (Embedding)      (None, 250, 100)     3247100     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_bilstm (Bidirectional)  [(None, 200), (None, 160800      encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedd (Embedding)      (None, 250, 100)     3247100     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_hidden_states (Add)         (None, 100)          0           encoder_bilstm[0][1]             \n",
      "                                                                 encoder_bilstm[0][3]             \n",
      "__________________________________________________________________________________________________\n",
      "add_cell_states (Add)           (None, 100)          0           encoder_bilstm[0][2]             \n",
      "                                                                 encoder_bilstm[0][4]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 250, 100), ( 80400       decoder_embedd[0][0]             \n",
      "                                                                 add_hidden_states[0][0]          \n",
      "                                                                 add_cell_states[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_timedistributed (TimeDi (None, 250, 32471)   3279571     decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_activation (Activation) (None, 250, 32471)   0           decoder_timedistributed[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 10,014,971\n",
      "Trainable params: 10,014,971\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#generator for training and validation\n",
    "genTrain = generator(trainX, trainY, batch_size = batch_size)\n",
    "genVal =  generator(valX, valY, nb_batches = len(valX)// batch_size, batch_size = batch_size)\n",
    "#load embedding matrix\n",
    "#this corresponds to the self-trained glove embedding\n",
    "#for now we will use it for demo\n",
    "embeddMatrix = np.load('rnn_training_history/embeddMatrix.npy')\n",
    "#compile rnn model\n",
    "K.clear_session()\n",
    "rnn = getRNNModel(genTrain, genVal, embeddMatrix,\n",
    "                  learning_rate, clip_norm, nUnique,\n",
    "                  embeddDim, hidden_units)\n",
    "#output model summary\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RNN Model with Attention and Context Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explored the use of an attention mechanism in our RNN model after training the above model. Intuitively, this mechanism learns where the model should pay attention to in a sequence. The change to the original RNN model is relatively minor, where instead of using the decoder LSTM output for predictions, we combine the states output by the bidirectional LSTM from the encoder with that from the decoder LSTM. However, because the dimensionalities of outputs need to match for the attention mechanism, we chose to pass the forward LSTM output from the encoder into the attention layer for further learning. However, note that the states from decoder is still used for initialization of the decoder LSTM layer. The flowchart below shows a simple demonstration of the learning steps of the model incorporating attention and context mechanism. The operation of the mechanism is also fairly simple: \n",
    "\n",
    "* combine states from encoder forward LSTM and decoder LSTM through dot product, which comprises the attention weights\n",
    "* applies attention weights to encoder outputs through another matrix multiplication for learning with attention\n",
    "* combine attention outputs with decoder outputs for prediction\n",
    "\n",
    "The prediction is again passed into a time-distributed dense layer for probability predictions of each unique word at each index of a sequence. \n",
    "\n",
    "![](rnn_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttentionModel(genTrain, genVal, embeddMatrix,\n",
    "                      learning_rate, clip_norm, nUnique,\n",
    "                      embeddDim, hidden_units, encoder_shape = aMaxLen,\n",
    "                      decoder_shape = tMaxLen):\n",
    "\n",
    "    '''\n",
    "    RNN Model with added Attention/Context Mechanism.\n",
    "    Attention model code reference @ https://github.com/wanasit/katakana.git\n",
    "    '''\n",
    "\n",
    "    #ENCODER\n",
    "    #input shape as the vector of sequence, with length padded to 250\n",
    "    encoder_inputs = Input(shape = (encoder_shape, ), name = 'encoder_input')\n",
    "\n",
    "    #encode input with embedding layer\n",
    "    #do not mask 0s because the attention layer does not allow this\n",
    "    encoder_embedding = Embedding(nUnique, embeddDim,\n",
    "                                  input_length = encoder_shape,\n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = True,\n",
    "                                  name = 'encoder_embedd')(encoder_inputs)\n",
    "\n",
    "    #forward\n",
    "    encoder_lstm = LSTM(hidden_units, dropout_U = 0.2, dropout_W = 0.2, \n",
    "                        return_sequences = True, return_state=True, name = 'encoder_forward_lstm')\n",
    "    encoder_lstm_rev = LSTM(hidden_units, dropout_U = 0.2, dropout_W = 0.2,\n",
    "                            go_backwards = True, return_sequences = True, \n",
    "                            return_state=True, name = 'encoder_backward_lstm')\n",
    "\n",
    "    #get states from LSTM\n",
    "    encoder_outputs_f, h_f, c_f = encoder_lstm(encoder_embedding)\n",
    "    encoder_outputs_r, h_r, c_r = encoder_lstm_rev(encoder_embedding)\n",
    "\n",
    "    #save encoder states\n",
    "    state_hfinal=Add()([h_f, h_r])\n",
    "    state_cfinal=Add()([c_f, c_r])\n",
    "\n",
    "    #save encoder states\n",
    "    encoder_states = [state_hfinal,state_cfinal]\n",
    "\n",
    "    #DECODER\n",
    "    decoder_inputs = Input(shape = (decoder_shape, ), name = 'decoder_input')\n",
    "\n",
    "    #encode decoder input with embedding matrix\n",
    "    decoder_embedding = Embedding(nUnique, embeddDim,\n",
    "                                  input_length = decoder_shape,\n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = True,\n",
    "                                  name = 'decoder_embedd')\n",
    "    \n",
    "    #1-layer lstm\n",
    "    decoder_lstm = LSTM(hidden_units,return_sequences = True, return_state=True, name = 'decoder_lstm')\n",
    "\n",
    "    #save decoder outputs\n",
    "    decoder_outputs, s_h, s_c = decoder_lstm(decoder_embedding(decoder_inputs), initial_state = encoder_states)\n",
    "  \n",
    "    #ATTENTION\n",
    "    attention = Dot(axes = [2,2], name = 'attention')([decoder_outputs, encoder_outputs_f])\n",
    "    attention = Activation('softmax')(attention)\n",
    "    context = Dot(axes = [2,1], name = 'context')([attention, encoder_outputs_f])\n",
    "    decoder_combined_context = Concatenate(name = 'decoder_added_attention')([context, decoder_outputs])\n",
    "\n",
    "\n",
    "    #time distributed layer, probability predictions for all unique words\n",
    "    decoder_time_distributed = TimeDistributed(Dense(nUnique), name = 'decoder_timedistributed')\n",
    "    decoder_activation = Activation('softmax', name = 'decoder_activation')\n",
    "    decoder_outputs = decoder_activation(decoder_time_distributed(decoder_combined_context))\n",
    "\n",
    "    #MODEL\n",
    "    model = Model(inputs = [encoder_inputs,decoder_inputs], outputs = decoder_outputs)\n",
    "    rmsprop = RMSprop(lr = learning_rate, clipnorm = clip_norm)\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer = rmsprop)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedd (Embedding)      (None, 250, 100)     3247100     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_forward_lstm (LSTM)     [(None, 250, 100), ( 80400       encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_backward_lstm (LSTM)    [(None, 250, 100), ( 80400       encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedd (Embedding)      (None, 250, 100)     3247100     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 100)          0           encoder_forward_lstm[0][1]       \n",
      "                                                                 encoder_backward_lstm[0][1]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 100)          0           encoder_forward_lstm[0][2]       \n",
      "                                                                 encoder_backward_lstm[0][2]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 250, 100), ( 80400       decoder_embedd[0][0]             \n",
      "                                                                 add_1[0][0]                      \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention (Dot)                 (None, 250, 250)     0           decoder_lstm[0][0]               \n",
      "                                                                 encoder_forward_lstm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 250, 250)     0           attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "context (Dot)                   (None, 250, 100)     0           activation_1[0][0]               \n",
      "                                                                 encoder_forward_lstm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_added_attention (Concat (None, 250, 200)     0           context[0][0]                    \n",
      "                                                                 decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_timedistributed (TimeDi (None, 250, 32471)   6526671     decoder_added_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_activation (Activation) (None, 250, 32471)   0           decoder_timedistributed[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 13,262,071\n",
      "Trainable params: 13,262,071\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#compile rnn model\n",
    "K.clear_session()\n",
    "attention = getAttentionModel(genTrain, genVal, embeddMatrix,\n",
    "                              learning_rate, clip_norm, nUnique,\n",
    "                              embeddDim, hidden_units)\n",
    "#output model summary\n",
    "attention.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phoebe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating the attention mechanism in our model appeared to have rendered more diverse predictions than the model without such mechanism. However, the predictions are still far from perfect. One interpretation is that perhaps the discrepancy in sequence length in the current model is too large from the attention mechanism to have an effect. Because our titles are all within 20 word of length, we have a lot of paddings in the decoder inputs. Moreover, our abstract sequence is front-padded, whereas the title sequence is back-padded. Perhaps that this design also compromised the performance of the attention mechanism. Another possible caveat in our implementation of the attention mechanism is that we intialized the states of the decoder with encoder states. Perhaps that with the attention mechanism, this step is not necessary as we are combining the outputs in the attention layer. If the attention mechanism is indeed not performing well due to an excessive amount of padding in title, one idea to improve the attention mechanism is to employ a convolutional neural network with pooling to downplay the padding weights. Alternatively, the current attention mechanism might have worked best for translation tasks, where sequences are of roughly equal lengths, and we should explore other attention mechanisms that apply better to summarization tasks.  \n",
    "\n",
    "In addition to the two models reported above, we also explored using bidirectional layer for the decoder. Our original thought was that it would not be necessary to employ such layer in the decoder part; in fact, using a bidirectional layer for the decoder appeared to break the syntax learning. Our models using bidirectional LSTM in decoder (with and without attention) predicted more diverse words in comparison to the RNN model without attention above. However, the words were mostly disconnected, yielding more of a keyword prediction than an actual sentence prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#karina\n",
    "#abhi\n",
    "#PHOEBE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
