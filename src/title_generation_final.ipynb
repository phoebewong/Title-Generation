{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Title Generation </center></h1>\n",
    "<h2><center>Sequence-to-Sequence Text Summarization for Academic Journal Articles </center></h2>\n",
    "<center>Karina Huang, Abhimanyu Vasishth, Phoebe Wong </center>\n",
    "<center>AC209b: Advanced Topics in Data Science </center>\n",
    "<center>Spring 2019 </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import package dependencies\n",
    "import re\n",
    "import glove\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model - Nearest Neighbors with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Preprocessing for Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Cleaning\n",
    "The original [NIPS dataset](https://www.kaggle.com/benhamner/nips-2015-papers/version/2) acquired from Kaggle included XXXX journal articles, most of which were missing abstracts. Our first attempt to clean the data was to find and extract abstracts for articles missing the piece of information. The `getAbstract` code performs the search for abstract in two steps. These two steps were results of ad-hoc identification of abstract extractions and recovered 3,250 abstracts. We removed all articles missing abstracts and formatted the text for data cleaning. Due to computational constraints, we subsetted articles with abstract of length 250 in words for the current study. The final dataset used included 4,638 observations without missing data in title or abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatText(x):\n",
    "    \"render space in text and text to lowercase\"\n",
    "    for i in range(len(x)):\n",
    "        #check for data type\n",
    "        if type(x[i]) == str:\n",
    "            try:\n",
    "                x[i] = x[i].replace('\\n', ' ').lower()\n",
    "            except:\n",
    "                x[i] = x[i].lower()\n",
    "    return x\n",
    "\n",
    "def getAbstract(paper_text, methods = 1):\n",
    "    \"extract abstract from text in two steps\"\n",
    "    #step 1:\n",
    "    #find 'abstract' in text\n",
    "    #find the next word/phrase in all cap, wrapped in '\\n'\n",
    "    #extract everything in between as abstract\n",
    "    if methods == 1:\n",
    "        try:\n",
    "            #find abstract\n",
    "            a1 = re.search('abstract\\n', paper_text, re.IGNORECASE)\n",
    "            paper_text = paper_text[a1.end():]\n",
    "            #find the next section in all cap\n",
    "            a2 = re.search(r'\\n+[A-Z\\s]+\\n', paper_text)\n",
    "            return paper_text[: a2.start()]\n",
    "        except:\n",
    "            return np.nan\n",
    "    #step 2:\n",
    "    #find abstract in text\n",
    "    #find next item wrapped between '\\n\\n' and '\\n\\n'\n",
    "    #extract everything in between as abstract\n",
    "    if methods == 2:\n",
    "        try:\n",
    "            a1 = re.search('abstract\\n', paper_text, re.IGNORECASE)\n",
    "            paper_text = paper_text[a1.end():]\n",
    "            #find the next section in all cap\n",
    "            a2 = re.search(r'\\n\\n+.+\\n\\n', paper_text)\n",
    "            return paper_text[: a2.start()]\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "def preprocessing(papers, formatCols = ['title', 'abstract','paper_text'], dropnan = False):\n",
    "    \"preliminary data preprocessing for model fitting\"\n",
    "    #avoid modifying original dataset\n",
    "    papersNew = papers.copy()\n",
    "    #replace missing values with nan\n",
    "    papersNew.abstract = papersNew.abstract.apply(lambda x: np.nan if x == 'Abstract Missing' else x)\n",
    "    #extract missing abstract in two steps\n",
    "    #steps identified by ad-hoc examination of missing values\n",
    "    for m in [1, 2]:\n",
    "        #try searching for abstract in text if value is missing\n",
    "        papersNew['abstract_new'] = papersNew.paper_text.apply(lambda x: getAbstract(x, methods = m))\n",
    "        #replace nan in abstract with extracted abstract\n",
    "        papersNew.loc[papersNew.abstract.isnull(), 'abstract'] = papersNew.abstract_new\n",
    "        papersNew.drop(['abstract_new'], axis = 1, inplace = True)\n",
    "    #format columns of interest\n",
    "    papersNew[formatCols] = papersNew[formatCols].apply(lambda x: formatText(x), axis = 1)\n",
    "    if dropnan:\n",
    "        #drop na in abstract\n",
    "        papersNew = papersNew.dropna(subset = ['abstract'])\n",
    "        #append abstract and title length to data frame\n",
    "        papersNew ['aLen'] = papersNew.abstract.apply(lambda x: len(x.split(' ')))\n",
    "        papersNew ['tLen'] = papersNew.title.apply(lambda x: len(x.split(' ')))\n",
    "    return papersNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Used\n",
      "==================\n",
      "Number of observations:  4638\n",
      "Maximum title length:  20\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data = pd.read_csv('../data/papers.csv')\n",
    "#preprocessing\n",
    "dataNew = preprocessing(data, dropnan = True)\n",
    "#subset articles with a length less than or equal to 250\n",
    "data250 = dataNew[dataNew.aLen <= 250]\n",
    "print('Final Dataset Used')\n",
    "print('==================')\n",
    "print('Number of observations: ', data250.shape[0])\n",
    "print('Maximum title length: ', data250.tLen.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocessing for Model Training\n",
    "\n",
    "After cleaning the data, we processed the titles and abstracts using `processText` below. The class object records and returns:\n",
    "\n",
    "* number of unique words\n",
    "* maximum sequence length (should be 250 as titles are shorter than abstracts)\n",
    "* dictionaries for tokenization\n",
    "* tokenized vector of titles and abstracts\n",
    "\n",
    "Note that an important step of our tokenization was the qualification of rare, or unwanted, words. This is because NIPS articles are often written in laTex, and included scientific equations that may compromise the learning of important words. We approximated patterns of unwanted words and replaced them with an `<ign>` tag in the tokenization process. This resulted in 32,468 unique words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualify(word):\n",
    "    '''helper function to select words for tokenization.'''\n",
    "    #symbols\n",
    "    symbols = \"\"\"/?~`!@#$%^&*()_-+=|\\{}[];<>\"'.,:\"\"\"\n",
    "    #abbreviations\n",
    "    abb = \"\"\"e.g.,i.e.,etal.,\"\"\"\n",
    "    #disqualify empty space and words starting with symbol\n",
    "    if len(word) < 1 or word[0] in symbols:\n",
    "        return False\n",
    "    elif len(word) > 2:\n",
    "        #disqualify abbreviations\n",
    "        if word in abb:\n",
    "            return False\n",
    "        #otherwise count all combinations with length > 2\n",
    "        else:\n",
    "            return True\n",
    "    #if input length is one\n",
    "    #count only if it is 'a'\n",
    "    elif len(word) == 1:\n",
    "        if word in ['a', 'i']:\n",
    "            return True\n",
    "    #with input length of 2\n",
    "    #disqualify those with a symbol as the second character\n",
    "    elif len(word) == 2:\n",
    "        if word[1] not in symbols:\n",
    "            return True\n",
    "    #otherwise disqualify input\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "class processText:\n",
    "    '''\n",
    "    class object for data processing preperation for embedding training.\n",
    "\n",
    "    Parameters:\n",
    "    ===========\n",
    "    1) textVec: list of array-like, vector of text in strings\n",
    "\n",
    "    Methods:\n",
    "    ===========\n",
    "    1) updateMaxLen: count and update maximum sequence length\n",
    "    2) getDictionary: update dictionaries of words and tokens,\n",
    "        function called in `tokenize`\n",
    "    3) tokenize: return tokenized vector of text for model training\n",
    "    '''\n",
    "    def __init__(self, textVec):\n",
    "\n",
    "        #initiate class object\n",
    "        self.textVec = list()\n",
    "        for vec in textVec:\n",
    "            #string to list\n",
    "            vec = [x.strip().split(' ') for x in vec]\n",
    "            self.textVec.append(vec)\n",
    "\n",
    "        #prep  dictionaries for update\n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.maxLen = 0\n",
    "        self.nUnique = 0\n",
    "\n",
    "    def updateMaxLen(self):\n",
    "        for vec in self.textVec:\n",
    "            for txt in vec:\n",
    "                #get length of sequence\n",
    "                cntLen = len(txt)\n",
    "                #update maximum sequence length\n",
    "                if self.maxLen < cntLen:\n",
    "                    self.maxLen = cntLen\n",
    "\n",
    "    def getDictionary(self):\n",
    "\n",
    "        if len(self.word2idx) != 0:\n",
    "            print(\"Dictionary already updated.\")\n",
    "\n",
    "        else:\n",
    "            #initiate dictionary updates\n",
    "            #pad with 0\n",
    "            #end of sequence as 1\n",
    "            #ignored/disqualified words as 2\n",
    "            #start tokenization at 3\n",
    "            pad = 0\n",
    "            eos = 1\n",
    "            ign = 2\n",
    "            start = 3\n",
    "\n",
    "            self.word2idx['_'] = pad\n",
    "            self.word2idx['*'] = eos\n",
    "            self.word2idx['<ign>'] = ign\n",
    "\n",
    "            for vec in self.textVec:\n",
    "                for txt in vec:\n",
    "                    for w in txt:\n",
    "                        if qualify(w) == True:\n",
    "                            if w not in self.word2idx.keys():\n",
    "                                self.word2idx.update({w: start})\n",
    "                                start += 1\n",
    "\n",
    "            #update number of unique words in data set\n",
    "            self.nUnique = start - 3\n",
    "            #update idx to word dictionary\n",
    "            self.idx2word = dict((idx,word) for word,idx in self.word2idx.items())\n",
    "\n",
    "    def tokenize(self):\n",
    "        #get dictionaries if function hasn't been called\n",
    "        if len(self.word2idx) == 0:\n",
    "            self.getDictionary()\n",
    "        #cache list for tokenization\n",
    "        tokenizedVec = list()\n",
    "        for i in range(len(self.textVec)):\n",
    "            vec = self.textVec[i]\n",
    "            #cache list for the tokenized vector\n",
    "            tempVec = list()\n",
    "            for txt in vec:\n",
    "                #cache list for sequence\n",
    "                sVec = list()\n",
    "                for w in txt:\n",
    "                    #if word is in dictionary, tokenize\n",
    "                    if w in self.word2idx:\n",
    "                        sVec.append(self.word2idx[w])\n",
    "                    #if word not in dictionary, tag as ignored\n",
    "                    else:\n",
    "                        sVec.append(self.word2idx['<ign>'])\n",
    "                tempVec.append(sVec)\n",
    "            tokenizedVec.append(tempVec)\n",
    "\n",
    "        return tokenizedVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  32468\n",
      "Maxmimum sequence length:  250\n",
      "==============================================================================================================\n",
      "Example of tokenized title:\n",
      " [3, 4, 5, 6, 7, 8, 9] => ['self-organization', 'of', 'associative', 'database', 'and', 'its', 'applications']\n",
      "==============================================================================================================\n",
      "Example of tokenized abstract:\n",
      " [42, 466, 64, 4, 580, 5, 5497, 431, 5498, 5499, 51, 9, 19, 321, 5500, 5501, 58, 5498, 5497, 176, 5502, 3251, 503, 51, 309, 5503, 75, 58, 619, 5504, 1743, 4, 5505, 42, 61, 4, 3, 431, 5506, 368, 42, 1019, 4, 5507, 1727, 5508, 10, 289, 4072, 4, 21, 5509, 75, 58, 5510, 5504, 5511, 42, 5512, 19, 187, 1181, 92, 7, 122, 19, 42, 319, 320, 321, 159, 1391, 5513] => ['an', 'efficient', 'method', 'of', 'self-organizing', 'associative', 'databases', 'is', 'proposed', 'together', 'with', 'applications', 'to', 'robot', 'eyesight', 'systems.', 'the', 'proposed', 'databases', 'can', 'associate', 'any', 'input', 'with', 'some', 'output.', 'in', 'the', 'first', 'half', 'part', 'of', 'discussion,', 'an', 'algorithm', 'of', 'self-organization', 'is', 'proposed.', 'from', 'an', 'aspect', 'of', 'hardware,', 'it', 'produces', 'a', 'new', 'style', 'of', 'neural', 'network.', 'in', 'the', 'latter', 'half', 'part,', 'an', 'applicability', 'to', 'handwritten', 'letter', 'recognition', 'and', 'that', 'to', 'an', 'autonomous', 'mobile', 'robot', 'system', 'are', 'demonstrated.']\n"
     ]
    }
   ],
   "source": [
    "#tokenize data\n",
    "prep = processText(data250[['title', 'abstract']].values.T)\n",
    "#update sequence length\n",
    "prep.updateMaxLen()\n",
    "#get dictionaries of word and tags\n",
    "prep.getDictionary()\n",
    "word2idx = prep.word2idx\n",
    "idx2word = prep.idx2word\n",
    "\n",
    "print('Number of unique words: ', prep.nUnique)\n",
    "print('Maxmimum sequence length: ', prep.maxLen)\n",
    "print('='*110)\n",
    "\n",
    "#get tokenized vector of text\n",
    "txtTokenized = prep.tokenize()\n",
    "titles = txtTokenized[0]\n",
    "abstracts = txtTokenized[1]\n",
    "print('Example of tokenized title:\\n {0} => {1}'.format(titles[0], [prep.idx2word[i] for i in titles[0]]))\n",
    "print('='*110)\n",
    "print('Example of tokenized abstract:\\n {0} => {1}'.format(abstracts[0],[prep.idx2word[i] for i in abstracts[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split our dataset into training ($\\approx$72\\%), validation ($\\approx$8\\%) and test set ($\\approx$20\\%). For consistency, we set the random state to 209."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  3339\n",
      "Number of validation samples:  371\n",
      "Number of test samples:  928\n"
     ]
    }
   ],
   "source": [
    "#split data into train, validation, and test set\n",
    "trainX, testX, trainY, testY = train_test_split(abstracts, titles, test_size = 0.2 , random_state = 209)\n",
    "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size = 0.1 , random_state = 209)\n",
    "\n",
    "print('Number of training samples: ', len(trainX))\n",
    "print('Number of validation samples: ', len(valX))\n",
    "print('Number of test samples: ', len(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding karina\n",
    "#word2vec phoebe\n",
    "#glove pre-trained abhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Self-Trained GloVe Embeddings\n",
    "\n",
    "One motivation to train our own embedding is that pre-trained word embeddings may not capture well the similarities and co-occurances between words in the current dataset. Because academic journal article come with many technical terms, it is possible that weights using pre-trained embeddings do not apply to these texts. As mentioned above, only half of the unique words in the current dataset were found in the pre-trained GloVe embeddings. Therefore, we experimented with training our own embedding matrix, in hopes that the initialized weights would help our models learn to summarize the abstracts more effectively. Note that the embedding matrix is only trained on the training examples split using the code in the data preprocessing section. This is because realistically we do not have access to the hold-out sets. Again, the trained embedding matrix dimension is (32,471, 100); the height corresponds to the sum of the number of unique words and the number of special tags (`<eos>`, `<ign>`, `<pad>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words for embedding training:  27141\n"
     ]
    }
   ],
   "source": [
    "#get text for training\n",
    "#remove ignored/disqualified words\n",
    "#because we do not want to learn this tag\n",
    "embedd_trainX = [[idx2word[x] for x in v if x != 2] for v in trainX]\n",
    "embedd_trainY = [[idx2word[x] for x in v if x != 2] for v in trainY]\n",
    "embeddTxt = trainX + trainY\n",
    "\n",
    "#prep dictionary for embedding training\n",
    "#drop pad, eos, and ignore tag\n",
    "start = 0\n",
    "embeddDict = dict()\n",
    "for vec in embeddTxt:\n",
    "    for w in vec:\n",
    "        if w not in embeddDict.keys():\n",
    "            embeddDict[w] = start\n",
    "            start += 1\n",
    "\n",
    "print('Number of unique words for embedding training: ', len(embeddDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code below were used for embedding training\n",
    "#please see rnn_preprocessing for the execution history\n",
    "\n",
    "# #train glove embedding \n",
    "# #creating a corpus object\n",
    "# corpus_ = glove.Corpus(dictionary = embeddDict) \n",
    "# #training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "# corpus_.fit(embeddTxt, window = 10)\n",
    "# #train embedding using corpus weight matrix created above \n",
    "# glove_ = glove.Glove(no_components = 100, learning_rate = 0.01, random_state = 209)\n",
    "# glove_.fit(corpus_.matrix, epochs=50, no_threads=10, verbose = True)\n",
    "# glove_.add_dictionary(corpus_.dictionary)\n",
    "\n",
    "# #embedding matrix\n",
    "# #initiate a matrix with shape \n",
    "# #(number of unique words in our dataset, latent dimension of embedding)\n",
    "# embeddMatrix = np.zeros((len(word2idx), 100))\n",
    "# #loop through trained embedding matrix to find weights of trained words\n",
    "# for i, w in enumerate(word2idx):\n",
    "#     try:\n",
    "#         embeddVec = glove_.word_vectors[glove_.dictionary[w]]\n",
    "#         embeddMatrix[i] = embeddVec\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words most similar to \"stochastic\"\n",
      "=========================================\n",
      "word: gradient | cosine similarity: 0.8991909690378893\n",
      "word: three-composite | cosine similarity: 0.8937557315973716\n",
      "word: descent | cosine similarity: 0.8744804215977376\n",
      "word: proximal | cosine similarity: 0.76864338053953\n",
      "word: optimization | cosine similarity: 0.7646848913256339\n",
      "word: descent. | cosine similarity: 0.7640788665473602\n",
      "word: accelerated | cosine similarity: 0.7629822276705301\n",
      "word: gradients | cosine similarity: 0.7624208397216201\n",
      "word: projected | cosine similarity: 0.7623820982466453\n"
     ]
    }
   ],
   "source": [
    "#load trained glove model\n",
    "glove_ = glove.Glove.load('rnn_training_history/glove_.model')\n",
    "#display 10 words most similar to 'stochastic' by glove training\n",
    "print('Top 10 words most similar to \"stochastic\"')\n",
    "print('=========================================')\n",
    "for (i,j) in glove_.most_similar(word = 'stochastic', number = 10):\n",
    "    print(\"word: {0} | cosine similarity: {1}\".format(i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#karina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RNN Model with Attention and Context Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#karina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phoebe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#karina\n",
    "#abhi\n",
    "#PHOEBE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
