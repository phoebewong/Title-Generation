{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils check \n",
    "import utils_updated\n",
    "from rnn import processText\n",
    "\n",
    "#import packages\n",
    "import string\n",
    "# import glove\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# # RNN\n",
    "# from keras.callbacks import LambdaCallback\n",
    "# from keras.layers.recurrent import LSTM\n",
    "# from keras.layers.embeddings import Embedding\n",
    "# from keras.layers import Dense, Activation\n",
    "# from keras.models import Sequential\n",
    "\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "      <th>aLen</th>\n",
       "      <th>tLen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>self-organization of associative database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>an efficient method of self-organizing associa...</td>\n",
       "      <td>767  self-organization of associative database...</td>\n",
       "      <td>73</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>a mean field theory of layer iv of visual cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>a single cell theory for the development of se...</td>\n",
       "      <td>683  a mean field theory of layer iv of visual...</td>\n",
       "      <td>91</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>storing covariance by the associative long-ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>in modeling studies or memory based on neural...</td>\n",
       "      <td>394  storing covariance by the associative lon...</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>bayesian query construction for neural network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>if data collection is costly, there is much to...</td>\n",
       "      <td>bayesian query construction for neural network...</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>neural network ensembles, cross validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>learning of continuous valued functions using ...</td>\n",
       "      <td>neural network ensembles, cross validation, an...</td>\n",
       "      <td>129</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  self-organization of associative database and ...        NaN   \n",
       "1    10  1987  a mean field theory of layer iv of visual cort...        NaN   \n",
       "2   100  1988  storing covariance by the associative long-ter...        NaN   \n",
       "3  1000  1994  bayesian query construction for neural network...        NaN   \n",
       "4  1001  1994  neural network ensembles, cross validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name  \\\n",
       "0  1-self-organization-of-associative-database-an...   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...   \n",
       "2  100-storing-covariance-by-the-associative-long...   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...   \n",
       "4  1001-neural-network-ensembles-cross-validation...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  an efficient method of self-organizing associa...   \n",
       "1  a single cell theory for the development of se...   \n",
       "2   in modeling studies or memory based on neural...   \n",
       "3  if data collection is costly, there is much to...   \n",
       "4  learning of continuous valued functions using ...   \n",
       "\n",
       "                                          paper_text  aLen  tLen  \n",
       "0  767  self-organization of associative database...    73     7  \n",
       "1  683  a mean field theory of layer iv of visual...    91    17  \n",
       "2  394  storing covariance by the associative lon...   300    15  \n",
       "3  bayesian query construction for neural network...    94     7  \n",
       "4  neural network ensembles, cross validation, an...   129     8  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv('../data/papers.csv')\n",
    "#preprocessing\n",
    "dataNew = utils_updated.preprocessing(data, dropnan = True)\n",
    "#check data## I. Data Preprocessing\n",
    "dataNew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4638, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for now we use articles with a length less than 250\n",
    "data250 = dataNew[dataNew.aLen <= 250]\n",
    "data250.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:  32468\n",
      "Maxmimum sequence length:  250\n"
     ]
    }
   ],
   "source": [
    "#tokenize data\n",
    "prep = processText(data250[['title', 'abstract']].values.T)\n",
    "#get dictionaries of word and tags\n",
    "prep.getDictionary()\n",
    "#update sequence length\n",
    "prep.updateMaxLen()\n",
    "print('Number of unique words: ', prep.nUnique)\n",
    "print('Maxmimum sequence length: ', prep.maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of tokenized title:\n",
      " [3, 4, 5, 6, 7, 8, 9] => ['self-organization', 'of', 'associative', 'database', 'and', 'its', 'applications']\n",
      "Example of tokenized abstract:\n",
      " [42, 466, 64, 4, 580, 5, 5497, 431, 5498, 5499, 51, 9, 19, 321, 5500, 5501, 58, 5498, 5497, 176, 5502, 3251, 503, 51, 309, 5503, 75, 58, 619, 5504, 1743, 4, 5505, 42, 61, 4, 3, 431, 5506, 368, 42, 1019, 4, 5507, 1727, 5508, 10, 289, 4072, 4, 21, 5509, 75, 58, 5510, 5504, 5511, 42, 5512, 19, 187, 1181, 92, 7, 122, 19, 42, 319, 320, 321, 159, 1391, 5513] => ['an', 'efficient', 'method', 'of', 'self-organizing', 'associative', 'databases', 'is', 'proposed', 'together', 'with', 'applications', 'to', 'robot', 'eyesight', 'systems.', 'the', 'proposed', 'databases', 'can', 'associate', 'any', 'input', 'with', 'some', 'output.', 'in', 'the', 'first', 'half', 'part', 'of', 'discussion,', 'an', 'algorithm', 'of', 'self-organization', 'is', 'proposed.', 'from', 'an', 'aspect', 'of', 'hardware,', 'it', 'produces', 'a', 'new', 'style', 'of', 'neural', 'network.', 'in', 'the', 'latter', 'half', 'part,', 'an', 'applicability', 'to', 'handwritten', 'letter', 'recognition', 'and', 'that', 'to', 'an', 'autonomous', 'mobile', 'robot', 'system', 'are', 'demonstrated.']\n"
     ]
    }
   ],
   "source": [
    "#get tokenized vector of text\n",
    "txtTokenized = prep.tokenize()\n",
    "titles = txtTokenized[0]\n",
    "abstracts = txtTokenized[1]\n",
    "print('Example of tokenized title:\\n {0} => {1}'.format(titles[0], [prep.idx2word[i] for i in titles[0]]))\n",
    "print('Example of tokenized abstract:\\n {0} => {1}'.format(abstracts[0],[prep.idx2word[i] for i in abstracts[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  3339\n",
      "Number of validation samples:  371\n",
      "Number of test samples:  928\n"
     ]
    }
   ],
   "source": [
    "#split data into train, validation, and test set\n",
    "trainX, testX, trainY, testY = train_test_split(abstracts, titles, test_size = 0.2 , random_state = 209)\n",
    "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size = 0.1 , random_state = 209)\n",
    "\n",
    "print('Number of training samples: ', len(trainX))\n",
    "print('Number of validation samples: ', len(valX))\n",
    "print('Number of test samples: ', len(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENSIM - Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a nested list of tokenized word in each abstract in the training set\n",
    "X_train_list_word = []\n",
    "for i in range(len(trainX)):\n",
    "    X_train_list_word.append([prep.idx2word[word] for word in trainX[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a nested list of tokenized word in each abstract in the training set\n",
    "X_test_list_word = []\n",
    "for i in range(len(testX)):\n",
    "    X_test_list_word.append([prep.idx2word[`word] for word in testX[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gensim.summarize to generate \"summary\" of abstracts as a proxy of title\n",
    "gen_summary = []\n",
    "for i, abstract in enumerate(X_train_list_word):\n",
    "    print(i)\n",
    "    temp_abs = ' '.join(abstract)\n",
    "    word_count = 1\n",
    "    \n",
    "    try:\n",
    "        temp_sum = summarize(temp_abs, ratio=None, word_count = word_count)\n",
    "\n",
    "    except:\n",
    "        temp_sum = []\n",
    "        gen_summary.append(temp_sum)\n",
    "        continue\n",
    "        \n",
    "    while temp_sum == '':\n",
    "        word_count += 1 # keep adding \"word_count\" until a summary is produced\n",
    "        temp_sum = summarize(temp_abs, ratio=None, word_count = word_count)\n",
    "        if word_count >= len(temp_abs):\n",
    "            temp_sum = []\n",
    "            gen_summary.append(temp_sum)\n",
    "            continue\n",
    "    \n",
    "    gen_summary.append(temp_sum)\n",
    "        \n",
    "## Test-set\n",
    "gen_summary_test = []\n",
    "for i, abstract in enumerate(X_test_list_word):\n",
    "    print(i)\n",
    "    temp_abs = ' '.join(abstract)\n",
    "    word_count = 1\n",
    "    \n",
    "    try:\n",
    "        temp_sum = summarize(temp_abs, ratio=None, word_count = word_count)\n",
    "\n",
    "    except:\n",
    "        temp_sum = []\n",
    "        gen_summary.append(temp_sum)\n",
    "        continue\n",
    "        \n",
    "    while temp_sum == '':\n",
    "        word_count += 1\n",
    "        temp_sum = summarize(temp_abs, ratio=None, word_count = word_count)\n",
    "        if word_count >= len(temp_abs):\n",
    "            temp_sum = []\n",
    "            gen_summary.append(temp_sum)\n",
    "            continue\n",
    "    \n",
    "    gen_summary_test.append(temp_sum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_path = 'baseline-gensim/gen_training'\n",
    "with open(pickle_path,'wb') as fp:\n",
    "    pickle.dump(gen_summary, fp)\n",
    "\n",
    "pickle_path = 'baseline-gensim/gen_test'\n",
    "with open(pickle_path,'wb') as fp:\n",
    "    pickle.dump(gen_summary_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07132585562266322"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(list(map(lambda x: x == [], gen_summary)))/len(gen_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0390032502708559"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(list(map(lambda x: x == [], gen_summary_test)))/len(gen_summary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
