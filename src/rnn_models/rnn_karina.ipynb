{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSbQIvnPx0W0"
   },
   "source": [
    "# RNN Model Training with Self-Trained GloVe Embedding\n",
    "\n",
    "**Karina Huang**\n",
    "\n",
    "**May 11, 2019**\n",
    "\n",
    "This notebook records the training history of all models explored for the Title Generation project. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sUjoOSJCx0W2",
    "outputId": "a98b5ac6-1673-422c-c7c4-dd91d6cf17cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#load packages\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from tensorflow.contrib import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "#models \n",
    "from rnn_model import getBaseModel, getBidirectionalModel, getAttentionModel, getAttentionLSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vav79cEmx0W9"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "histPath = ''\n",
    "#load training data\n",
    "with open(histPath+'train.txt', \"rb\") as f1, open(histPath+'val.txt', \"rb\") as f2, open(histPath+'test.txt', \"rb\") as f3: \n",
    "    trainX, trainY = pickle.load(f1)\n",
    "    valX, valY = pickle.load(f2)\n",
    "    testX, testY = pickle.load(f3)\n",
    "#load dictionaries\n",
    "with open(histPath+'word2idx_master.json', 'r') as f1, open(histPath+'idx2word_master.json', 'r') as f2:\n",
    "    word2idx = json.load(f1)\n",
    "    idx2word = json.load(f2)\n",
    "\n",
    "#load embedding matrix\n",
    "embeddMatrix = np.load(histPath+'embeddMatrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADJqSYpIx0W_"
   },
   "outputs": [],
   "source": [
    "#params for model training\n",
    "seed = 209\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "LR = 1e-4\n",
    "batch_size = 32\n",
    "\n",
    "num_train_batches = len(trainX) // batch_size\n",
    "num_val_samples = len(valX) + len(trainX) - batch_size*num_train_batches\n",
    "num_val_batches = len(valX) // batch_size\n",
    "total_entries = (num_train_batches + num_val_batches)*batch_size\n",
    "\n",
    "#maximum length for title \n",
    "tMaxLen = 250\n",
    "#maximum length for abstract\n",
    "aMaxLen = 250\n",
    "#total maximum length\n",
    "maxlen = tMaxLen + aMaxLen\n",
    "\n",
    "batch_norm=False\n",
    "\n",
    "embeddDim = embeddMatrix.shape[1]\n",
    "nUnique = embeddMatrix.shape[0]\n",
    "hidden_units= embeddDim\n",
    "\n",
    "learning_rate = 0.002\n",
    "clip_norm = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjLBQTBJx0XC"
   },
   "source": [
    "---\n",
    "\n",
    "## I. Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wy8dEpxKx0XD"
   },
   "outputs": [],
   "source": [
    "#padding function for abstracts\n",
    "def padAbstract(x, maxL = aMaxLen, dictionary = word2idx):\n",
    "    n = len(x)\n",
    "    if n > maxL:\n",
    "        x = x[-maxL:]\n",
    "        n = maxL\n",
    "    return [dictionary['_']]*(maxL - n) + x + [dictionary['*']]\n",
    "\n",
    "#build generator for model\n",
    "def generator(trainX, trainY, batch_size = batch_size, \n",
    "              nb_batches = None, model = None, seed = seed):\n",
    "    \n",
    "    c = nb_batches if nb_batches else 0\n",
    "    while True:\n",
    "        titles = list()\n",
    "        abstracts = list()\n",
    "        if nb_batches and c >= nb_batches:\n",
    "            c = 0\n",
    "        new_seed = random.randint(0, sys.maxsize)\n",
    "        random.seed(c+123456789+seed)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            a = random.randint(0,len(trainX)-1)\n",
    "            \n",
    "            #random shuffling of data\n",
    "            abstract = trainX[a]\n",
    "            s = random.randint(min(aMaxLen,len(abstract)), max(aMaxLen,len(abstract)))\n",
    "            abstracts.append(abstract[:s])\n",
    "            \n",
    "            title = trainY[a]\n",
    "            s = random.randint(min(tMaxLen,len(title)), max(tMaxLen,len(title)))\n",
    "            titles.append(title[:s])\n",
    "\n",
    "        # undo the seeding before we yield in order not to affect the caller\n",
    "        c+= 1\n",
    "        random.seed(new_seed)\n",
    "\n",
    "        yield conv_seq_labels(abstracts, titles)\n",
    "\n",
    "#pad sequence and convert title to labels\n",
    "def conv_seq_labels(abstracts, titles, nflips = None, model = None, dictionary = word2idx):\n",
    "    \"\"\"abstract and titles are converted to padded input vectors. Titles are one-hot encoded to labels.\"\"\"\n",
    "    batch_size = len(titles)\n",
    "    \n",
    "    \n",
    "    x = [padAbstract(a)+t for a,t in zip(abstracts, titles)] \n",
    "    x = sequence.pad_sequences(x, maxlen = maxlen, value = dictionary['_'], \n",
    "                               padding = 'post', truncating = 'post')\n",
    "        \n",
    "    y = np.zeros((batch_size, tMaxLen, nUnique))\n",
    "    for i, it in enumerate(titles):\n",
    "        it = it + [dictionary['*']] + [dictionary['_']]*tMaxLen  # output does have a eos at end\n",
    "        it = it[:tMaxLen]\n",
    "        y[i,:,:] = np_utils.to_categorical(it, nUnique)\n",
    "        \n",
    "    #The 3 inputs are abstract, title starting with eos and a one-hot encoding of the title categorical variables.\n",
    "    return [x[:,:aMaxLen],x[:,aMaxLen:]], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "Qu8vqCWfx0XF",
    "outputId": "ea483079-9a74-4c2f-ae97-3d68efcfb768",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 250) (32, 250) (32, 250, 32471)\n",
      "Abstract  :  ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'with', 'the', 'increase', 'in', 'available', 'data', 'parallel', 'machine', 'learning', 'has', '<ign>', '<ign>', 'become', 'an', 'increasingly', 'pressing', 'problem.', 'in', 'this', 'paper', 'we', 'present', '<ign>', '<ign>', 'the', 'first', 'parallel', 'stochastic', 'gradient', 'descent', 'algorithm', 'including', 'a', '<ign>', '<ign>', 'detailed', 'analysis', 'and', 'experimental', 'evidence.', 'unlike', 'prior', 'work', 'on', '<ign>', '<ign>', 'parallel', 'optimization', 'algorithms', 'our', '<ign>', '<ign>', 'variant', 'comes', 'with', 'parallel', 'acceleration', 'guarantees', 'and', 'it', 'poses', 'no', '<ign>', '<ign>', 'overly', 'tight', 'latency', 'constraints,', 'which', 'might', 'only', 'be', 'available', 'in', '<ign>', '<ign>', 'the', 'multicore', 'setting.', 'our', 'analysis', 'introduces', 'a', 'novel', 'proof', '<ign>', '<ign>', 'technique', '<ign>', 'contractive', 'mappings', 'to', 'quantify', 'the', '<ign>', '<ign>', 'speed', 'of', 'convergence', 'of', 'parameter', 'distributions', 'to', 'their', 'asymptotic', '<ign>', '<ign>', 'limits.', 'as', 'a', 'side', 'effect', 'this', 'answers', 'the', 'question', 'of', 'how', 'quickly', '<ign>', '<ign>', 'stochastic', 'gradient', 'descent', 'algorithms', 'reach', 'the', 'asymptotically', '<ign>', '<ign>', 'normal', 'regime.']\n",
      "Title  :  ['*', 'parallelized', 'stochastic', 'gradient', 'descent', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "#check generator\n",
    "check = next(generator(trainX, trainY, batch_size = batch_size))\n",
    "print(check[0][0].shape,check[0][1].shape,check[1].shape)\n",
    "print(\"Abstract  : \", [idx2word[str(i)] for i in check[0][0][1]])\n",
    "print(\"Title  : \", [idx2word[str(i)] for i in check[0][1][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsDy6pxNx0XJ"
   },
   "outputs": [],
   "source": [
    "#generator for training and validation\n",
    "genTrain = generator(trainX, trainY, batch_size = batch_size)\n",
    "genVal =  generator(valX, valY, nb_batches = len(valX)// batch_size, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-9jfFGRx0XM"
   },
   "source": [
    "---\n",
    "\n",
    "## II. Base Model\n",
    "\n",
    "Encoder-Decoder model, with a single bidirectional LSTM layer in encoder and a unidirectional LSTM layer in decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FETS5-o0w4O"
   },
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "\n",
    "# #base model\n",
    "# rnn_base = getBaseModel(genTrain, genVal, embeddMatrix, \n",
    "#                         learning_rate, clip_norm, nUnique,\n",
    "#                         embeddDim, hidden_units)\n",
    "# #base model summary\n",
    "# rnn_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #train base model\n",
    "# filepath = 'rnn_training_history/'\n",
    "\n",
    "# checkpoint = ModelCheckpoint(filepath + 'rnn_base.h5', monitor = 'val_loss', \n",
    "#                              verbose = 1, save_best_only = True, mode = 'min')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "# #fit base model\n",
    "# rnn_base.fit_generator(genTrain,\n",
    "#                        steps_per_epoch = num_train_batches,\n",
    "#                        epochs = 10, \n",
    "#                        validation_data = genVal,\n",
    "#                        validation_steps = num_val_batches,\n",
    "#                        callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save model weights\n",
    "# rnn_base.save_weights(filepath+'rnn_base_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedd (Embedding)      (None, 250, 100)     3247100     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) [(None, 200), (None, 160800      encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedd (Embedding)      (None, 250, 100)     3247100     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 100)          0           bidirectional_3[0][1]            \n",
      "                                                                 bidirectional_3[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 100)          0           bidirectional_3[0][2]            \n",
      "                                                                 bidirectional_3[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 250, 100), ( 80400       decoder_embedd[0][0]             \n",
      "                                                                 add_5[0][0]                      \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 250, 32471)   3279571     lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_activation (Activation) (None, 250, 32471)   0           time_distributed_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 10,014,971\n",
      "Trainable params: 10,014,971\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#load base model\n",
    "rnn_base = load_model('rnn_training_history/rnn_base.h5')\n",
    "rnn_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## III. Bidirectional Model\n",
    "\n",
    "Adjusted unidirectional layer in decoder to bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #train bidirectional model\n",
    "# rnn_bidirectional = getBidirectionalModel(genTrain, genVal, embeddMatrix, \n",
    "#                                           learning_rate, clip_norm, nUnique,\n",
    "#                                           embeddDim, hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # define the checkpoint\n",
    "# filepath = \"rnn_training_history/rnn_bidirectional_checkpoint.h5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "# #fit model\n",
    "# rnn_bidirectional.fit_generator(genTrain,\n",
    "#                                 steps_per_epoch = num_train_batches,\n",
    "#                                 epochs = 50, \n",
    "#                                 validation_data = genVal,\n",
    "#                                 validation_steps = num_val_batches,\n",
    "#                                 callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save model and model weights\n",
    "# rnn_bidirectional.save('rnn_training_history/rnn_bidirectional.h5')\n",
    "# rnn_bidirectional.save_weights('rnn_training_history/rnn_bidirectional_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedd (Embedding)      (None, 250, 100)     3247100     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedd (Embedding)      (None, 250, 100)     3247100     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 200), (None, 160800      encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 250, 200), ( 160800      decoder_embedd[0][0]             \n",
      "                                                                 bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 250, 32471)   6526671     bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_activation (Activation) (None, 250, 32471)   0           time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 13,342,471\n",
      "Trainable params: 13,342,471\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#load bidirectional model\n",
    "rnn_bidirectional = load_model('rnn_training_history/rnn_bidirectional_checkpoint.h5')\n",
    "rnn_bidirectional.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## IV. Attention Model\n",
    "\n",
    "Incorporated an attention/context mechanism in the base model. Attention mechanism combines forward-LSTM encoder outputs with the decoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #train attention model - LSTM\n",
    "# rnn_attention_lstm = getAttentionLSTMModel(genTrain, genVal, embeddMatrix, \n",
    "#                                            learning_rate, clip_norm, nUnique,\n",
    "#                                            embeddDim, hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # define the checkpoint\n",
    "# filepath = \"rnn_training_history/rnn_attention_lstm_checkpoint.h5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "# #fit model\n",
    "# rnn_attention_lstm.fit_generator(genTrain,\n",
    "#                                  steps_per_epoch = num_train_batches,\n",
    "#                                  epochs = 20, \n",
    "#                                  validation_data = genVal,\n",
    "#                                  validation_steps = num_val_batches,\n",
    "#                                  callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save model and model weights\n",
    "# rnn_attention_lstm.save('rnn_training_history/rnn_attention_lstm.h5')\n",
    "# rnn_attention_lstm.save_weights('rnn_training_history/rnn_attention_lstm_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedd (Embedding)      (None, 250, 100)     3247100     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 250, 100), ( 80400       encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 250, 100), ( 80400       encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedd (Embedding)      (None, 250, 100)     3247100     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 100)          0           lstm_1[0][1]                     \n",
      "                                                                 lstm_2[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 100)          0           lstm_1[0][2]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 250, 100), ( 80400       decoder_embedd[0][0]             \n",
      "                                                                 add_1[0][0]                      \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention (Dot)                 (None, 250, 250)     0           lstm_3[0][0]                     \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 250, 250)     0           attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "context (Dot)                   (None, 250, 100)     0           activation_1[0][0]               \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 250, 200)     0           context[0][0]                    \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 250, 32471)   6526671     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_activation (Activation) (None, 250, 32471)   0           time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 13,262,071\n",
      "Trainable params: 13,262,071\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#load attention-LSTM model\n",
    "rnn_attention_lstm = load_model('rnn_training_history/rnn_attention_lstm.h5')\n",
    "rnn_attention_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## V. Attention Model - Bi-LSTM\n",
    "\n",
    "Incorporated an attention/context mechanism in the bidirectional model. Attention mechanism combines Bidirectional-LSTM encoder outputs with the decoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "Gkx0XSiRx0XQ",
    "outputId": "a4c3bbbc-0d02-45a2-b6b6-6af17a45f58d"
   },
   "outputs": [],
   "source": [
    "# #train model with attention\n",
    "# rnn_attention = getAttentionModel(genTrain, genVal, embeddMatrix, \n",
    "#                                   learning_rate, clip_norm, nUnique,\n",
    "#                                   embeddDim, hidden_units)\n",
    "\n",
    "# rnn_attention.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # define the checkpoint\n",
    "# filepath = \"rnn_training_history/rnn_model_attention_2.h5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "# #fit model\n",
    "# rnn_attention.fit_generator(genTrain,\n",
    "#                             steps_per_epoch = num_train_batches,\n",
    "#                             epochs = 50, \n",
    "#                             validation_data = genVal,\n",
    "#                             validation_steps = num_val_batches,\n",
    "#                             callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save model and model weights\n",
    "# rnn_attention.save('rnn_training_history/rnn_attention_2.h5')\n",
    "# rnn_attention.save_weights('rnn_training_history/rnn_attention_2_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedd (Embedding)      (None, 250, 100)     3247100     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 250, 200), ( 160800      encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedd (Embedding)      (None, 250, 100)     3247100     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 250, 200), ( 160800      decoder_embedd[0][0]             \n",
      "                                                                 bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 250, 250)     0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 250, 250)     0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 250, 200)     0           activation_1[0][0]               \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 250, 400)     0           dot_2[0][0]                      \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 250, 32471)   13020871    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_activation (Activation) (None, 250, 32471)   0           time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 19,836,671\n",
      "Trainable params: 19,836,671\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "rnn_attention_bilstm = load_model('rnn_training_history/rnn_attention_2.h5')\n",
    "rnn_attention_bilstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## VI. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(model, seq, idx2word, tMaxLen, \n",
    "                  num_iteration, greedy = True, latitude = 5):\n",
    "    '''\n",
    "    Prediction for a given sequence. \n",
    "    \n",
    "    Parameters:\n",
    "    ===========\n",
    "    1)model: rnn model\n",
    "    2)seq: a single abstract, should be a vector of length 250\n",
    "    3)tMaxLen: maximum length of title, should match with training title input\n",
    "    4)num_iteration: maximum length allowed for title prediction\n",
    "    5)idx2word: dictionary for index to word\n",
    "    6)greedy: default to greedy search predictions, otherwise beam search\n",
    "    7)latitude: for greedy search, how many top words to consider for random choice\n",
    "    '''\n",
    "    \n",
    "    #cache list of prediction\n",
    "    prediction = list()\n",
    "    #initiate title to be a vector of zeros\n",
    "    init = np.zeros(maxLen)\n",
    "             \n",
    "    #for maximum prediction length\n",
    "    for i in range(num_iteration):\n",
    "        #get prediction probabilities for all unique words\n",
    "        predRNN = model.predict([np.reshape(seq, (1, 250)), init.reshape(1, 250)])\n",
    "        \n",
    "        if greedy:\n",
    "\n",
    "            #update next title vector to be the predicted vector\n",
    "            idx = np.argmax(predRNN[0, i])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #get top number of words\n",
    "            idxV = np.argsort(predRNN[0, i])[-latitude: ]\n",
    "            #randomly choose from the top words\n",
    "            idx = np.random.choice(idxV)\n",
    "            if i == 0:\n",
    "                while idx == 1:\n",
    "                    idx = np.random.choice(idxV)\n",
    "            else:\n",
    "                while idx == word2idx[prediction[i-1]]:\n",
    "                    idx = np.random.choice(idxV)\n",
    "        \n",
    "        #index to word\n",
    "        word = idx2word[str(idx)]\n",
    "        init[i] = idx\n",
    "        #if eos tag is predicted\n",
    "        #break out of loop\n",
    "        if idx == 1:\n",
    "            break\n",
    "        prediction.append(word)\n",
    "            \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load base model\n",
    "rnn_base = load_model('rnn_training_history/rnn_base.h5')\n",
    "\n",
    "#get predictions for attention model\n",
    "\n",
    "basePred = defaultdict(list)\n",
    "\n",
    "testX_padded = sequence.pad_sequences(testX, 250, value = word2idx['_'], padding = 'pre')\n",
    "\n",
    "for i in range(21):\n",
    "    \n",
    "    seq = testX_padded[i]\n",
    "    truth = [idx2word[str(x)]for x in testY[i]]\n",
    "    greedy_pred = getPrediction(rnn_base, seq, idx2word, 250, 20, greedy = True)\n",
    "    non_greedy_pred = getPrediction(rnn_base, seq, idx2word, 250, 20, greedy = False)\n",
    "    space = ' '\n",
    "    basePred['Truth'].append(space.join(truth))\n",
    "    basePred['Greedy'].append(space.join(greedy_pred))\n",
    "    basePred['Non-Greedy'].append(space.join(non_greedy_pred))\n",
    "    \n",
    "    #checkpoint\n",
    "    print(i)\n",
    "#     if i % 100 == 0:\n",
    "#         print(i // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#get predictions for bidirectional model\n",
    "\n",
    "biLSTMPred = defaultdict(list)\n",
    "\n",
    "testX_padded = sequence.pad_sequences(testX, 250, value = word2idx['_'], padding = 'pre')\n",
    "\n",
    "for i in range(21):\n",
    "    \n",
    "    seq = testX_padded[i]\n",
    "    truth = [idx2word[str(x)]for x in testY[i]]\n",
    "    greedy_pred = getPrediction(rnn_bidirectional, seq, idx2word, 250, 20, greedy = True)\n",
    "    non_greedy_pred = getPrediction(rnn_bidirectional, seq, idx2word, 250, 20, greedy = False)\n",
    "    space = ' '\n",
    "    biLSTMPred['Truth'].append(space.join(truth))\n",
    "    biLSTMPred['Greedy'].append(space.join(greedy_pred))\n",
    "    biLSTMPred['Non-Greedy'].append(space.join(non_greedy_pred))\n",
    "    \n",
    "    #checkpoint\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(biLSTMPred['Truth'])):\n",
    "    print('Truth: ', biLSTMPred['Truth'][i])\n",
    "    print('Greedy Search: ', biLSTMPred['Greedy'][i])\n",
    "    print('Non-Greedy Search: ', biLSTMPred['Non-Greedy'][i])\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions for attention lstm model\n",
    "\n",
    "attLSTMPred = defaultdict(list)\n",
    "\n",
    "testX_padded = sequence.pad_sequences(testX, 250, value = word2idx['_'], padding = 'pre')\n",
    "\n",
    "for i in range(21):\n",
    "    \n",
    "    seq = testX_padded[i]\n",
    "    truth = [idx2word[str(x)]for x in testY[i]]\n",
    "    greedy_pred = getPrediction(rnn_attention_lstm, seq, idx2word, 250, 20, greedy = True)\n",
    "    non_greedy_pred = getPrediction(rnn_attention_lstm, seq, idx2word, 250, 20, greedy = False)\n",
    "    space = ' '\n",
    "    attLSTMPred['Truth'].append(space.join(truth))\n",
    "    attLSTMPred['Greedy'].append(space.join(greedy_pred))\n",
    "    attLSTMPred['Non-Greedy'].append(space.join(non_greedy_pred))\n",
    "    \n",
    "    #checkpoint\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(attLSTMPred['Truth'])):\n",
    "    print('Truth: ', attLSTMPred['Truth'][i])\n",
    "    print('Greedy Search: ', attLSTMPred['Greedy'][i])\n",
    "    print('Non-Greedy Search: ', attLSTMPred['Non-Greedy'][i])\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## VII. Random Predictions\n",
    "\n",
    "Predictions for out-of-dataset text files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRandomText(txt, model, greedy = True):\n",
    "    \n",
    "    #format input text string \n",
    "    txtV = txt.lower().split(' ')\n",
    "    #cache list for tokenization\n",
    "    tokenized = list()\n",
    "    #tokenize text\n",
    "    for w in txtV:\n",
    "        try:\n",
    "            tokenized.append(word2idx[w])\n",
    "        except:\n",
    "            tokenized.append(word2idx['<ign>'])\n",
    "    #pad sequence for prediction\n",
    "    tokenized_padded = sequence.pad_sequences([tokenized], 250, value = word2idx['_'], padding = 'pre')\n",
    "    #return prediction given mode\n",
    "    if greedy:\n",
    "        pred = getPrediction(model, tokenized_padded[0], idx2word, 250, 20, greedy = True)\n",
    "    else:\n",
    "        pred = getPrediction(model, tokenized_padded[0], idx2word, 250, 20, greedy = False)\n",
    "    \n",
    "    return pred"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_model.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
