{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSbQIvnPx0W0"
   },
   "source": [
    "# RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sUjoOSJCx0W2",
    "outputId": "a98b5ac6-1673-422c-c7c4-dd91d6cf17cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import gensim as gs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.tokenize import sent_tokenize as st\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Bidirectional, Dense,LSTM,Input,Activation,Add,TimeDistributed,\\\n",
    "Permute,Flatten,RepeatVector,merge,Lambda,Multiply,Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vav79cEmx0W9"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "histPath = ''\n",
    "#load training data\n",
    "with open(histPath+'train.txt', \"rb\") as f1, open(histPath+'val.txt', \"rb\") as f2, open(histPath+'test.txt', \"rb\") as f3: \n",
    "    trainX, trainY = pickle.load(f1)\n",
    "    valX, valY = pickle.load(f2)\n",
    "    testX, testY = pickle.load(f3)\n",
    "#load dictionaries\n",
    "with open(histPath+'word2idx_master.json', 'r') as f1, open(histPath+'idx2word_master.json', 'r') as f2:\n",
    "    word2idx = json.load(f1)\n",
    "    idx2word = json.load(f2)\n",
    "\n",
    "#load embedding matrix\n",
    "embeddMatrix = np.load(histPath+'embeddMatrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADJqSYpIx0W_"
   },
   "outputs": [],
   "source": [
    "#params for model training\n",
    "seed = 209\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "LR = 1e-4\n",
    "batch_size = 8\n",
    "\n",
    "num_train_batches = len(trainX) // batch_size\n",
    "num_val_samples = len(valX) + len(trainX) - batch_size*num_train_batches\n",
    "num_val_batches = len(valX) // batch_size\n",
    "total_entries = (num_train_batches + num_val_batches)*batch_size\n",
    "\n",
    "#maximum length for title \n",
    "# tMaxLen = 20\n",
    "tMaxLen = 250\n",
    "#maximum length for abstract\n",
    "aMaxLen = 250\n",
    "#total maximum length\n",
    "maxlen = tMaxLen + aMaxLen\n",
    "\n",
    "batch_norm=False\n",
    "\n",
    "embeddDim = embeddMatrix.shape[1]\n",
    "nUnique = embeddMatrix.shape[0]\n",
    "hidden_units= embeddDim\n",
    "\n",
    "learning_rate = 0.002\n",
    "clip_norm = 1.0\n",
    "# regularizer = l2(weight_decay) if weight_decay else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjLBQTBJx0XC"
   },
   "source": [
    "---\n",
    "\n",
    "## I. Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wy8dEpxKx0XD"
   },
   "outputs": [],
   "source": [
    "#padding function for abstracts\n",
    "def padAbstract(x, maxL = aMaxLen, dictionary = word2idx):\n",
    "    n = len(x)\n",
    "    if n > maxL:\n",
    "        x = x[-maxL:]\n",
    "        n = maxL\n",
    "    return [dictionary['_']]*(maxL - n) + x + [dictionary['*']]\n",
    "\n",
    "#build generator for model\n",
    "def generator(trainX, trainY, batch_size = batch_size, \n",
    "              nb_batches = None, model = None, seed = seed):\n",
    "    \n",
    "    #UNDERSTAND THIS\n",
    "    c = nb_batches if nb_batches else 0\n",
    "    while True:\n",
    "        titles = list()\n",
    "        abstracts = list()\n",
    "        if nb_batches and c >= nb_batches:\n",
    "            c = 0\n",
    "        new_seed = random.randint(0, sys.maxsize)\n",
    "        random.seed(c+123456789+seed)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            a = random.randint(0,len(trainX)-1)\n",
    "            \n",
    "            #random shuffling of data\n",
    "            abstract = trainX[a]\n",
    "            s = random.randint(min(aMaxLen,len(abstract)), max(aMaxLen,len(abstract)))\n",
    "            abstracts.append(abstract[:s])\n",
    "            \n",
    "            title = trainY[a]\n",
    "            s = random.randint(min(tMaxLen,len(title)), max(tMaxLen,len(title)))\n",
    "            titles.append(title[:s])\n",
    "\n",
    "        # undo the seeding before we yield in order not to affect the caller\n",
    "        c+= 1\n",
    "        random.seed(new_seed)\n",
    "\n",
    "        yield conv_seq_labels(abstracts, titles)\n",
    "\n",
    "#pad sequence and convert title to labels\n",
    "def conv_seq_labels(abstracts, titles, nflips = None, model = None, dictionary = word2idx):\n",
    "    \"\"\"abstract and titles are converted to padded input vectors. Titles are one-hot encoded to labels.\"\"\"\n",
    "    batch_size = len(titles)\n",
    "    \n",
    "    \n",
    "    x = [padAbstract(a)+t for a,t in zip(abstracts, titles)] \n",
    "    x = sequence.pad_sequences(x, maxlen = maxlen, value = dictionary['_'], \n",
    "                               padding = 'post', truncating = 'post')\n",
    "        \n",
    "    y = np.zeros((batch_size, tMaxLen, nUnique))\n",
    "    for i, it in enumerate(titles):\n",
    "        it = it + [dictionary['*']] + [dictionary['_']]*tMaxLen  # output does have a eos at end\n",
    "        it = it[:tMaxLen]\n",
    "        y[i,:,:] = np_utils.to_categorical(it, nUnique)\n",
    "        \n",
    "    #The 3 inputs are abstract, title starting with eos and a one-hot encoding of the title categorical variables.\n",
    "    return [x[:,:aMaxLen],x[:,aMaxLen:]], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "Qu8vqCWfx0XF",
    "outputId": "ea483079-9a74-4c2f-ae97-3d68efcfb768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 250) (8, 250) (8, 250, 32471)\n",
      "Abstract  :  ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'with', 'the', 'increase', 'in', 'available', 'data', 'parallel', 'machine', 'learning', 'has', '<ign>', '<ign>', 'become', 'an', 'increasingly', 'pressing', 'problem.', 'in', 'this', 'paper', 'we', 'present', '<ign>', '<ign>', 'the', 'first', 'parallel', 'stochastic', 'gradient', 'descent', 'algorithm', 'including', 'a', '<ign>', '<ign>', 'detailed', 'analysis', 'and', 'experimental', 'evidence.', 'unlike', 'prior', 'work', 'on', '<ign>', '<ign>', 'parallel', 'optimization', 'algorithms', 'our', '<ign>', '<ign>', 'variant', 'comes', 'with', 'parallel', 'acceleration', 'guarantees', 'and', 'it', 'poses', 'no', '<ign>', '<ign>', 'overly', 'tight', 'latency', 'constraints,', 'which', 'might', 'only', 'be', 'available', 'in', '<ign>', '<ign>', 'the', 'multicore', 'setting.', 'our', 'analysis', 'introduces', 'a', 'novel', 'proof', '<ign>', '<ign>', 'technique', '<ign>', 'contractive', 'mappings', 'to', 'quantify', 'the', '<ign>', '<ign>', 'speed', 'of', 'convergence', 'of', 'parameter', 'distributions', 'to', 'their', 'asymptotic', '<ign>', '<ign>', 'limits.', 'as', 'a', 'side', 'effect', 'this', 'answers', 'the', 'question', 'of', 'how', 'quickly', '<ign>', '<ign>', 'stochastic', 'gradient', 'descent', 'algorithms', 'reach', 'the', 'asymptotically', '<ign>', '<ign>', 'normal', 'regime.']\n",
      "Title  :  ['*', 'parallelized', 'stochastic', 'gradient', 'descent', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "#check generator\n",
    "check = next(generator(trainX, trainY, batch_size = batch_size))\n",
    "print(check[0][0].shape,check[0][1].shape,check[1].shape)\n",
    "print(\"Abstract  : \", [idx2word[str(i)] for i in check[0][0][1]])\n",
    "print(\"Title  : \", [idx2word[str(i)] for i in check[0][1][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsDy6pxNx0XJ"
   },
   "outputs": [],
   "source": [
    "#generator for training and validation\n",
    "genTrain = generator(trainX, trainY, batch_size = batch_size)\n",
    "genVal =  generator(valX, valY, nb_batches = len(valX)// batch_size, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-9jfFGRx0XM"
   },
   "source": [
    "---\n",
    "\n",
    "## II. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FETS5-o0w4O"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0SuCA_hx0XN"
   },
   "outputs": [],
   "source": [
    "#encoder\n",
    "def getModel(num_epochs, genTrain, genVal, embeddMatrix, learning_rate, clip_norm,\n",
    "             encoder_shape = aMaxLen, decoder_shape = tMaxLen, \n",
    "             nUnique = nUnique, embeddDim = embeddDim, hidden_units = hidden_units):\n",
    "    \n",
    "    #ENCODER\n",
    "    #input shape as the vector of sequence, with length padded to 250\n",
    "    encoder_inputs = Input(shape = (encoder_shape, ), name = 'encoder_input')\n",
    "    \n",
    "    encoder_embedding = Embedding(nUnique, embeddDim, \n",
    "                                  input_length = encoder_shape, \n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = True,\n",
    "                                  name = 'encoder_embedd')(encoder_inputs)\n",
    "    \n",
    "    encoder_lstm = Bidirectional(LSTM(hidden_units, dropout_U = 0.2,\n",
    "                                      dropout_W = 0.2 , return_state=True))\n",
    "    \n",
    "    encoder_outputs, f_h, f_c, b_h, b_c = encoder_lstm(encoder_embedding)\n",
    "    \n",
    "    state_hfinal=Add()([f_h, b_h])\n",
    "    state_cfinal=Add()([f_c, b_c])\n",
    "    \n",
    "    encoder_states = [state_hfinal,state_cfinal]\n",
    "        \n",
    "    #DECODER\n",
    "    decoder_inputs = Input(shape = (decoder_shape, ), name = 'decoder_input')\n",
    "    \n",
    "    decoder_embedding = Embedding(nUnique, embeddDim, \n",
    "                                  input_length = decoder_shape, \n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = True,\n",
    "                                  name = 'decoder_embedd')\n",
    "    \n",
    "    decoder_lstm = LSTM(hidden_units,return_sequences = True, return_state=True)\n",
    "    \n",
    "    decoder_outputs, s_h, s_c = decoder_lstm(decoder_embedding(decoder_inputs), initial_state = encoder_states)    \n",
    "    decoder_dense = Dense(decoder_shape, activation='linear')\n",
    "    decoder_time_distributed = TimeDistributed(Dense(nUnique,\n",
    "                                                     name = 'decoder_timedistributed'))\n",
    "    decoder_activation = Activation('softmax', name = 'decoder_activation')\n",
    "    decoder_outputs = decoder_activation(decoder_time_distributed(decoder_outputs))\n",
    "    \n",
    "    #MODEL\n",
    "    model = Model(inputs = [encoder_inputs,decoder_inputs], outputs = decoder_outputs) \n",
    "    rmsprop = RMSprop(lr = learning_rate, clipnorm = clip_norm)\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer = rmsprop)\n",
    "    \n",
    "    #FIT MODEL\n",
    "    model.fit_generator(genTrain,\n",
    "                        steps_per_epoch = num_train_batches,\n",
    "                        epochs=num_epochs, \n",
    "                        validation_data = genVal,\n",
    "                        validation_steps = num_val_batches)\n",
    "    \n",
    "    #ENCODER MODEL\n",
    "    encoder_model = Model(encoder_inputs,encoder_states)\n",
    "    \n",
    "    #DECODER MODEL\n",
    "    decoder_state_inputs_h = Input(shape=(hidden_units,))\n",
    "    decoder_state_inputs_c = Input(shape=(hidden_units,)) \n",
    "    decoder_state_inputs = [decoder_state_inputs_h, decoder_state_inputs_c]\n",
    "    decoder_outputs, decoder_state_h, decoder_state_c = decoder_LSTM(decoder_embedding(decoder_inputs),\n",
    "                                                                     initial_state = decoder_state_inputs)\n",
    "    decoder_states = [decoder_state_h, decoder_state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model = Model([decoder_inputs] + decoder_state_inputs,\n",
    "                          [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "Gkx0XSiRx0XQ",
    "outputId": "a4c3bbbc-0d02-45a2-b6b6-6af17a45f58d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "rnn = getModel(5, genTrain, genVal, embeddMatrix, learning_rate, clip_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nfd-8N8x0XV",
    "outputId": "a8097d79-159a-4dd0-a053-2785c60759ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedd (Embedding)      (None, 250, 100)     3247100     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) [(None, 200), (None, 160800      encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedd (Embedding)      (None, 250, 100)     3247100     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 100)          0           bidirectional_3[0][1]            \n",
      "                                                                 bidirectional_3[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 100)          0           bidirectional_3[0][2]            \n",
      "                                                                 bidirectional_3[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 250, 100), ( 80400       decoder_embedd[0][0]             \n",
      "                                                                 add_5[0][0]                      \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 250, 32471)   3279571     lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_activation (Activation) (None, 250, 32471)   0           time_distributed_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 10,014,971\n",
      "Trainable params: 10,014,971\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LV9K7V6Hx0XZ",
    "outputId": "16a251f1-3f8e-4337-ec57-d64c9c679299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "   4/1669 [..............................] - ETA: 1:33:19 - loss: 10.3254"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-78b0358521af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m rnn.fit_generator(genTrain, steps_per_epoch = num_train_batches, epochs=1, \n\u001b[0;32m----> 2\u001b[0;31m                   validation_data = genVal, validation_steps = num_val_batches)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/109b/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/109b/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/109b/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/109b/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/109b/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/109b/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/109b/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn.fit_generator(genTrain, steps_per_epoch = num_train_batches, epochs=1, \n",
    "                  validation_data = genVal, validation_steps = num_val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrsGXLs3x0Xc"
   },
   "outputs": [],
   "source": [
    "# #single layger LSTM \n",
    "# def encoder_decoder(genTrain, genVal, mode = 'fit', num_epochs = 1, \n",
    "#                     en_shape = aMaxLen, de_shape = tMaxLen):\n",
    "    \n",
    "# #     print('Encoder_Decoder LSTM...')\n",
    "   \n",
    "# #     \"\"\"__encoder___\"\"\"\n",
    "# #     encoder_inputs = Input(shape=(en_shape,), name='inputE')\n",
    "# #     print(encoder_inputs)\n",
    "    \n",
    "# #     #APPLY EMBEDDING LAYER. https://keras.io/layers/embeddings/       \n",
    "# #     input_emb = Embedding(nUnique, embeddDim,\n",
    "# #                           input_length = aMaxLen,\n",
    "# #                           W_regularizer = regularizer, dropout = p_emb, \n",
    "# #                           weights=[embeddMatrix], mask_zero = True,\n",
    "# #                           name='embedding_1')\n",
    "    \n",
    "# #     #ENCODER LSTM - FORWARD   https://keras.io/layers/recurrent/  \n",
    "# #     encoder_LSTM = LSTM(hidden_units, dropout_U = 0.2, dropout_W = 0.2 ,return_state=True)\n",
    "# #     encoder_LSTM_rev = LSTM(hidden_units,return_state=True,go_backwards=True)\n",
    "    \n",
    "# #     #ENCODER LSTM - REVERSE \n",
    "# #     encoder_outputsR, state_hR, state_cR = encoder_LSTM_rev(input_emb(encoder_inputs))\n",
    "# #     encoder_outputs, state_h, state_c = encoder_LSTM(input_emb(encoder_inputs))\n",
    "        \n",
    "# #     state_hfinal=Add()([state_h,state_hR])\n",
    "# #     state_cfinal=Add()([state_c,state_cR])\n",
    "    \n",
    "# #     encoder_states = [state_hfinal,state_cfinal]\n",
    "    \n",
    "#     \"\"\"____decoder___\"\"\"\n",
    "#     #Input to the decoder would be the summary(headline) sequence starting from ~ character.\n",
    "#     decoder_inputs = Input(shape=(de_shape,), name = 'inputD')\n",
    "# #     decoder_inputs = Input(shape=(en_shape,))\n",
    "#     print(decoder_inputs)\n",
    "      \n",
    "#     decoder_LSTM = LSTM(hidden_units,return_sequences=True,return_state=True)\n",
    "#     decoder_outputs, _, _ = decoder_LSTM(input_emb(decoder_inputs),initial_state=encoder_states) \n",
    "# #     decoder_dense = Dense(de_shape,activation='linear')\n",
    "    \n",
    "#     # Apply a dense layer that has vocab_size(40000) outputs which learns probability of each word when softmax is applied.\n",
    "#     # TimeDistributed is a wrapper for applying the same function over all the time step outputs. \n",
    "#     # Refer https://keras.io/layers/wrappers/\n",
    "#     decoder_time_distributed = TimeDistributed(Dense(nUnique,\n",
    "#                                                      W_regularizer=regularizer, \n",
    "#                                                      b_regularizer=regularizer,\n",
    "#                                                      name = 'decoder_timedistributed'))\n",
    "#     activation = Activation('softmax', name = 'activation_1')\n",
    "#     decoder_outputs = activation(time_distributed(decoder_outputs))\n",
    "    \n",
    "#     #Model groups layers into an object with training and inference features.\n",
    "#     #https://www.tensorflow.org/api_docs/python/tf/keras/models/Model        \n",
    "#     model= Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)\n",
    "    \n",
    "#     rmsprop = RMSprop(lr = learning_rate,clipnorm = clip_norm)\n",
    "    \n",
    "#     model.compile(loss='categorical_crossentropy',optimizer=rmsprop)\n",
    "    \n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.fit_generator(genTrain,\n",
    "#                             steps_per_epoch = num_train_batches,\n",
    "#                             epochs=5,  #Try different epochs as hyperparameter \n",
    "#                             validation_data = genVal,\n",
    "#                             validation_steps = num_val_batches)\n",
    "    \n",
    "#     #_________________________INFERENCE MODE______________________________#  \n",
    "    \n",
    "#     encoder_model_inf = Model(encoder_inputs,encoder_states)\n",
    "    \n",
    "#     decoder_state_input_H = Input(shape=(hidden_units,))\n",
    "#     decoder_state_input_C = Input(shape=(hidden_units,)) \n",
    "#     decoder_state_inputs = [decoder_state_input_H, decoder_state_input_C]\n",
    "#     decoder_outputs, decoder_state_h, decoder_state_c = decoder_LSTM(input_emb(decoder_inputs),\n",
    "#                                                                      initial_state=decoder_state_inputs)\n",
    "#     decoder_states = [decoder_state_h, decoder_state_c]\n",
    "#     decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "#     decoder_model_inf= Model([decoder_inputs]+decoder_state_inputs,\n",
    "#                              [decoder_outputs]+decoder_states)\n",
    "    \n",
    "#     return model,encoder_model_inf,decoder_model_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_xodO1dx0Xe"
   },
   "outputs": [],
   "source": [
    "#let's try this\n",
    "# model = encoder_decoder(genTrain, genVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZuKtx7wKx0Xg"
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxqF7dqZx0Xi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_model.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
