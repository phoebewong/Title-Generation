{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSbQIvnPx0W0"
   },
   "source": [
    "# RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sUjoOSJCx0W2",
    "outputId": "a98b5ac6-1673-422c-c7c4-dd91d6cf17cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import gensim as gs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.tokenize import sent_tokenize as st\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Bidirectional, Dropout, Dense,LSTM,Input,Activation,Add,TimeDistributed,\\\n",
    "Permute,Flatten,RepeatVector,merge,Lambda,Multiply,Reshape, Concatenate\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "#models \n",
    "from rnn_model import getBaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vav79cEmx0W9"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "histPath = ''\n",
    "#load training data\n",
    "with open(histPath+'train.txt', \"rb\") as f1, open(histPath+'val.txt', \"rb\") as f2, open(histPath+'test.txt', \"rb\") as f3: \n",
    "    trainX, trainY = pickle.load(f1)\n",
    "    valX, valY = pickle.load(f2)\n",
    "    testX, testY = pickle.load(f3)\n",
    "#load dictionaries\n",
    "with open(histPath+'word2idx_master.json', 'r') as f1, open(histPath+'idx2word_master.json', 'r') as f2:\n",
    "    word2idx = json.load(f1)\n",
    "    idx2word = json.load(f2)\n",
    "\n",
    "#load embedding matrix\n",
    "embeddMatrix = np.load(histPath+'embeddMatrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADJqSYpIx0W_"
   },
   "outputs": [],
   "source": [
    "#params for model training\n",
    "seed = 209\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "LR = 1e-4\n",
    "batch_size = 32\n",
    "\n",
    "num_train_batches = len(trainX) // batch_size\n",
    "num_val_samples = len(valX) + len(trainX) - batch_size*num_train_batches\n",
    "num_val_batches = len(valX) // batch_size\n",
    "total_entries = (num_train_batches + num_val_batches)*batch_size\n",
    "\n",
    "#maximum length for title \n",
    "tMaxLen = 250\n",
    "#maximum length for abstract\n",
    "aMaxLen = 250\n",
    "#total maximum length\n",
    "maxlen = tMaxLen + aMaxLen\n",
    "\n",
    "batch_norm=False\n",
    "\n",
    "embeddDim = embeddMatrix.shape[1]\n",
    "nUnique = embeddMatrix.shape[0]\n",
    "hidden_units= embeddDim\n",
    "\n",
    "learning_rate = 0.002\n",
    "clip_norm = 1.0\n",
    "# regularizer = l2(weight_decay) if weight_decay else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjLBQTBJx0XC"
   },
   "source": [
    "---\n",
    "\n",
    "## I. Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wy8dEpxKx0XD"
   },
   "outputs": [],
   "source": [
    "#padding function for abstracts\n",
    "def padAbstract(x, maxL = aMaxLen, dictionary = word2idx):\n",
    "    n = len(x)\n",
    "    if n > maxL:\n",
    "        x = x[-maxL:]\n",
    "        n = maxL\n",
    "    return [dictionary['_']]*(maxL - n) + x + [dictionary['*']]\n",
    "\n",
    "#build generator for model\n",
    "def generator(trainX, trainY, batch_size = batch_size, \n",
    "              nb_batches = None, model = None, seed = seed):\n",
    "    \n",
    "    #UNDERSTAND THIS\n",
    "    c = nb_batches if nb_batches else 0\n",
    "    while True:\n",
    "        titles = list()\n",
    "        abstracts = list()\n",
    "        if nb_batches and c >= nb_batches:\n",
    "            c = 0\n",
    "        new_seed = random.randint(0, sys.maxsize)\n",
    "        random.seed(c+123456789+seed)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            a = random.randint(0,len(trainX)-1)\n",
    "            \n",
    "            #random shuffling of data\n",
    "            abstract = trainX[a]\n",
    "            s = random.randint(min(aMaxLen,len(abstract)), max(aMaxLen,len(abstract)))\n",
    "            abstracts.append(abstract[:s])\n",
    "            \n",
    "            title = trainY[a]\n",
    "            s = random.randint(min(tMaxLen,len(title)), max(tMaxLen,len(title)))\n",
    "            titles.append(title[:s])\n",
    "\n",
    "        # undo the seeding before we yield in order not to affect the caller\n",
    "        c+= 1\n",
    "        random.seed(new_seed)\n",
    "\n",
    "        yield conv_seq_labels(abstracts, titles)\n",
    "\n",
    "#pad sequence and convert title to labels\n",
    "def conv_seq_labels(abstracts, titles, nflips = None, model = None, dictionary = word2idx):\n",
    "    \"\"\"abstract and titles are converted to padded input vectors. Titles are one-hot encoded to labels.\"\"\"\n",
    "    batch_size = len(titles)\n",
    "    \n",
    "    \n",
    "    x = [padAbstract(a)+t for a,t in zip(abstracts, titles)] \n",
    "    x = sequence.pad_sequences(x, maxlen = maxlen, value = dictionary['_'], \n",
    "                               padding = 'post', truncating = 'post')\n",
    "        \n",
    "    y = np.zeros((batch_size, tMaxLen, nUnique))\n",
    "    for i, it in enumerate(titles):\n",
    "        it = it + [dictionary['*']] + [dictionary['_']]*tMaxLen  # output does have a eos at end\n",
    "        it = it[:tMaxLen]\n",
    "        y[i,:,:] = np_utils.to_categorical(it, nUnique)\n",
    "        \n",
    "    #The 3 inputs are abstract, title starting with eos and a one-hot encoding of the title categorical variables.\n",
    "    return [x[:,:aMaxLen],x[:,aMaxLen:]], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "Qu8vqCWfx0XF",
    "outputId": "ea483079-9a74-4c2f-ae97-3d68efcfb768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 250) (32, 250) (32, 250, 32471)\n",
      "Abstract  :  ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'with', 'the', 'increase', 'in', 'available', 'data', 'parallel', 'machine', 'learning', 'has', '<ign>', '<ign>', 'become', 'an', 'increasingly', 'pressing', 'problem.', 'in', 'this', 'paper', 'we', 'present', '<ign>', '<ign>', 'the', 'first', 'parallel', 'stochastic', 'gradient', 'descent', 'algorithm', 'including', 'a', '<ign>', '<ign>', 'detailed', 'analysis', 'and', 'experimental', 'evidence.', 'unlike', 'prior', 'work', 'on', '<ign>', '<ign>', 'parallel', 'optimization', 'algorithms', 'our', '<ign>', '<ign>', 'variant', 'comes', 'with', 'parallel', 'acceleration', 'guarantees', 'and', 'it', 'poses', 'no', '<ign>', '<ign>', 'overly', 'tight', 'latency', 'constraints,', 'which', 'might', 'only', 'be', 'available', 'in', '<ign>', '<ign>', 'the', 'multicore', 'setting.', 'our', 'analysis', 'introduces', 'a', 'novel', 'proof', '<ign>', '<ign>', 'technique', '<ign>', 'contractive', 'mappings', 'to', 'quantify', 'the', '<ign>', '<ign>', 'speed', 'of', 'convergence', 'of', 'parameter', 'distributions', 'to', 'their', 'asymptotic', '<ign>', '<ign>', 'limits.', 'as', 'a', 'side', 'effect', 'this', 'answers', 'the', 'question', 'of', 'how', 'quickly', '<ign>', '<ign>', 'stochastic', 'gradient', 'descent', 'algorithms', 'reach', 'the', 'asymptotically', '<ign>', '<ign>', 'normal', 'regime.']\n",
      "Title  :  ['*', 'parallelized', 'stochastic', 'gradient', 'descent', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "#check generator\n",
    "check = next(generator(trainX, trainY, batch_size = batch_size))\n",
    "print(check[0][0].shape,check[0][1].shape,check[1].shape)\n",
    "print(\"Abstract  : \", [idx2word[str(i)] for i in check[0][0][1]])\n",
    "print(\"Title  : \", [idx2word[str(i)] for i in check[0][1][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsDy6pxNx0XJ"
   },
   "outputs": [],
   "source": [
    "#generator for training and validation\n",
    "genTrain = generator(trainX, trainY, batch_size = batch_size)\n",
    "genVal =  generator(valX, valY, nb_batches = len(valX)// batch_size, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-9jfFGRx0XM"
   },
   "source": [
    "---\n",
    "\n",
    "## II. Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FETS5-o0w4O"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedd (Embedding)      (None, 250, 100)     3247100     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) [(None, 200), (None, 160800      encoder_embedd[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedd (Embedding)      (None, 250, 100)     3247100     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 100)          0           bidirectional_3[0][1]            \n",
      "                                                                 bidirectional_3[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 100)          0           bidirectional_3[0][2]            \n",
      "                                                                 bidirectional_3[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 250, 100), ( 80400       decoder_embedd[0][0]             \n",
      "                                                                 add_5[0][0]                      \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 250, 32471)   3279571     lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_activation (Activation) (None, 250, 32471)   0           time_distributed_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 10,014,971\n",
      "Trainable params: 10,014,971\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#base model\n",
    "rnn_base = getBaseModel(genTrain, genVal, embeddMatrix, \n",
    "                        learning_rate, clip_norm, nUnique,\n",
    "                        embeddDim, hidden_units)\n",
    "#base model summary\n",
    "rnn_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "104/104 [==============================] - 368s 4s/step - loss: 6.2035 - val_loss: 6.2983\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.29829, saving model to rnn_training_history/rnn_base.h5\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 361s 3s/step - loss: 5.8782 - val_loss: 6.1925\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.29829 to 6.19251, saving model to rnn_training_history/rnn_base.h5\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 362s 3s/step - loss: 5.6691 - val_loss: 6.0852\n",
      "\n",
      "Epoch 00003: val_loss improved from 6.19251 to 6.08518, saving model to rnn_training_history/rnn_base.h5\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 365s 4s/step - loss: 5.4394 - val_loss: 5.9791\n",
      "\n",
      "Epoch 00004: val_loss improved from 6.08518 to 5.97910, saving model to rnn_training_history/rnn_base.h5\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 363s 3s/step - loss: 5.2423 - val_loss: 5.9233\n",
      "\n",
      "Epoch 00005: val_loss improved from 5.97910 to 5.92332, saving model to rnn_training_history/rnn_base.h5\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 363s 3s/step - loss: 5.0506 - val_loss: 5.9064\n",
      "\n",
      "Epoch 00006: val_loss improved from 5.92332 to 5.90641, saving model to rnn_training_history/rnn_base.h5\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 363s 3s/step - loss: 4.9232 - val_loss: 5.8716\n",
      "\n",
      "Epoch 00007: val_loss improved from 5.90641 to 5.87158, saving model to rnn_training_history/rnn_base.h5\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 364s 4s/step - loss: 4.7476 - val_loss: 5.8607\n",
      "\n",
      "Epoch 00008: val_loss improved from 5.87158 to 5.86071, saving model to rnn_training_history/rnn_base.h5\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 368s 4s/step - loss: 4.5591 - val_loss: 5.8524\n",
      "\n",
      "Epoch 00009: val_loss improved from 5.86071 to 5.85241, saving model to rnn_training_history/rnn_base.h5\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 363s 3s/step - loss: 4.3843 - val_loss: 5.8855\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 5.85241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3f1158438>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train base model\n",
    "filepath = 'rnn_training_history/'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath + 'rnn_base.h5', monitor = 'val_loss', \n",
    "                             verbose = 1, save_best_only = True, mode = 'min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "#fit base model\n",
    "rnn_base.fit_generator(genTrain,\n",
    "                       steps_per_epoch = num_train_batches,\n",
    "                       epochs = 10, \n",
    "                       validation_data = genVal,\n",
    "                       validation_steps = num_val_batches,\n",
    "                       callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model weights\n",
    "rnn_base.save_weights(filepath+'rnn_base_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(model, seq, maxLen, num_iteration, idx2word):\n",
    "    '''\n",
    "    Prediction for a given sequence. \n",
    "    \n",
    "    Parameters:\n",
    "    ===========\n",
    "    1)model: rnn model\n",
    "    2)seq: a single abstract, should be a vector of length 250\n",
    "    3)maxLen: maximum length of predicted title\n",
    "    4)idx2word: dictionary for index to word\n",
    "    '''\n",
    "    \n",
    "    #cache list of prediction\n",
    "    prediction = list()\n",
    "    #initiate title to be a vector of zeros\n",
    "    init = np.zeros(maxLen)\n",
    "    \n",
    "    #for maximum prediction length\n",
    "    for i in range(num_iteration):\n",
    "        #get prediction probabilities for all unique words\n",
    "        predRNN = model.predict([np.reshape(seq, (1, 250)), init.reshape(1, 250)])\n",
    "        #greedy mode prediction\n",
    "        #update next title vector to be the predicted vector\n",
    "        init = np.argmax(predRNN, axis = 2)\n",
    "        #get probabilities of all unique words\n",
    "        pVec = predRNN[0, 0, :]\n",
    "        #get the word with maximum predicted probability as the predicted words\n",
    "        idx = np.argmax(pVec)\n",
    "        #index to word\n",
    "        word = idx2word[str(idx)]\n",
    "        #if eos tag is predicted\n",
    "        #break out of loop\n",
    "        if idx == 1:\n",
    "            break\n",
    "        prediction.append(word)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(model, seq, idx2word, maxLen, \n",
    "                  num_iteration, greedy = True, latitude = 5):\n",
    "    '''\n",
    "    Prediction for a given sequence. \n",
    "    \n",
    "    Parameters:\n",
    "    ===========\n",
    "    1)model: rnn model\n",
    "    2)seq: a single abstract, should be a vector of length 250\n",
    "    3)maxLen: maximum length of predicted title\n",
    "    4)idx2word: dictionary for index to word\n",
    "    5)greedy: default to greedy search predictions, otherwise beam search\n",
    "    6)latitude: for greedy search, how many top words to consider for random choice\n",
    "    '''\n",
    "    \n",
    "    #cache list of prediction\n",
    "    prediction = list()\n",
    "    #initiate title to be a vector of zeros\n",
    "    init = np.zeros(maxLen)\n",
    "             \n",
    "    #for maximum prediction length\n",
    "    for i in range(num_iteration):\n",
    "        #get prediction probabilities for all unique words\n",
    "        predRNN = model.predict([np.reshape(seq, (1, 250)), init.reshape(1, 250)])\n",
    "        \n",
    "        if greedy:\n",
    "\n",
    "            #update next title vector to be the predicted vector\n",
    "            idx = np.argmax(predRNN[0, i])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #get top number of words\n",
    "            idxV = np.argsort(predRNN[0, i])[-latitude: ]\n",
    "            #randomly choose from the top words\n",
    "            idx = np.random.choice(idxV)\n",
    "            if i == 0:\n",
    "                while idx == 1:\n",
    "                    idx = np.random.choice(idxV)\n",
    "        \n",
    "        #index to word\n",
    "        word = idx2word[str(idx)]\n",
    "        init[i] = idx\n",
    "        #if eos tag is predicted\n",
    "        #break out of loop\n",
    "        if idx == 1:\n",
    "            break\n",
    "        prediction.append(word)\n",
    "            \n",
    "    return prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title:  ['a', 'concave', 'regularization', 'technique', 'for', 'sparse', 'mixture', 'models']\n",
      "\n",
      "True Abstract: \n",
      "['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'latent', 'variable', 'mixture', 'models', 'are', 'a', 'powerful', 'tool', 'for', 'exploring', 'the', 'structure', 'in', 'large', 'datasets.', 'a', 'common', 'challenge', 'for', 'interpreting', 'such', 'models', 'is', 'a', 'desire', 'to', 'impose', 'sparsity,', 'the', 'natural', 'assumption', 'that', 'each', 'data', 'point', 'only', 'contains', 'few', 'latent', 'features.', 'since', 'mixture', 'distributions', 'are', 'constrained', 'in', 'their', 'l1', 'norm,', 'typical', 'sparsity', 'techniques', 'based', 'on', 'l1', 'regularization', 'become', 'toothless,', 'and', 'concave', 'regularization', 'becomes', 'necessary.', 'unfortunately', 'concave', 'regularization', 'typically', 'results', 'in', 'em', 'algorithms', 'that', 'must', 'perform', 'problematic', 'non-concave', 'm-step', 'maximizations.', 'in', 'this', 'work,', 'we', 'introduce', 'a', 'technique', 'for', 'circumventing', 'this', 'difficulty,', 'using', 'the', 'so-called', 'mountain', 'pass', 'theorem', 'to', 'provide', 'easily', 'verifiable', 'conditions', 'under', 'which', 'the', 'm-step', 'is', 'well-behaved', 'despite', 'the', 'lacking', 'concavity.', 'we', 'also', 'develop', 'a', 'correspondence', 'between', 'logarithmic', 'regularization', 'and', 'what', 'we', 'term', 'the', 'pseudo-dirichlet', 'distribution,', 'a', 'generalization', 'of', 'the', 'ordinary', 'dirichlet', 'distribution', 'well-suited', 'for', 'inducing', 'sparsity.', 'we', 'demonstrate', 'our', 'approach', 'on', 'a', 'text', 'corpus,', 'inferring', 'a', 'sparse', 'topic', 'mixture', 'model', 'for', '2,406', 'weblogs.']\n"
     ]
    }
   ],
   "source": [
    "#check prediction \n",
    "check = testX.copy()\n",
    "check = sequence.pad_sequences(check, 250, value = word2idx['_'], \n",
    "                               padding = 'pre')\n",
    "\n",
    "#example of seq2seq prediction\n",
    "#true title\n",
    "print('True Title: ', [idx2word[str(m)] for m in testY[0]])\n",
    "\n",
    "#true abstract\n",
    "print()\n",
    "print('True Abstract: ')\n",
    "print([idx2word[str(m)] for m in check[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning', 'with', 'gaussian', 'process', 'regression']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#greedy prediction\n",
    "check_pred = getPrediction(rnn_base, check[0], idx2word, 250, 20, greedy = True)\n",
    "check_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'bayesian',\n",
       " 'model',\n",
       " 'of',\n",
       " 'latent',\n",
       " 'markov',\n",
       " 'models',\n",
       " 'and',\n",
       " 'latent',\n",
       " 'dirichlet',\n",
       " 'variable',\n",
       " 'processes',\n",
       " 'models',\n",
       " 'and',\n",
       " 'applications',\n",
       " 'to',\n",
       " 'applications',\n",
       " 'on',\n",
       " 'applications',\n",
       " 'to']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#beam search prediction\n",
    "check_pred = getPrediction(rnn_base, check[0], idx2word, 250, 20, greedy = False)\n",
    "check_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttentionModel(genTrain, genVal, embeddMatrix,\n",
    "                      learning_rate, clip_norm, nUnique,\n",
    "                      embeddDim, hidden_units, encoder_shape = aMaxLen,\n",
    "                      decoder_shape = tMaxLen):\n",
    "\n",
    "    '''Base Model - Code Adopted from Computefest'''\n",
    "\n",
    "    #ENCODER\n",
    "    #input shape as the vector of sequence, with length padded to 250\n",
    "    encoder_inputs = Input(shape = (encoder_shape, ), name = 'encoder_input')\n",
    "\n",
    "    #encode input with embedding layer\n",
    "    #do not mask 0s because the attention layer does not allow this\n",
    "    encoder_embedding = Embedding(nUnique, embeddDim,\n",
    "                                  input_length = encoder_shape,\n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = False,\n",
    "                                  name = 'encoder_embedd')(encoder_inputs)\n",
    "\n",
    "    #1-layer bidirectional LSTM\n",
    "    encoder_lstm = Bidirectional(LSTM(hidden_units, dropout_U = 0.2, dropout_W = 0.2, \n",
    "                                      return_sequences = True, return_state=True))\n",
    "\n",
    "    #get states from Bi-LSTM\n",
    "    encoder_outputs, f_h, f_c, b_h, b_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "    #add final states together\n",
    "    state_hfinal=Add()([f_h, b_h])\n",
    "    state_cfinal=Add()([f_c, b_c])\n",
    "\n",
    "    #save encoder states\n",
    "    encoder_states = [state_hfinal,state_cfinal]\n",
    "\n",
    "    #DECODER\n",
    "    decoder_inputs = Input(shape = (decoder_shape, ), name = 'decoder_input')\n",
    "\n",
    "    #encode decoder input with embedding matrix\n",
    "    decoder_embedding = Embedding(nUnique, embeddDim,\n",
    "                                  input_length = decoder_shape,\n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = False,\n",
    "                                  name = 'decoder_embedd')\n",
    "\n",
    "    #1-layer lstm\n",
    "    decoder_lstm = LSTM(hidden_units,return_sequences = True, return_state=True)\n",
    "\n",
    "    #save decoder outputs\n",
    "    decoder_outputs, s_h, s_c = decoder_lstm(decoder_embedding(decoder_inputs), \n",
    "                                             initial_state = encoder_states)\n",
    "  \n",
    "    #ATTENTION\n",
    "    attention = TimeDistributed(Dense(1, activation = 'tanh'))(encoder_outputs)\n",
    "    attention = Multiply()([attention, decoder_outputs])\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "\n",
    "    #time distributed layer, probability predictions for all unique words\n",
    "    decoder_time_distributed = TimeDistributed(Dense(nUnique,\n",
    "                                                     name = 'decoder_timedistributed'))\n",
    "    decoder_activation = Activation('softmax', name = 'decoder_activation')\n",
    "    decoder_outputs = decoder_activation(decoder_time_distributed(decoder_outputs))\n",
    "\n",
    "    #MODEL\n",
    "    model = Model(inputs = [encoder_inputs,decoder_inputs], outputs = decoder_outputs)\n",
    "    rmsprop = RMSprop(lr = learning_rate, clipnorm = clip_norm)\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer = rmsprop)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0SuCA_hx0XN"
   },
   "outputs": [],
   "source": [
    "#encoder\n",
    "def getModel(genTrain, genVal, embeddMatrix, learning_rate, clip_norm,\n",
    "             encoder_shape = aMaxLen, decoder_shape = tMaxLen, \n",
    "             nUnique = nUnique, embeddDim = embeddDim, hidden_units = hidden_units):\n",
    "    \n",
    "    #ENCODER\n",
    "    #input shape as the vector of sequence, with length padded to 250\n",
    "    encoder_inputs = Input(shape = (encoder_shape, ), name = 'encoder_input')\n",
    "\n",
    "    encoder_embedding = Embedding(nUnique, embeddDim, \n",
    "                                  input_length = encoder_shape, \n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = False,\n",
    "                                  name = 'encoder_embedd')(encoder_inputs)\n",
    "    \n",
    "    encoder_lstm = Bidirectional(LSTM(hidden_units, dropout_U = 0.20,\n",
    "                                      dropout_W = 0.20, \n",
    "                                      return_sequences = True,\n",
    "                                      return_state=True))\n",
    "    \n",
    "    encoder_outputs, f_h, f_c, b_h, b_c = encoder_lstm(encoder_embedding)\n",
    "    \n",
    "    state_hfinal=Add()([f_h, b_h])\n",
    "    state_cfinal=Add()([f_c, b_c])\n",
    "    \n",
    "    encoder_states = [state_hfinal,state_cfinal]\n",
    "        \n",
    "    #DECODER\n",
    "    decoder_inputs = Input(shape = (decoder_shape, ), name = 'decoder_input')\n",
    "    decoder_embedding = Embedding(nUnique, embeddDim, \n",
    "                                  input_length = decoder_shape, \n",
    "                                  weights = [embeddMatrix],\n",
    "                                  mask_zero = False,\n",
    "                                  name = 'decoder_embedd')\n",
    "    \n",
    "    decoder_lstm = LSTM(hidden_units, dropout_U = 0.20,\n",
    "                        dropout_W = 0.20,return_sequences = True, return_state=True)\n",
    "\n",
    "  \n",
    "    decoder_outputs, s_h, s_c = decoder_lstm(decoder_embedding(decoder_inputs), \n",
    "                                             initial_state = encoder_states)    \n",
    "    \n",
    "    #ATTENTION\n",
    "    attention = TimeDistributed(Dense(1, activation = 'tanh'))(encoder_outputs)\n",
    "    attention = Multiply()([attention,decoder_outputs])\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "    \n",
    "    decoder_time_distributed = TimeDistributed(Dense(nUnique,name = 'decoder_timedistributed'))\n",
    "    decoder_activation = Activation('softmax', name = 'decoder_activation')\n",
    "    decoder_outputs = decoder_activation(decoder_time_distributed(decoder_outputs))\n",
    "    \n",
    "    #MODEL\n",
    "    model = Model(inputs = [encoder_inputs,decoder_inputs], outputs = decoder_outputs)\n",
    "    rmsprop = RMSprop(lr = learning_rate, clipnorm = clip_norm)\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer = rmsprop)\n",
    "    return model, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "Gkx0XSiRx0XQ",
    "outputId": "a4c3bbbc-0d02-45a2-b6b6-6af17a45f58d"
   },
   "outputs": [],
   "source": [
    "rnn, encoder, decoder = getModel(genTrain, genVal, \n",
    "                                 embeddMatrix, learning_rate, clip_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath = \"rnn_model_0509_checkpoint.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "#fit model\n",
    "rnn.fit_generator(genTrain,\n",
    "                  steps_per_epoch = num_train_batches,\n",
    "                  epochs=10, \n",
    "                  validation_data = genVal,\n",
    "                  validation_steps = num_val_batches,\n",
    "                  callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save models\n",
    "# rnn.save_weights('rnn_weights_0509.h5')\n",
    "# encoder.save('encoder.h5')\n",
    "# decoder.save('decoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(model, seq, maxLen, num_iteration, idx2word):\n",
    "    '''\n",
    "    Prediction for a given sequence. \n",
    "    \n",
    "    Parameters:\n",
    "    ===========\n",
    "    1)model: rnn model\n",
    "    2)seq: a single abstract, should be a vector of length 250\n",
    "    3)maxLen: maximum length of predicted title\n",
    "    4)idx2word: dictionary for index to word\n",
    "    '''\n",
    "    \n",
    "    #cache list of prediction\n",
    "    prediction = list()\n",
    "    #initiate title to be a vector of zeros\n",
    "    init = np.zeros(maxLen)\n",
    "    \n",
    "    #for maximum prediction length\n",
    "    for i in range(num_iteration):\n",
    "        #get prediction probabilities for all unique words\n",
    "        predRNN = model.predict([np.reshape(seq, (1, 250)), init.reshape(1, 250)])\n",
    "        #greedy mode prediction\n",
    "        #update next title vector to be the predicted vector\n",
    "        init = np.argmax(predRNN, axis = 2)\n",
    "        #get probabilities of all unique words\n",
    "        pVec = predRNN[0, 0, :]\n",
    "        #get the word with maximum predicted probability as the predicted words\n",
    "        idx = np.argmax(pVec)\n",
    "        #index to word\n",
    "        word = idx2word[str(idx)]\n",
    "        #if eos tag is predicted\n",
    "        #break out of loop\n",
    "        if idx == 1:\n",
    "            break\n",
    "        prediction.append(word)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check prediction \n",
    "check = testX.copy()\n",
    "check = sequence.pad_sequences(check, 250, value = word2idx['_'], \n",
    "                               padding = 'pre')\n",
    "\n",
    "#example of seq2seq prediction\n",
    "#true title\n",
    "[idx2word[str(m)] for m in testY[40]]\n",
    "\n",
    "#true abstract\n",
    "[idx2word[str(m)] for m in check[40]]\n",
    "\n",
    "#prediction\n",
    "check_pred = getPredictions(rnn, check[40], 250, 20, idx2word)\n",
    "check_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate target given source sequence\n",
    "# def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
    "#     # encode\n",
    "#     state = infenc.predict(np.reshape(source,(1,250)))\n",
    "#     #start of sequence input\n",
    "#     target_seq = np.array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "# #     target_seq = np.zeros((source,1, 250))\n",
    "#     # collect predictions\n",
    "#     output = list()\n",
    "#     for t in range(n_steps):\n",
    "#         # predict next char\n",
    "#         yhat, h, c = infdec.predict([target_seq] + state)\n",
    "#         # store prediction\n",
    "#         output.append(yhat[0,0,:])\n",
    "#         # update state\n",
    "#         state = [h, c]\n",
    "#         # update target sequence\n",
    "#         target_seq = yhat\n",
    "#     return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrsGXLs3x0Xc"
   },
   "outputs": [],
   "source": [
    "# #single layger LSTM \n",
    "# def encoder_decoder(genTrain, genVal, mode = 'fit', num_epochs = 1, \n",
    "#                     en_shape = aMaxLen, de_shape = tMaxLen):\n",
    "    \n",
    "# #     print('Encoder_Decoder LSTM...')\n",
    "   \n",
    "# #     \"\"\"__encoder___\"\"\"\n",
    "# #     encoder_inputs = Input(shape=(en_shape,), name='inputE')\n",
    "# #     print(encoder_inputs)\n",
    "    \n",
    "# #     #APPLY EMBEDDING LAYER. https://keras.io/layers/embeddings/       \n",
    "# #     input_emb = Embedding(nUnique, embeddDim,\n",
    "# #                           input_length = aMaxLen,\n",
    "# #                           W_regularizer = regularizer, dropout = p_emb, \n",
    "# #                           weights=[embeddMatrix], mask_zero = True,\n",
    "# #                           name='embedding_1')\n",
    "    \n",
    "# #     #ENCODER LSTM - FORWARD   https://keras.io/layers/recurrent/  \n",
    "# #     encoder_LSTM = LSTM(hidden_units, dropout_U = 0.2, dropout_W = 0.2 ,return_state=True)\n",
    "# #     encoder_LSTM_rev = LSTM(hidden_units,return_state=True,go_backwards=True)\n",
    "    \n",
    "# #     #ENCODER LSTM - REVERSE \n",
    "# #     encoder_outputsR, state_hR, state_cR = encoder_LSTM_rev(input_emb(encoder_inputs))\n",
    "# #     encoder_outputs, state_h, state_c = encoder_LSTM(input_emb(encoder_inputs))\n",
    "        \n",
    "# #     state_hfinal=Add()([state_h,state_hR])\n",
    "# #     state_cfinal=Add()([state_c,state_cR])\n",
    "    \n",
    "# #     encoder_states = [state_hfinal,state_cfinal]\n",
    "    \n",
    "#     \"\"\"____decoder___\"\"\"\n",
    "#     #Input to the decoder would be the summary(headline) sequence starting from ~ character.\n",
    "#     decoder_inputs = Input(shape=(de_shape,), name = 'inputD')\n",
    "# #     decoder_inputs = Input(shape=(en_shape,))\n",
    "#     print(decoder_inputs)\n",
    "      \n",
    "#     decoder_LSTM = LSTM(hidden_units,return_sequences=True,return_state=True)\n",
    "#     decoder_outputs, _, _ = decoder_LSTM(input_emb(decoder_inputs),initial_state=encoder_states) \n",
    "# #     decoder_dense = Dense(de_shape,activation='linear')\n",
    "    \n",
    "#     # Apply a dense layer that has vocab_size(40000) outputs which learns probability of each word when softmax is applied.\n",
    "#     # TimeDistributed is a wrapper for applying the same function over all the time step outputs. \n",
    "#     # Refer https://keras.io/layers/wrappers/\n",
    "#     decoder_time_distributed = TimeDistributed(Dense(nUnique,\n",
    "#                                                      W_regularizer=regularizer, \n",
    "#                                                      b_regularizer=regularizer,\n",
    "#                                                      name = 'decoder_timedistributed'))\n",
    "#     activation = Activation('softmax', name = 'activation_1')\n",
    "#     decoder_outputs = activation(time_distributed(decoder_outputs))\n",
    "    \n",
    "#     #Model groups layers into an object with training and inference features.\n",
    "#     #https://www.tensorflow.org/api_docs/python/tf/keras/models/Model        \n",
    "#     model= Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)\n",
    "    \n",
    "#     rmsprop = RMSprop(lr = learning_rate,clipnorm = clip_norm)\n",
    "    \n",
    "#     model.compile(loss='categorical_crossentropy',optimizer=rmsprop)\n",
    "    \n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.fit_generator(genTrain,\n",
    "#                             steps_per_epoch = num_train_batches,\n",
    "#                             epochs=5,  #Try different epochs as hyperparameter \n",
    "#                             validation_data = genVal,\n",
    "#                             validation_steps = num_val_batches)\n",
    "    \n",
    "#     #_________________________INFERENCE MODE______________________________#  \n",
    "    \n",
    "#     encoder_model_inf = Model(encoder_inputs,encoder_states)\n",
    "    \n",
    "#     decoder_state_input_H = Input(shape=(hidden_units,))\n",
    "#     decoder_state_input_C = Input(shape=(hidden_units,)) \n",
    "#     decoder_state_inputs = [decoder_state_input_H, decoder_state_input_C]\n",
    "#     decoder_outputs, decoder_state_h, decoder_state_c = decoder_LSTM(input_emb(decoder_inputs),\n",
    "#                                                                      initial_state=decoder_state_inputs)\n",
    "#     decoder_states = [decoder_state_h, decoder_state_c]\n",
    "#     decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "#     decoder_model_inf= Model([decoder_inputs]+decoder_state_inputs,\n",
    "#                              [decoder_outputs]+decoder_states)\n",
    "    \n",
    "#     return model,encoder_model_inf,decoder_model_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_xodO1dx0Xe"
   },
   "outputs": [],
   "source": [
    "#let's try this\n",
    "# model = encoder_decoder(genTrain, genVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZuKtx7wKx0Xg"
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxqF7dqZx0Xi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_model.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
